Country,Publishing organisations,Organisation size,Results,Jury's comments,Project title,Publication date,Tags,Technologies tools used,A short description of the project,What was the impact of the project,What tools techniques technologies did you use and how did you use them,What was the hardest part of this project What should the jury know to better understand what you did and why it should be selected ,What can other journalists learn from this project,Languages,Project link 1,Project link 2,Project link 3,Project link 4,Project link 5,Project link 6,Project link 7,Project link 8,Who made this project,Short biography/ies,
Italy,International School for Advanced Studies (SISSA),Small,Shortlist,,The Great Wave,13/09/21,"Long-form, Infographics, Chart, Video, Health","Animation, Personalisation, Scraping, Json, Adobe Creative Suite, CSV, R","The Great Wave is a project developed during a year-long data journalism fellowship at the International School for Advanced Studies (SISSA) in Trieste, Italy, a research center that focuses on physics, neuroscience, mathematics and science communication. Its purpose was to think about the pandemic and its effects in a more systematic way, taking a step back from emergency writing to get a broader look at what happened in 2020. We wanted to understand what did we think about the virus, how did we know it, and whether it was the best use of available scientific evidence, especially in the West.","The results of our project were featured in national and local media in Italy, sparking debates around the timing and effectiveness of the country’s pandemic response. Some public health authorities denied our FOIA requests, and the lack of transparency around critical epidemiological data was also one of the most contested issues.","Data visualization was key to our project. We wanted to report the best available evidence we could find sifting through hundreds of papers, and talking to dozens of experts, in a concise and compelling way. At the same time, given the topic we absolutely wanted to avoid compiling a statistical atlas of the pandemic. Working with information designer Federica Fragapane, we designed our visualizations to make numbers as human as possible. Every time we showed individuals, for example, we made them look all different from each other. There are no two identical objects to represent them. For us it was very important to remind readers that we were talking about people and their stories, not abstract figures. This was the principle that led us to design many hand-made visualizations, crafted from scratch instead of using off the shelf tools.","Our goal, especially in the first half of project, was to try to give some dignity and respect to everybody involved. This was a story about people that lost family, friends, loved ones, and we wanted to do all we could to show empathy and compassion, which is especially hard when working with numbers. One such example can be found in the first chapter when we talked about what happened in Bergamo, the most hit Italian area during the first wave. To give readers an idea of the amount of destruction the virus brought, we tracked each of the excess deaths as best as we could, through all available public information. We then built a simulation of their social structure using demographic data to show how many family members were left without their loved ones.","This project taught us that data driven journalism was invaluable in understanding and reporting this pandemic, but also carried the risk of being cold and heartless. We strived for balance between rigor and humanity. Numbers are great for telling some stories, but - like when we talked about mental health - they also can have large limitations. You can't really use data to talk about feelings in a meaningful way, for example, and so we didn't. Sometimes all that's needed is just a voice. Our hope is that those lessons can be of use to other journalists as well.","English, Italian",http://www.grandeonda.it/en/,,,,,,,,Federica Fragapane,"Davide Mancino (1983) lives in Turin, Italy. He is a visual journalist, information designer and trainer. His collaborations include news outlets in the United States like Fivethirtyeight and Quartz, others in Spain and Germany. In Italy he writes for Il Corriere della Sera and Il Sole 24 Ore.",
Turkey,Journo,Small,Shortlist,,News Deserts Spread in Turkey,24/02/21,"Investigation, Explainer, Database, News application, Map","Scraping, JQuery, Microsoft Excel, Google Sheets, Python","Over the course of two weeks in February 2021, we scraped more than a million news articles from more than 6,500 Turkish news websites in which the names of 973 administrative districts of Turkey appear. With a choropleth map, we revealed which districts have the highest or lowest news coverage per capita, and which districts of Turkey face the risk of emerging ""news deserts."" We also analyzed the authenticity of news content for each district, showing where original reporting is We hope that this research has humbly contributed to a subject area with hardly any data about digital journalism in","Until our study, there was no data about the geographical scope and originality of digital journalism in Turkey. For the first time, we presented the public data-based insights on the issue, showing that news deserts are spreading in certain parts of Turkey —particularly the Inner Aegean, Central Anatolian, and Eastern regions where President Erdogan's ruling Justice and Develıpment Party (AKP) dominates, while original reporting is surprisingly higher in other areas, including the Kurdish-majority Southeast region and in most strongholds of the main opposition Republican People's Party (CHP) across the country.  Perhaps the most striking findings showed that 85% of the news articles published in Turkish digital media are not original reporting but are merely copy-pasted from initial sources (particularly the news agencies). Meanwhile, many provinces that seem like an ""oasis"" in the gross number of news articles are actually ""deserts"" of original journalism. Our results were reported by dozens of news outlets in national and local media. We were interviewed by seven newspapers, five national TV networks, and one national radio channel. The Global Investigative Journalism Network (GIJN) shared our story through its social media accounts and its newsletter. According to our estimates, the project reached more than 13 million people in Turkey through conventional and digital media outlets. We were also hosted at panels to present our findings at physical and online events organized by respected institutions like Columbia University's Global Centers in Istanbul, the European Endowment for Democracy, the International Press Institute, and the Journalists' Association in Ankara. Kizilkaya will also talk about this project at the Data Visualization Society's Outlier Conference in February 2022.","We manually constructed a dataset of more than 6,500 news websites —including almost all national, regional and local outlets in Turkey. We coded a Python script and used jQuery to scrape the articles that these news websites published for two weeks. We then built a knowledge graph to analyze this Turkish-language news content, semantically pinpointing the administrative district names in each article, while also assigning them ""originality scores"" by using a custom algorithm. We computed all results in a custom-designed macro-powered spreadsheet, completing the analysis and data visualization by using Tableau and Datawrapper. As we adhere to the principles of open data, we published the dataset as part of our feature article about this study on our non-profit news website Journo.com.tr in Turkish and in English (see Project Link 3 for the dataset). Several Turkish and international communication academics contacted us to thank us for releasing the dataset and explaining the methodology so that they can replicate or expand this study on Turkish journalism. ","Turkey is a highly challenging country for media outlets. Journalists are being jailed like in China, physically or virtually assaulted like in Russia, and are forced to self-censor like in many Asian and African countries. Independent media outlets are shut down or heavily fined like in Iran, the cronies of the government capture newspapers and TV outlets like in Hungary, and the public broadcaster is under heavy government influence like in Romania and Bulgaria. Independent journalists are labeled as “traitors”, and pro-government media spread fake news like in Serbia, while defamation laws are weaponized by the ruling elite like in Poland. How these challenges affect original reporting in Turkey was a big question mark because there was no extensive study on the subject until we embarked on this route. The hardest part of our project is to come up with a viable method to answer at least a part of this question. After we figured out how we can technically scrape and then semantically analyze so many news articles, the rest was all about succinctly and aesthetically presenting the significant findings related to this Big Data. We narrated our data-based story on these findings and designed social media posts in accordance with the features of the specific audiences and algorithms on various platforms.  ","We believe that this project has humbly contributed to a subject area with hardly any data about digital journalism in Turkey. Our findings should be particularly helpful for those who try to understand the current state of local journalism and original reporting in Turkey amid so many challenges. Journalists can benefit from this study especially by focusing on the geographical discrepancies of original news reporting. In some provinces, we showed that two neighboring districts may be hugely different in this regard. Although they share similar resources and features, one district may have a significantly higher rate of original journalism than its neighbor. As an on-the-field follow-up to this data journalism study, the Journo website sent its reporters to three geographical regions to investigate the reasons behind these discrepancies (see Project Link 4 for these three news articles).   By using this work, Turkish journalists can now microscopically analyze the supply-demand of news reporting on the district level, and use these actionable insights to decide about their newsroom's operations on the ground. Our main map simply shows where newsrooms can invest (for instance, by hiring more local reporters there) because there is an opportunity to satisfy the public demand for original reporting per capita in certain districts. They can also quickly see where competition in local journalism is relatively more intense.  Local journalism is the heart of any democracy. We hope that these findings will also serve the reporters and media executives in their endeavor to inform the public, including the electorate in rural districts, in line with universal standards and rules of high-quality, original journalism.","English, Turkish",https://journo.com.tr/turkey-news-deserts,https://journo.com.tr/haber-colleri,https://docs.google.com/spreadsheets/u/1/d/1UGyBHkpW2PQdIWpqdtTb1P_VzLSz3Q-vFr_6l1ISgzs/edit?usp=sharing,https://journo.com.tr/konu/neden-col-niye-vaha,,,,,"EMRE KIZILKAYA, EMRAH YILMAZ","Emre Kizilkaya worked for Hurriyet newspaper, the flagship outlet of Turkey's largest media corporation, for more than 10 years during which he served as digital coordinator and managing editor. He completed the Knight Visiting Nieman Fellowship at Harvard in 2019, focusing on media sustainability. He is now a vice-chair of the Vienna-based International Press Institute (IPI), and the Project Editor of Journo.com.tr, a news platform for Turkey's next-generation journalists, supported by the Journalists' Union (TGS) and the EU. Emrah Yilmaz is a freelance web developer and data scientist. He worked for Turkey's two leading media monitoring agencies for 12 years.",
Spain,El País,Big,Shortlist,,Avoiding coronavirus infection in indoor spaces: don't breathe other people's air,29/03/21,"Investigation, Explainer, Infographics, Chart, Health","Animation, 3D modelling, Microsoft Excel","Constant ventilation and permanent control of CO₂ levels are two of the keys to avoiding transmission in closed rooms, as fresh air dilutes the infected particles",This report was visited by half a million people in its versions in English and Spanish. It is one of the most viewed contents of the El País science department in 2021. The clarity with which it explains the accumulation of infectious aerosols has been valued by scientists from several countries and hundreds of readers and teachers.,"We use a CO2 meter for hours of measurements in different closed spaces such as cars, elevators and rooms. These data were represented visually through tools such as Excel and Adobe After Effects. 3D models and animations were made in Adobe After Effects","After taking more than 20 CO2 samples from more than a dozen locations, the hardest part was designing an effective visual narrative to show accumulated aerosols and how they dissipate when ventilated.",To always look in their articles for a point of utility and service journalism. While we report indoor aerosol build-up we also provide helpful information to reduce risks.,"English, Spanish",https://elpais.com/especiales/coronavirus-covid-19/how-to-avoid-the-infection-in-indoor-spaces/,,,,,,,,Mariano Zafra and Javier Salas,"Since October 2019 I lead the Graphic and Storytelling team of El País, in Madrid. During the last four years I worked as a Graphics Reporter for special projects in The Wall Street Journal, in New York, and I started and led the Infographic and Data Visualization Department of Univision News in Miami. Before moving to the United States, I spent fourteen years working at the two most prominent national daily newspapers in Spain: El Mundo and El País.",
Brazil,"g1, O Globo, Extra, O Estado de S.Paulo, Folha de S.Paulo, UOL",Big,Shortlist,,Consórcio de veículos de imprensa (Consortium of press vehicles),24/03/21,"Investigation, Multiple-newsroom collaboration, Database, Open data, Health",Google Sheets,"This is an unprecedented initiative involving six of the main vehicles of communication in Brazil to promote the transparency of data on cases and deaths caused by Covid-19 and, more recently, on applied vaccines. In the consortium, teams share tasks and the information obtained so that Brazilians can learn about the evolution of Covid-19, the consolidated numbers of tested cases with positive results for the new coronavirus and the doses of vaccines applied in all units of the federation. A balance is released daily. It is a fundamental service, broadcast on all platforms and with unquestionable journalistic relevance.","The consortium was formed in response to the decision by the government of president Jair Bolsonaro to restrict access to data on the pandemic. So, g1, O Globo, Extra, O Estado de S.Paulo, Folha de S.Paulo and UOL decided to form a partnership, leaving the competition aside, to work collaboratively in search of the necessary information in the 26 states and the District Federal. Something new, but innovative and necessary. By joining forces and showing their commitment to informing, the vehicles sent a sign of confidence to the population in the professional media and reinforced their fundamental role of demanding transparency from government entities. There has never been a journalistic partnership between the vehicles, direct competitors before. Journalism's mission to inform the population, however, made it necessary to act at a decisive moment in the midst of the pandemic. The speed with which the teams were formed to collect the data, the accuracy of the information and the concern with the public are outstanding characteristics of this partnership. The daily collaboration between newsrooms around a common goal makes the consortium a unique initiative. The consortium became a national reference. The data are considered more reliable than those released by the Ministry of Health. The partnership is today the main source of information for other press vehicles and is seen as a fundamental channel by the population.","There is a whole concern with the verification of the data. A careful collection is carried out in each unit of the federation (that is, in the 26 states and the Federal District), whether by email, WhatsApp, website or phone, and a spreadsheet (of Google), which all vehicles have access to, is used to consolidate the information. There is an entire collaborative process carried out in an organized manner, with the objective of providing the population with the most faithful and accurate picture of the current situation of the pandemic. The cells are automated in the spreadsheet, providing a daily bulletin to be shared with all newsrooms, to be published daily at 8 pm. Each vehicle, however, has its way of disclosing data. On g1, for example, there is also a map: https://especiais.g1.globo.com/bemestar/coronavirus/estados-brasil-mortes-casos-media-movel/ ","There are more than 100 journalists involved in the project, spread across the newsrooms of six competing vehicles in Brazil. The organization of work and collaboration involving reporters and newsroom editors with different ways of working are, without a doubt, one of the hallmarks of this partnership. It is no easy task to make a task force like this work, organically and without errors, for so long. This is an unprecedented, exclusive and unique initiative in the Brazilian media.","Journalism lives. Journalism breathes. And its role, of promoting transparency and providing an essential service to the public, always speaks louder. The purpose of bringing information to the public is the engine of this collaboration. A purpose that motivates much more than competition.",Portuguese,https://g1.globo.com/politica/noticia/2021/10/20/consorcio-de-veiculos-de-imprensa-completa-500-dias-de-trabalho-colaborativo.ghtml,https://g1.globo.com/bemestar/coronavirus/noticia/2021/04/29/400-mil-mortes-covid.ghtml,https://oglobo.globo.com/saude/coronavirus/numero-de-mortos-por-covid-19-ultrapassa-350-mil-no-brasil-pais-volta-registrar-media-acima-de-3-mil-mortes-por-dia-24965561,https://extra.globo.com/noticias/coronavirus/brasil-tem-mais-de-100-milhoes-de-vacinados-com-primeira-dose-contra-covid-19-25134610.html,"https://saude.estadao.com.br/noticias/geral,brasil-atinge-a-marca-de-450-mil-mortes-por-covid-19,70003725280",https://www1.folha.uol.com.br/equilibrioesaude/2021/03/brasil-chega-a-300-mil-mortos-por-covid-apenas-75-dias-depois-de-registrar-200-mil.shtml,https://www.uol.com.br/vivabem/noticias/redacao/2021/07/29/vacinacao-covid-19-coronavirus-29-de-julho.htm,,"Renato Franzini (g1), Constança Tatsch (O Globo), Adriana Dias (Extra), Daniel Bramatti (O Estado de S.Paulo), Flávia Faria (Folha de S.Paulo), Marcos Sérgio Silva (UOL)",Representatives of each vehicle: g1 - Renato Franzini O Globo – Constança Tatsch Extra – Adriana Dias O Estado de S.Paulo – Daniel Bramatti Folha de S.Paulo – Flávia Faria UOL – Marcos Sérgio Silva,
Indonesia,Katadata Indonesia,Small,Shortlist,,Katadata Indonesia,23/11/21,"Database, Infographics, Chart, Video, Environment, Health","Animation, 3D modelling, OpenStreetMap","Restrictions on residents' activities during the Covid-19 are believed to reduced air pollution over the city of Jakarta. However, air quality index data reveals the opposite fact that pollution still surrounds the capital city.  In fact, some residents still believe that Jakarta's air is not completely bad. This is evident from the results of a survey conducted by the Katadata Insight Center (KIC) in August 2021. We also summarize a number of ""myths"" that some of the residents believe. As a comparison, we used data collected by Nafas, a startup provider of air quality measurement applications.  ","The poor air quality in Jakarta and its surrounding areas should be a lesson that air pollution is the enemy of all of us. Because, this can have an impact on health in the future. Residents of the capital city needs to encourage the government to immediately deal with sources of pollution in the capital city. For example, by conducting emission tests for cars, factories, and so on.","We interviewed the governments, the observers, and the startup who provides air quality measurement. We use Flourish and Microsoft Excel to calculate and analyze the data that we collect.",Gather all the informations we need and analyze our findings based on the data.,"The importance of data to strengthen the argument in our article. In this case, is about how bad the air quality is in Jakarta, Indonesia.",Bahasa Indonesia,https://katadata.co.id/polusi-udara,https://katadata.co.id/ariayudhistira/analisisdata/61a5f14e4b4b9/alarm-baru-bahaya-udara-jakarta,https://katadata.co.id/ariaw/analisisdata/61a5f1c23910d/polusi-tak-berkurang-meski-pandemi,https://katadata.co.id/ariaw/analisisdata/61a5f243c56fc/meluruskan-mitos-mitos-udara-bersih-di-sekitar-kita,https://katadata.co.id/ariaw/analisisdata/61a5f29cd82ee/video-the-silent-killer-udara-jakarta,,,,"Cindy Mutia Annur, Dimas Jarot, Monavia Ayu, Ratri Kartika","The authors are part of the data journalism team at Katadata Indonesia. They write articles on various topics such as business-economics, social, to politics based on data.",
Nigeria,"International Centre for Investigative Reporting, BusinessDay",Small,Shortlist,,Money for the boys: How agberos pocket billions of Lagos transport revenue,19/07/21,"Investigation, Corruption, Economy",Canvas,"The project focuses on how non-state actors collect transport revenue belonging to Lagos State, Nigeria's richest state, and pocket it. Ana analysis of data shows that they amount of money collected by these non-state actors is $300 million annually. I checked the Lagos State financial statements to asvcertain whether government keeps record of their colections but did not find the record anywhere. I contacted Lagos State Internal Revenue Service and the Lagos State Ministry of Finance but neither could give account of the money. This is the first time anybody could use data to identify financial leakages in Lagos State","The project led to several rebuttals by the Lagos State government and the Lagos Internal Revenue Service. The Lagos State government claims it is now involved in the collection of transport taxes, though there is no evidence that corruption has stopped in the process.   ","I gathered information from local governments in Lagos State and used the information to obtain an average amount collected each day, month and year.  I was able to obtain the number of vehicles, tricycles and motorcycles in Lagos from open source tools and questionnaires obtained from drivers on Lagos roads, and  transport touts.  I then used Canvas for visualisation.","It was very difficult to visit 21 out of 37 local government development areas in Lagos. Secondly, it was also difficult to obtain information from commercial vehicle divers and transport touts themselves. I had to accept to not publish their names before they could give out important information that supported the story. Originally, I went out to seek information on how much these non-state actors collected each year, with the hope that government would have that. But when I could not get it from the government agencies that should have the records, I went through Lagos State's financial statements. Government officials, including one mentioned in the story, wanted to meet me physcically to stop the story but I did not accept to meet him.  It involved some creativity to be able to generate the data myself, after intervewing up to 100 persons for the story. Interviewing those people at bus stops is hard as I was insulted and rebuffed on many occasions. Also, getting the pictures of the perpetrators was really hard. I had to use secret cameras to avoid ""being caught"" by the criminals who could have harmed me.","Journalists cam learn that it is possbible to get original data on any illegal transaction by government. All that is needed is to get micro data from the people involved and extrapolate to a larger population.  Doing this, however, involves skills .  Also, data should not be 100 per cent accurate before they are accepted as correct. The process of gathering data determines whether data is correct or not. So, journalists should pay attention to the process of gathering data, rather than just the final product. ",English,https://www.icirnigeria.org/money-for-the-boys-how-agberos-pocket-billions-of-lagos-transport-revenue/,https://www.icirnigeria.org/profiting-from-illegality-lagos-govt-watches-as-local-councils-touts-exploit-proscribed-cyclists/,https://www.icirnigeria.org/lagos-micro-businesses-in-the-grip-of-area-boys/,https://www.icirnigeria.org/inside-lagos-local-government-councils-where-officials-divert-taxes-to-personal-bank-accounts/,https://businessday.ng/news/article/ongoing-projects-in-south-east-truth-vs-lies/,,,,Odinaka Anudu,"Odinaka Anudu is News Editor of the International Centre for Investigative Reporting. He had worked in BusinessDay Nigeria as Assistant Editor and Head of Investigations. He had also worked as South-East Bureau Chief at Orient Magazine and Newspaper in Nigeria.  He has over 11 years of experience in the media. He has won a number of journalism awards,  including  African Fact-Checking Awards, Citi Journalistic Excellence Award, and PwC Media Excellence Award, among others. He was educated at Nnamdi Azikiwe University in Nigeria and the University of Salford, UK. ",
Russia,Novaya Gazeta,Small,Shortlist,,Fraud in e-voting on the Russian parliament elections,09/11/21,"Investigation, Explainer, Database, Open data, Infographics, Chart, Map, Elections, Politics","Adobe Creative Suite, OpenStreetMap, Python","In Russia, electronic voting was used for the first time in major elections. The authorities reassured that it would be transparent and fair. The source code and full voting data were never released to the public. Data from officials was decoded and independent analysts were able to convincingly prove the existence of falsifications in favor of the pro-government candidates and the ruling party—United Russia. In half of Moscow's districts, opposition candidates won at physical precincts but lost due to e-voting.","The project summarizes a series of materials by independent researchers convincingly proving the existence of falsifications in e-voting to the Russian parliament. The most striking were the articles by the physicist Maxim Gongalsky and the developer Pyotr Zhizhin. The presented article summarizes the previous findings and adds several new proofs. The main one is a sudden and simultaneous increase in the share of re-voting among ALL opposition candidates on the second and third days of voting. The second is a synchronous gap in all districts between voting for pro-government candidates and a pro-government party, which cannot be explained by any real reasons. Articles on the topic of fraud in electronic voting caused a wide response. There were street protests in Moscow against the results of electronic voting. A public group was created to investigate electoral fraud. Legal proceedings are still ongoing. The charts presented in this material were quoted by popular YouTube bloggers and journalists and were discussed at meetings of the Moscow City Duma. The material was published in Novaya Gazeta, on the website (57 thousand views), and in the newspaper with a circulation of 102 thousand copies. Unfortunately, despite this, the Russian government plans to use e-voting in the next presidential elections.",I used decoded blockchain dump. This job was performed by Arthur Hachuyan and Pyotr Zhizhin. Final dataset contains few millions rows. All data analysis was performed in Tableau. Adobe Illustrator were used to create final versions of charts.,"It was rather hard to collect all the data, clean it and match different parts with each other.  The most difficult part was to create clear data visualizations to exactly show the details of the fraud.  This article is of great socio-political importance since it clearly shows that e-voting has become just another tool for uncontrolled falsification by authorities and should be completely abolished in elections at all levels in Russia.   ",Electronic voting is a technically complex thing built on the blockchain. Only a community of caring people with a technical background was able to collect and prepare data that helps uncover violations and falsifications. This case shows how important it is for journalists to be able to work with complex and large datasets.,Russian,https://novayagazeta.ru/articles/2021/11/09/zazor-i-pozor,https://www.dw.com/ru/ng-rasskazala-o-novom-dokazatelstve-vbrosov-golosov-na-djeg-na-dumskih-vyborah/a-59766514,https://www.svoboda.org/a/opublikovano-eschyo-odno-dokazateljstvo-vozmozhnyh-faljsifikatsiy-na-vyborah/31553076.html,https://youtu.be/NqeEmtrW2o0?t=1524,,,,,Aleksandr Bogachev,"Aleksandr Bogachev, 37 years old, data visualization specialist, data journalist. Author of the book about data visualization ""Charts that convince everyone."" Worked as the head of the data journalism department at RBC. Worked with Transparency International, Internews, Article 19, Golos.  I want to thank: - Novaya Gazeta editor Andrei Zayakin, who made a significant contribution to this article - Boris Ovchinnikov, independent analyst, on whose findings, among other things, one of the charts is based - Petr Zhizhin for help with data preparation.",
Spain,Civio,Small,Winner,,Use and abuse of emergency contracts during the pandemic,23/03/21,"Investigation, Long-form, Database, Open data, News application, Infographics, Corruption, Economy","Scraping, D3.js, Microsoft Excel, Google Sheets, CSV, PostgreSQL, Node.js","In 2020, due to the COVID pandemic, Spanish public administrations used emergency procedures to grant companies - in a discretionary way and with minimal control nor transparency -16,589 contracts for 6,445 million euros. And, although the vast majority of these contracts were used to purchase health supplies against COVID-19, they were also used to grant public television concessions, to purchase tasers and even to hire camels for a parade on Christmas. In this data-driven investigation, Civio analyzed for the first time the emergency contracts of all the Spanish public administrations -national, regional and local- awarded and published in 2020.","This investigation is a series of three articles. To work on the issue, the team extracted, cleaned and structured and completed data for months (finding omission of information, errors, incoherent codes, delays and differences in the forms of publication of each administration...) to make up the most exhaustive emergency contracting database available in Spain. Understanding the data required the knowledge accumulated for years investigating public contracts and the technical capacities of the team. Sparing no time, cost or effort.  We reveal three facts: 1) There was a lot of concentration of the money granted in a few companies. Four companies took 10% of all 6,445 million euros. And the one which was awarded the most was not even related to health matters before the pandemic; 2) The supply price gouging which unleashed as the pandemic evolved. There was a lack of stock of many products and intermediaries eager to do business, on occasions, deceived public administrations. For example, while some public bodies paid 25 cents per unit for FFP2 masks, others paid 8 euros each. 3) The rules of emergency contracting were not followed. More than half of the contracts were published late, and many had not yet been made public when this investigation came to light. This series was published by Civio -with El País acting as republishing partner- and reached tens of thousands of readers. Additionally, many other Spanish outlets echoed the investigation, e.g. La Sexta, RTVE, TV3 and La Razón, among many others. One of Civio’s main goals is to lobby when we, after spending months working on an issue, find something that should be changed. For this purpose, we have shared our datasets with almost twenty public procurement and anti-corruption monitoring bodies and have met with some of them to share our recommendations.   ","The data used in this article comes from the Public Sector Contracting Platform (PLCSP), where the majority of Spanish public entities publish procurement data. We scraped all contracts published from 1 January 2020 to 31 December 2020. They total 119,976. In addition, we have also added the contracts that some regional administrations (such as Madrid and Catalonia) publish on their own websites and only summarise on the PLCSP. We found 53,838 such contracts. The goal was to create a comprehensive database for understanding emergency procurement in 2020 and detect potential abuses of the rules. Once we created the database, we reviewed it for possible errors through a combination of automated and manual processes: duplicate files, mis-classified procedures, lots with the wrong prices, mistaken fiscal identification numbers or names written differently each time… Once the dataset was cleased and stored in a PostgreSQL database, a numer of SQL queries were performed to provide data journalists with subsets to analyse. E.g. total awards per company, evolution of unit prices along time for specific products, time delays. Interactive visualisations were a key element of the final product, in order to explain to a general audience a complex subject. A variety of visual elements were custom developed using the JavaScript D3.js library on the Observable platform through different modular notebooks. All of them are responsive, adapting to the different device sizes including small mobile displays, and are easily embeddable in multiple languages through Iframe HTML code.","We have been investigating public contracting for years in Civio and it is always one of the most complex subjects we put ourselves to work in. Data has mistakes: duplicate files, mis-classified procedures, wrong prices, mistaken fiscal identification numbers (especially in the case of non-EU companies, which tend to be inconsistently filled in) or names written differently each time, and separate contracts published together that we have had to extract and separate ourselves manually or any other problem that you can imagine. We always plan these projects for a work load of months knowing the difficulties, and we always end up spending, at least, a couple more months to finish them.  We have filled in as many gaps as possible using the information we had, so we have had to dig up the original award documents when needed. And, sometimes, we couldn’t even find basic information in those documents, such as what had been bought from whom and for how much money. The two main barriers have been missing information and inconsistency and typos in the data. But the end result has been worth it: we have the most comprehensive emergency contracting database available in Spain. It has served to radiograph how our public institutions have been contracting during a time when everybody was looking at the pandemic and to detect abuses. And also, we designed a search engine to facilitate any query about emergency contracting in public administrations and have released all databases on our site.  ","We take public contracting very seriously and have been advocating for changes for many years. We believe that transparency in this issue could, at least, make it easier to find abuses within the system. Since it has been very difficult to pursue these kinds of changes, we always make the subject knowledge available for everyone, specially, journalists. For instance, we have recorded a series of short videos in which we explain basic principles for investigating public contracts: legal framework, sources of information, essential concepts, red flags (signs of possible fraud) and how to find journalistic stories in the fine print of the contractual information. All videos are available in both Spanish and English on our webpage or Youtube.  Also, we made public all of the four repositories of emergency contracts that we created for this investigation. They have already been downloaded about a hundred times. Mainly by journalists, academic institutions and civil society organizations. Many of them even made their own stories with our data. For example, Carlos Otto, from El Confidencial, used the data to put the magnifying glass on emergency awards in a particular area: the cybersecurity of administrations; the newspaper Las Provincias looked at the public administrations of the Valencian Community in this information. And El Comercio focused on Asturias: it used our data to report emergency awards in the region and how the price paid there for some products tripled in less than 10 days. We are proud to create information that can be used and analysed with different angles for common knowledge and for the fight for transparency in public administrations.","English, Spanish",https://civio.es/quien-cobra-la-obra/2021/03/23/four-companies-won-one-in-ten-euros-from-2020-emergency-contracts/,https://civio.es/quien-cobra-la-obra/2021/03/24/from-25-cents-to-8-euros-a-mask-the-price-war-that-paid-millions-to-middlemen/,https://civio.es/quien-cobra-la-obra/2021/03/25/transparency-delayed-officials-published-at-least-half-of-2020-emergency-contracts-late/,https://civio.es/quien-cobra-la-obra/buscador-de-contratos-de-emergencia-en/?lang=en,https://datos.civio.es/collection/contratacion-de-emergencia/,,,,"EVA BELMONTE, DAVID CABO, CARMEN TORRECILLAS, ANTONIO HERNÁNDEZ, ÁNGELA BERNARDO, MARÍA ÁLVAREZ DEL VAYO, MIGUEL ÁNGEL GAVILANES, LUCAS LAURSEN","The Civio Foundation (Fundación Ciudadana Civio) is an independent, non-profit organization based in Spain which monitors public authorities, reports to all citizens and lobbies to achieve true and effective transparency within our institutions. We work for the following: to achieve free access to the information generated by them; to understand how they make decisions; to have public policies based on evidence; to succeed in the public sector being held accountable; and to facilitate action and public participation. The levers we use to achieve these changes are journalism and innovation. We are a multi-disciplinary team of 10 (journalists, computer programmers, and experts in communications and marketing, institutional relations and business development) with international patronage and an advisory board committed to improving democracy.  This investigation is part of Tenders Guru, a project funded by the European Union that aims to monitor public procurement processes in Hungary, Poland, Romania, and Spain.",
United States,Virginia Center for Investigative Journalism; VPM (Virginia Public Media: NPR/PBS); WHRO (Hampton Roads' NPR station); Rappahannock News,Small,Shortlist,,Few Virginia Employers Pay Fines For COVID-19 Violations,30/08/21,"Investigation, Explainer, Solutions journalism, Long-form, Database, Open data, Infographics, Chart, Map, Business, Health, Economy, Employment","Personalisation, Scraping, Microsoft Excel, Google Sheets, CSV, OpenStreetMap, Python","Virginia workers have filed more than 1,450 ""valid complaints"" that employers failed to protect them against COVID-19, government data shows. Only seven states have logged more complaints than Virginia. But workplace health and safety inspectors have cited only about 70 Virginia employers for coronavirus-related violations – and as of Aug. 13, only 25 of them have paid fines. In 24 cases, including instances in which workers died from COVID-19, inspectors did not impose a financial penalty or dropped the fine during negotiations with employers. In the vast majority of complaints, employers were told to correct problems voluntarily, without an inspection.","The Virginia Department of Labor and Industry stepped up enforcement of the state's rules for protecting workers against COVID-19 after the story was published on Aug. 30. Before publication, DOLI was citing about four employers a month for coronavirus-related violations. Since publication, the agency has been citing almost six employers per month – even though complaints tapered off as vaccines became available. Between Aug. 30 and Dec. 30, DOLI posted 23 coronavirus-related citations. The Walmart store in Yorktown, Virginia, for example, was cited for three ""serious"" violations of COVID-19 standards and fined $22,845. The story had noted that Walmart operations in Virginia had drawn more complaints than any other employer but that none had resulted in a citation. Workers' rights organizations praised the investigative project and thanked me especially for writing a sidebar about how to file a complaint with government agencies about health and safety issues in the workplace. Since the project's publication, more employees have filed complaints with DOLI and OSHA alleging violations of COVID-19 rules. OSHA has posted more than 115 such complaints against Virginia employers since Aug. 30.","This project required multiple tools and techniques to execute various data-related tasks: Tabula — To extract text and data from PDFs of workplace citations I received from the Virginia Department of Labor and Industry (DOLI) under the state Freedom of Information Act. Open Refine, Notepad++ and Microsoft Excel — To clean datasets, including OSHA's downloadable list of ""closed and valid"" COVID-19 complaints against employers nationwide (about 60,000 records); a list of employers cited by OSHA for COVID-related violations (obtained under the federal FOIA); and a list of employers cited by DOLI for COVID-related violations (obtained under Virginia's FOIA). I used these tools to standardize spellings, correct misspellings and split and reformat fields. I also used Microsoft Access to add each employer's industry sector to the complaints data. Microsoft Excel and Access — To conduct most of the analysis. For example, I summarized the complaints data, counting the number of complaints filed in each state. I also extracted all complaints against Virginia employers and summarized those records (counting the number of complaints by employer, by industry sector and by date). I also used Excel and TextBlob (a Python library) to conduct a word analysis of the complaints — to see how many involved masks or face coverings, for example. Using Microsoft Access, I joined the complaints dataset and the citations dataset — to identify complaints that led to citations. Google Sheets and Google Drive — To post all of my data and analysis. I also posted full-text PDFs of citations and inspection reports. Caspio — To create a tool for the public to search the complaints data. StoryMapJS — To create a map of ""19 Employers Cited for COVID-19 Violations."" Datawrapper — To create data visualizations, including a chart of employers drawing the most complaints.","I faced three main challenges in reporting this project: 1. I had to submit FOIA requests for the data and documents I needed. OSHA eventually agreed to post the COVID-19 workplace complaints dataset online. However, OSHA and the Virginia Department of Labor and Industry (DOLI) would provide information about employers cited for COVID-19 violations only in response to formal FOIA requests. It took persistence and cajoling to get FOIA officers to respond to requests promptly. When I asked DOLI for the full text of citations and inspection reports, the agency said its policy was to provide paper copies only. It took me four months, and a $21 fee, to get the documents. 2. Data cleaning was a significant challenge. OSHA's file of COVID-19 complaints contained numerous misspellings and inconsistencies in the employer name field, and I had to split a field that contained both the number of alleged hazards and the number of employees affected. Moreover, the file included only a code for each employer's industry classification. The citation files also had inconsistencies: In many cases, they listed an employer's DBA or parent company — not its common name. As a result, joining the complaints file and the citations data required creativity (""fuzzy"" joins on names or joins on addresses, for example). 3. It took persistence and time to fact-check and contextualize COVID-related complaints and citations. By email and phone, I contacted every employer mentioned in the story, and I visited three of the cited businesses to seek comment. I received responses from many of the employers and wrote a sidebar about a restaurant that received DOLI's steepest fine for violating COVID workplace standards. But in several cases, employers refused to comment on COVID-19 complaints and citations filed against them.","Keep track of your FOIA requests with free online tools like iFOIA (from the Reporters Committee for Freedom of the Press) and MuckRock's FOIA letter generator. My project required filing 21 FOIA requests (for COVID-19 workplace complaints, for constantly updated lists of employers cited by state and federal agencies, and for the full-text of citations). These tools can monitor the status of each request and remind me when to send a follow-up letter. Make your FOIA requests polite (""I understand that the COVID-19 pandemic has disrupted work schedules …"") but firm (""The law requires agencies to respond within …""). Document and automate your data clean-up and analysis routines. This was important because I was constantly getting fresh data: The COVID-19 complaints dataset, for instance, was updated about 50 times while I was working on this story. (And the updates didn't simply include appended records. OSHA often added or revised records in the middle of the dataset.) I kept meticulous notes on how I scrubbed or manipulated each field so that I could repeat each step on new data. I used consistent file naming conventions and audit trails to track how the data had changed. I also used a tool called Workbench to replicate my analysis each time datasets were updated. (Workbench has since been shut down.) Let readers drill down to the data details that most interest them. People reading my article would want to know about complaints and citations involving their employers, their industry sector or their locality. So I put all of my data online: COVID-19 complaints nationwide; COVID-19 complaints in Virginia; OSHA citations; and citations by the Virginia Department of Labor and Industry. And I created a tool for easily searching the data. I also posted the full text of citations and workplace inspection reports.",English,https://vcij.org/stories/few-virginia-employers-pay-fines-for-covid-19-violations,https://vcij.org/analyzing-government-data,https://vcij.org/calabash-covid,http://bit.ly/vosh-citations-map,http://bit.ly/va-covid-complaints,https://bit.ly/vosh-covid-citations,https://c0ect156.caspio.com/dp/43422000d70210c42f294de188d7,,Jeff South,"Jeff South was a newspaper reporter and editor for 20 years in Texas, Arizona and Virginia and then taught journalism for 23 years at Virginia Commonwealth University, where he is an associate professor emeritus. He was the first data editor at the Austin American-Statesman and specialized in teaching digital skills at VCU. Over the years, his students won more than 65 national, regional and state awards for news stories produced under his guidance. Jeff himself has won several awards as both a journalist and a teacher, including a Fulbright, and has taught data journalism in China, Ukraine, Vietnam and Azerbaijan.",
Nigeria,NewswireNGR,Small,Shortlist,,David Hundeyin,13/05/21,"Investigation, Satellite images, Crime","Microsoft Excel, Google Sheets, OpenStreetMap","The story is an investigation of the circumstances surrounding the horrific rape and murder of a Nigerian jobseeker, Iniubong Umoren, whose tragic story gripped Nigeria in May 2021. This story brought together leaked call data from a source at the telecoms provider used by the primary suspect, OSINT and brute-force sleuthing to establish that the suspect had help and was part of a syndicate regularly targeting young women to be raped and murdered. Using the data I gathered and the timeline I conclusively established, the story identified another suspect and a key witness, who were then summoned to court.","The project led to the removal of the case from the jurisdiction of the Akwa Ibom State Police Command, which I was able to prove was actively collaborating with the suspects and sabotaging the crime scene so as to hobble the criminal investigation. As part of the project, I made a live TV appearance opposite the Akwa Ibom State Police Commissioner, Frank Amiengheme, whom I confronted with proof from my story that one of his officers was in contact with the suspect long before any police involvement in the case. His inability to muster a coherent response was instrumental in getting the case transferred out of his hands to federal prosecutors. Using my story as the new centre of the investigative framework, the federal prosecutors who took over the case then identified the suspect's sister Francisca, and his associate Kufre - both identified for the first time by the project - as a key suspect and star witness respectively, Without the project, which used OSINT tools like Truecaller and Facebook to establish their existence and identities. they would both have remained completely unknown to the prosecutors and to the public. In the aftermath of the story, a large number of women also came forward with their stories of almost falling into the same trap that Ini Umoren fell into, including a lady called Blessing Godwin, who was identified in the story as a potential alternate victim chosen by the suspect before he ended up murdering Ini Umoren. Essentially, this project proved the capacity of OSINT to help solve complex crimes in Nigeria, and for the first time, provided evidence of the long-rumoured existence of a criminal ring specialising in illegal harvesting of human parts and organs from young women in Akwa Ibom State.","The most important tool I used was a free global cell tower locator tool called cell2gps.com. Researching, learning and using this tool without any prior knowledge of the subject, I was able to make sense of the cell tower data from the leaked call records I gained exclusive access to, and present it as clear and readable information in the story. I also used Truecaller extensively in the story, as one of my 3 main OSINT sources. Running the numbers from the leaked call records through Truecaller, I was able to get names and photos of key characters in the story including Kufre Effiong, who is now a key witness in the ongoing murder trial. Without Truecaller in particular, this story could not have happened. My second main OSINT source was Facebook. Using the information I gleaned from Truecaller, I searched Facebook to identify Frank Akpan. Eventually - after scrolling through hundreds of profiles with the same name - I found the man from the Truecaller photo. Using the information on his profile, I was able to find him on LinkedIn and establish that he was a senior civil servant in the Niger Delta Ministry working in the office of the cabinet minister and former Akwa Ibom State governor, Godswill Akpabio. My third tool/OSINT source was Google Maps. Using satellite views and Google Street View in conjunction with the cell tower location data, I was able to prove that a serving police officer who would later be hailed as part of the team that ""arrested"" the suspect, was in fact in physical contact with the suspect and Frank Akpan, in addition to calling the suspect on the phone at least 24 hours before he should have been aware of his existence.","For me, the hardest part of the entire project was processing the huge volume of data I was dealing with and presenting it in a coherent narrative that would pass across the maximum possible information without compromising my source. To this end, I had to completely avoid using the screen captures my source sent to me, for fear of inadvertently identifying them. I sat with Google Sheets for the best part of 12 hours and I meticulously transcribed all the data from the screen captures into colour-coded cells, using a simplified format that the audience could understand. I then had to go through all this data line by line where the Vlookup function might not be useful, in order to find links, patterns and calls that might be of interest. Every single cell under the ""incoming call"" and ""outgoing call"" columns had to be manually matched to a name and photo via Truecaller, which was a gruelling, time-consuming task.\ For reference, the snippets of this reproduced data published in the story were less than 10 percent of the total data leak I got my hands on. With no other option however, I had no choice but to go through this data manually, line by line, meticulously marking each cell with a rudimentary code system linked to Truecaller names and photos, until I had turned this big data file into actionable information. This process of manually sifting through data took the best part of an entire week. By comparison, writing out the story draft following this process of research and data purification, took just one night.","In this part of the world, journalists could certainly learn to be more skeptical about publishing statements sent out by authorities as though they are incontrovertible facts of a matter. The entire reason I was driven to do the story in the first place, was that in the aftermath of the social media campaign that actually identified Frank Akpan as the culprit when Ini was declared missing, several videos and photos emerged of his hideout, with several telltale signs suggesting that this was not his first time. There were womens clothes'and shoes, notebooks belonging to female students with dates going back to 2013 and several unexplained holes and mounds in the compound. Following my story, a subsequent investigation by another journalist established that there were several eyewitness accounts describing a foul stench similar to that of putrefying flesh, permanently enveloping the compound in question. Despite all of this readily available OSINT indicating that something big and monstrous was happening there, Nigerian media was generally happy to report the press release sent out by the Akwa Ibom State Police Command as news. This completely uncritical, lazy and hasty approach to journalism is what I think of as a crime against the profession in this part of the world, where governments and authorities have extensive track records of telling lies and putting out statements filled with deliberate distortions and omissions. The job of a journalist in the Global South cannot be to simply parrot what someone in authority said. This project became the basis for the removal of the case from a clearly corrupted police command, and served as the basis for the new ongoing prosecution. Without this project, the Akwa Ibom Police Command's false statements and sham investigaiton would have flown unchallenged.",English,https://newswirengr.com/2021/05/13/murder-in-uyo-who-killed-hiny-umoren/,https://www.nextedition.com.ng/investigation-iniubong-umoren-new-evidence-show-police-mishandled-case-another-victim-speaks-out,https://thenationonlineng.net/court-summons-council-chair/,https://www.youtube.com/watch?v=Asj73JFyg4g,,,,,David Hundeyin,"I am a writer, investigative journalist and broadcaster whose work has appeared on CNN, The Africa Report, BBC and BusinessDay. My work as a satirist on 'The Other News,' Nigeria's answer to The Daily Show has featured in the New Yorker Magazine and in the Netflix documentary 'Larry Charles' Dangerous World of Comedy.'  In 2018, I was nominated by the US State Department for the 2019 Edward Murrow program for journalists under the International Visitors Leadership Program (IVLP). In February 2021, I won the People Journalism Prize for Africa 2020. In June 2021, I was selected as one of 12 writers and journalists from around the world to take part in the inaugural $1 million Substack Local program.",
France,Disclose,Small,Shortlist,,Disclose,21/11/21,"Investigation, Long-form, OSINT, Video, Satellite images, Human rights","3D modelling, Drone, Scraping, Json, Adobe Creative Suite, Microsoft Excel, CSV","Disclose has obtained hundreds of secret documents, circulated at the highest levels of the French state, which reveal the responsibility of France in crimes committed by the dictatorship of Abdel Fattah al-Sisi in Egypt. Discover “the terror memos” in a long series of investigative reports. ","Following the Disclose revelations, French MPs have called for a parliamentary commission of inquiry. NGOs such as Amnesty International and Human Rights Watch have called the French state to account for its responsibility in extrajudicial killings in Egypt. They have asked the UN rapporteur on judicial executions to launch an investigation following our revelations.  The government has acknowledged the existence of a secret military operation in Egypt since 2016. It announced the opening of an internal investigation in the Ministry of Defence into the information revealed by Disclose about the Egyptian army's use of intelligence provided by France to repress civilians. But at the same time, the government announced that it was launching legal proceedings against Disclose for leaking secret state documents, in order to identify Disclose's sources. This is an attack on the confidentiality of journalists' sources and their freedom to inform citizens about matters of public interest. In Egypt, despite the dictatorship's censorship of the Disclose website, the investigation has been read and commented on by millions of people on social networks. It has liberated the word on the arbitrary executions of Abdel Fattah Al Sissi's regime  ","Disclose obtained hundreds of secret documents by secure means from a whistleblower. We cross-checked information on the existence of a secret military intelligence mission in Egypt, using satellite imagery to identify the aircraft subcontracted by the French military to carry out this mission. We were able to confirm its presence on an Egyptian base from 2016 to 2021, thanks to satellite images cross-checked with secret documents. We also used aircraft tracking software to find the aircraft used by France.  We were able to cross-check and geo-locate, thanks to the analysis of hundreds of secret documents, 19 bombings against civilians in Egypt for which France is responsible.   In order to trace the responsibility of France in the extrajudicial executions in Egypt, we used the information contained in the secret documents and we used the 3D modeling of one of the strikes in a short video.  We scanned the hundreds of alerts issued by Amnesty International on the repressive actions in Egypt. This allowed us to create a chronology putting into perspective the arms sales from France to Egypt, the secret diplomatic cables obtained by Disclose, and the acts of repression carried out by the authoritarian regime.  ","The hardest part of this project was to work on this unprecedented leak of secret documents from the French state. During several month, Disclose journalists had to implement unprecedented digital security processes for investigate highly sensitive material in secret.  We also had to protect and censor the hundreds of documents obtained in order to protect our sources but also the safety of the French military present in Egypt.  A source has sent Disclose several hundred “classified” documents. These reports from French military intelligence, the Ministry of the Armed Forces and the French military general staff reveal the existence of a secret operation by French intelligence in Egypt called 'Operation Sirli'. Page after page, these documents highlight the abuses of an operation that began in 2016, and raise the question of France's responsibility in the crimes committed under Abdel Fattah al-Sisi's dictatorship in Egypt. This brutality must have been known about at the highest levels of the French state. Dozens of classified reports show that several military departments tried to alert the government of the operation's abuses, both during President François Hollande's term of office and then under the current presidency of President Emmanuel Macron. But in vain.  Given that these “classified” documents involve a subject of major public interest, we have taken the decision to inform citizens of them. In our view this decision is further justified by the fact that this military cooperation project has taken place without any democratic scrutiny.  This investigation is unprecedented in France because it reveals the secret relations with the Egyptian dictatorship.  It reveals how the French government compromised itself in possible crimes against humanity, with the main reason being to sell arms to Egypt. ","This project teaches how to investigate sensitive issues such as secret military operations, using both closed and open sources. It is an example of how to cover both a national issue - in this case military cooperation with Egypt and arms sales - while allowing readers to learn about the impact of cooperation with dictatorial regimes on the population of the country concerned. This project also shows that investigations on sensitive subjects such as military operations can bring together field journalism, data journalism, digital security, OSINT and 3D modelling. This meeting only increases the strength of journalistic investigations and their impact.   The documents obtained by Disclose were not published without a filter. We have reproduced documents in their entirety, we have censored words, document numbers, signatures, names in all documents. It is the mission of journalists to protect information that could endanger the safety of the people appearing in these documents. But also on how to protect the sources behind such leaked documents.     ","English, French",https://egypt-papers.disclose.ngo/en,https://www.youtube.com/watch?v=pEilaWRN_f8&feature=youtu.be,https://egypt-papers.disclose.ngo/en/chapter/operation-sirli,https://egypt-papers.disclose.ngo/en/chapter/cae-aviation,https://egypt-papers.disclose.ngo/en/chapter/france-egypt-arms-sale,https://egypt-papers.disclose.ngo/en/chapter/france_egypt_repression,https://egypt-papers.disclose.ngo/en/chapter/surveillance-dassault,,"Mathias Destal, Jean-Pierre Canet, Ariane Lavrilleux, Geoffrey Livolsi",Mathias Destal is an investigative journalist and cofunder of the investigative media Disclose. Ariane Lavrilleux is a freelance investigative journalist who has working during 5 years in Egypt for several french media.  Jean-Pierre Canet is a freelance investigative journalist and filmmaker of several documentaries.  Geoffrey Livolsi s an investigative journalist and cofunder of the investigative media Disclose.  ,
United States,"USA TODAY Network, Midwest Center for Investigative Reporting",Big,Shortlist,,Downpour,30/11/21,"Explainer, Multiple-newsroom collaboration, Database, News application, Infographics, Map, Environment","Animation, Personalisation, Scraping, D3.js, QGIS, CSV, R, RStudio","We think of climate change as a looming disaster. Yet historic shifts in the way rain falls, explained in shocking clarity through USA TODAY’s reporting, make clear the danger is already here. ""Downpour"" features an analysis of a century of precipitation records from the National Oceanic and Atmospheric Administration and a unique collection of snow and rain extremes computed by a private climate researcher. These revealed stunning increases in intense rainfall in vast sections of the country over recent decades. Through brilliant visualizations and innovative musical auralizations of rainfall data, ""Downpour"" provided deep contextual grounding to understand this year’s ​weather.","The general rise in temperature over the past century has become fairly well known in the collective mindset. The increase in rainfall is a phenomenon less widely understood. This explanatory investigation empowered readers to make sense of weather changes that have tangibly altered people's lives. Our approach was unique in the types of data points we assembled in one place, and scientific sources told us it was both sound and compelling. We achieved this kind of impact through the careful selection of numbers:  At some point over the past three years, 27 states – all east of the Rocky Mountains – hit their highest 30-year precipitation average since recordkeeping began in 1895. A dozen states, including Iowa, Ohio and Rhode Island, experienced five of their 10 wettest years in history over the past two decades. Of 285 weather stations in the continental United States, 44% get at least one more top rainfall event per year now than they did three decades ago. That means what used to count as their top three wettest rainfalls of the year now happen at least four times a year. Intense rainfall events can cause three times as much fertilizer runoff as other precipitation, contributing an outsized share of pollution associated with algae blooms in the Mississippi Basin and Gulf of Mexico, based on our analysis of one Illinois watershed. Among cities with sewer systems designed to discharge untreated waste into streams during heavy rains, 97% have experienced an uptick in both annual precipitation and extreme rainfall over the past 30 years. These cities are ill equipped to pay for sewer upgrades. The median household income in the 728 cities and towns with these vulnerable sewer systems is $45,520, compared with $67,520 nationally, and the poverty rate is 50% higher than the national average.","This project was powered by analyses in R, QGIS and ArcGIS and came to life via innovative visualizations and auralizations.  Rainfall changes. We used R and NOAA annual data to isolate record-setting precipitation years and average precipitation. For changes in high-intensity rainfall, we used statistics from climatologist Brian Brettschneider. With each U.S. weather station, he identified rainfall thresholds exceeded, on average, just three days a year from 1951 to 1990. He then calculated how often each station recorded that amount 1991-2020. There were widespread increases in downpours. Vulnerable sewers, residents. Some sewer systems overflow into rivers when storm drains fill. We used QGIS’ inverse distance weighting algorithm to interpolate Brettschneider’s weather station data into polygons, joining them on coordinates of overflow systems. Nearly all such systems had rising rainfall. The American Community Survey showed these cities averaged higher poverty rates and lower incomes, making infrastructure upgrades a struggle. Increased pollution. We identified one Illinois watershed with continuous, long-term USGS nitrate data, then scraped watershed precipitation levels from Daymet, an online dataset from Oakridge National Laboratory. About 10% of rain events contributed to 33% of pollution. Hearing rainfall. Composers from Full Sail University produced musical representations of state-level data. They combined traditional instruments, sampling and generative audio to embody annual rainfall. We matched each piece to an animated bar chart of underlying rainfall data with a custom player that lets users experience tones and bars simultaneously. Changes near you. We used Datasette, Svelte, the Mapbox geocoder and Turf.js to take a user’s location, find the nearest weather station and climate division, and present a century of local precipitation highs, lows and trends. Guessing trends. We presented a half-drawn line chart showing rainfall over time and let readers draw the remainder for recent years. The interactive then revealed the actual trendline.","The range of separate data analyses and visualizations this project called for was daunting. We set out not only to establish that climate-driven rainfall extremes are real, but also to quantify impacts on humans and ecosystems. We wanted data unassailable among climate researchers yet approachable by any audience.  Choosing the right weather measurements was key. The United States has many distinct climates, and accounting for that is a challenge. Reporters read numerous academic studies and consulted with top climate scientists who specialize in precipitation, formulating with their guidance a unique analysis that would measure how many record wet years and dry years each state had since 2000. The analysis also measured changes in average annual precipitation from 1895 to 2020 by state and in NOAA’s 344 climate divisions.  No database lists extreme precipitation events, and there is legitimate scientific debate about what constitutes “extreme.” To break through that obstacle, USA TODAY worked with a dataset assembled by climatologist Brian Brettschneider. Each U.S. weather station was measured against itself for two time periods, 1951-1990 and 1991-2020. This revealed numerous statistically significant increases in days with heavy downpours.  Another hurdle was missing data. Capturing the impact of heavy rainfall on sewer overflows demanded nationwide coverage. Yet while rain and snowfall amounts are collected each day, hour and minute at weather stations in hundreds of U.S. cities, big swaths of the country have no station. To fill data voids, reporters used an inverse distance weighting algorithm in QGIS to spatially interpolate Brettschneider’s weather station data. It enabled us to say that almost all cities with combined sewer systems are in areas that are having more days with heavy rainfall. Similarly, our analysis of how heavy rains wash polluting fertilizer into streams called for watershed-level rain data interpolated by researchers at Oakridge National Laboratory.","This project affirmed how crucial it is to enlist experts in the field to develop a sound methodology and choose appropriate data, especially when approaching a topic as complex and controversial as climate science. Your stories will have far more resonance, credibility, accuracy and nuance if you leverage these sources from the beginning. The scientists we consulted guided us toward the right data source for the right level of geography. We learned that NOAA's annual average rainfall figures are considered the most accurate for portraying change by state and at the level of climate divisions, which are sub-state regions drawn by NOAA scientists. For high-intensity rainfall events, climatologists encouraged us to talk with Brian Brettschneider, an Alaska-based researcher who shared with us his unique, apples-to-apples calculations on rainfall amounts that previously qualified as extreme for a given location -- and that have become more common in recent decades. His findings are rooted in U.S. weather stations, which, unlike climated divisions, are observation points at a single latitude and longitude.  To correlate fertilizer runoff pollution with downpours, we started out by examining data for the weather station closest to each waterborne U.S. Geological Survey nitrate gauge. But experts noted that even if a station is nearby, it might not actually be within the same watershed. We instead turned to (and scraped) the specialized Daymet dataset, compiled by Oak Ridge National Laboratory, which interpolates weather readings onto a uniform grid of North America. These data enabled us to connect rainfall levels to each square kilometer of the watershed we analyzed.",English,https://www.usatoday.com/in-depth/news/investigations/2021/11/30/climate-change-extreme-rainfall/8550366002/,https://www.usatoday.com/in-depth/graphics/2021/11/30/climate-change-impact-on-rainfall-in-your-state-explained/6249484001/,https://www.usatoday.com/in-depth/news/investigations/2021/11/30/fertilizer-runoff-rain/6201498001/,https://www.usatoday.com/in-depth/news/investigations/2021/11/30/sewer-systems-climate-change/6201425001/,https://www.usatoday.com/in-depth/news/investigations/2021/11/30/music-rainfall-climate-change/6354880001/,https://www.usatoday.com/storytelling/news/investigation/rainfall-lookup/,https://www.usatoday.com/in-depth/news/investigations/2021/11/30/wildfire-rainfall-mudslides-disasters/6201564001/,,"Kevin Crowe, Ignacio Calderon, Dinah Voyles Pulver, Kyle Bagenstose, Cheri Carlson, Ramon Padilla, Mitchell Thorson, Stephen J. Beard, Karina Zaiets, Shawn J. Sullivan, Chris Amico, Craig Johnson, Stan Wilson","Dinah Voyles Pulver and Kyle Bagenstose cover the environment and climate change, and Kevin Crowe is a data reporter. All are staff writers with USA TODAY. Ignacio Calderon was a USA TODAY data fellow at the Midwest Center for Investigative Reporting and is now enrolled in graduate school. Ramon Padilla, Mitchell Thorson, Stephen J. Beard, Karina Zaiets, Shawn J. Sullivan and Chris Amico, Craig Johnson and Stan Wilson create graphics and interactives for the news network. Cheri Carlson covers the environment for the Ventura County Star, part of the USA TODAY Network.  ",
Singapore,CNBC.com,Big,Shortlist,,The Quad Project,23/09/21,"Investigation, Explainer, Long-form, Cross-border, Documentary, Database, Mobile App, Infographics, Chart, Map, Politics, Business, Economy","Microsoft Excel, Google Sheets","CNBC International employed quantitative game theory to generate a digital feature that forecast the future of the ""Quad""—the strategic security grouping formed by the United States, Japan, India and Australia—and what it means for the economic and political future of the Indo-Pacific. To our knowledge, the Quad Project marks the most ambitious, data-intensive use of game theory by any major news organization in history. CNBC worked on the Project for seven months, publishing two days before Joe Biden hosted the first heads-of-state Quad meeting at the White House with Prime Ministers Narendra Modi, Yoshihide Suga and Scott Morrison.","The future of the Quad has tremendous and growing significance as China has become an increasingly assertive global power. The Quad digital feature, entitled ""The 'Quad' is on the rise in Asia-Pacific: Game theory has a prediction about its future,"" was formally distributed within the U.S. State Department and read widely in the capitals of Japan, India, Australia and other countries in the region.  The project's findings sparked intense interest at institutions that helped to ""populate"" the game theory model, including the Hoover Institution, Observer Research Foundation America, Eurasia Group, the Council on Foreign Relations, Brookings Institution, Asia Society, CSIS, Stanford University, New York University, the University of Pennsylvania, Princeton University and others. Especially noteworthy is that the piece was circulated and referenced among academics within China. The digital-first project prompted several live international TV interviews on CNBC with its author, Ted Kemp, and the creation of a feature TV documentary about the project that began airing the week of 3 January 2022 throughout Asia, Europe, Africa, Latin America and Australia. Jonathan Grady, the applied game theorist who worked closest with Kemp on the project, discussed findings on live radio in Seoul, South Korea. Policymakers, risk managers, investors, CEOs, and regular citizens in Asia, the United States and beyond are increasingly aware of rising stakes in the Indo-Pacific region. CNBC International's Quad feature has attracted 150,000 unique readers online, with an impressive average engagement time of almost four minutes. Even before project was published, it generated great excitement within policy circles. Among the 37 individuals who provided data to construct CNBC's Quad game theory model were two former Australian foreign ministers, a former U.S. Defense official, former officials from France and Taiwan, and senior policy experts at several think tanks and top universities.","The Quad Project was based on a proprietary game theory architecture designed by Hoover Institution's Bruce Bueno de Mesquita and his protégé Jonathan Grady of New York University. The Bueno de Mesquita model is well known within the highly specialized world of game theory. In short, game theory uses computing power and logic to predict what individuals will do when they're competing against each other. It creates a model that forecasts the decisions and counter-decisions within a scenario or ""game"" between those people, who are called ""players."" The Quad Project crunched inputs on almost 300 individual ""players""—most of them policymakers at the highest levels of government—in 15 countries and territories. The model ran millions of individual calculations to generate its final forecast. The game theory architecture designed by Bueno de Mesquita has been used internally by the CIA on more than 1,200 intelligence projects, and it was found by the agency to have an aggregate 90% accuracy rate, according to unclassified CIA documentation that was published Yale University Press. The model has also been used commercially by Fortune 500 companies, and it has been cited by the Wall Street Journal and New York Times Magazine. CNBC International's Quad Project model is the largest computation ever run by the Bueno de Mesquita model in its history—more complex than anything the CIA did. CNBC ""populated"" the Quad model with input data from 37 of the world's top experts on policymaking in China, the United States, India, Japan, Australia and 10 other countries and territories. ","The first challenge was constructing the Quad model itself—what's called ""populating"" the data that fed the model. It took seven months to find experts who could provide the data, to interview them in depth, to cross-check data points from experts against one another, and to format the data for the model. Running the model itself was by far the easiest part. Second, most people are not familiar with game theory. Therefore, the feature article had to educate readers on game theory, including its shortcomings and criticisms, in addition to explaining the results pertaining to the Quad itself. This made writing a serious challenge, because it was necessary to balance introductory explanations of game theory and the Quad as a political grouping with a brisk, interesting narrative that delivered the actual predictions. Third, almost as soon as we ran the model, some of its predictions began to actually take place in the real world. The final phases of publication were a race to incorporate those developments into the feature itself quickly so that events wouldn't outrun the predictions we generated. More on that below. This project was a first, not just for CNBC International, but on this scale a definite first for the Bueno de Mesquita model architecture, and we believe a first for any media organization anywhere. We strongly believe this project broke new ground for game theory and for an underexplored, data-driven journalistic practice.","Other journalists should learn from the Quad Project that quantitative methods, applied with rigor, can be used to render accurate predictions of the future, not just to analyze or assess events that have already happened. At CNBC, we came to see game theory as a new adjunct to the qualitative predictions—from analysts, strategists and others—for which we are well known on TV and online. Specific predictions made by the Quad Project began happening almost immediately after we ran the model. The model said other, non-Quad countries would align with the Quad or come close to its position on security, specifically naming the United Kingdom among others. After CNBC ran the model and generated that prediction—but just days prior to the Quad Project's publication online—the United Kingdom made a surprise announcement that it would join a new security partnership with the United States and Australia that will, among other things, equip Australia with nuclear-powered submarines. The UK is not the only non-Quad country that is stepping up maritime security work in the region since then. The model said that leaders in Australia, India, Japan and the United States would become more heavily focused on Indo-Pacific security, and the countries would act in an increasingly coordinated way. The joint statement by the Quad heads of state following their meeting at the White House testified to this new coordination, as have other events.  The model also singled out Vice President Kamala Harris as taking an increasingly sharp interest in the Quad, and it did so a week before she delivered pointed remarks about Indo-Pacific security while in Singapore and Vietnam. There are early, tantalizing indications that the model's most controversial prediction—that factional politics would develop within China in response to the Quad—may be taking shape as well.",English,https://www.cnbc.com/quad-summit-and-china-game-theory-predictions-for-the-future-of-the-quad/,https://www.youtube.com/watch?v=XVWvPyb6JZw,https://www.youtube.com/watch?v=jPKxlwomDlo,https://www.youtube.com/watch?v=caDm0fuvC9Y,https://www.youtube.com/watch?v=6J_FMdAvlEQ,https://www.youtube.com/watch?v=MVKP--chMtM,https://www.youtube.com/watch?v=7sTDJzc76rU,,"Ted Kemp, Bryn Bache","Ted Kemp has more than 20 years of experience as an editor and writer covering geopolitics, economics and business. He is Managing Editor for CNBC International Digital, coordinating global coverage outside of North America. He is based in Singapore. Kemp worked previously as Senior Editor, Markets and Finance, based in Englewood Cliffs, N.J. He also led CNBC.com's political coverage on an interim basis. In 2011-2012, he was editorial manager for CNBC.com's London office, where he guided coverage of the European financial crisis and the Arab Spring.  He reported from Thailand, Burma, Hawaii and elsewhere for the prime time 2018 CNBC documentary, ""Oceans of Crime,"" an investigation of the global fishing industry that has aired across North America, South America, Europe and Asia-Pacific. Kemp is executive producer of ""Beyond the Valley,"" CNBC's popular podcast on tech trends in Europe and Asia. He wrote previously for business and lifestyle magazines, web outlets and newspapers, on topics ranging from high finance to boxing, and is co-author, with Lt. Col. Michael Zacchea (USMC-ret.) of The Ragged Edge: A U.S. Marine's Account of Leading the Iraqi Fifth Battalion. Kemp began his career as a markets reporter at Dow Jones. He earned his MS in journalism from the University of Illinois at Urbana-Champaign and his BA at North Carolina State University. Bryn Bache is digital design director at CNBC International, based in London.  Bache designs, develops and manages design elements for CNBC International special projects pertaining to desktop, mobile web, digital apps and iTV products. He works with both internal and external development teams.  Bache is responsible for ensuring that CNBC's digital products continue to best serve its international audience, maintaining integrity in usability, design and best practices across digital.",
India,"IndiaSpend, The Hindu, Dainik Bhaskar, Scroll.in",Small,Winner,"""In the second year of the coronavirus pandemic, several of the strongest entries to the Sigma awards were data journalists’ efforts to use the concept of “excess mortality” to demonstrate that official death tolls in many jurisdictions were likely undercounts. Whether due to officials downplaying the severity of the disease, flawed or variable definitions of what deaths to attribute to the pandemic, or simpy limited administrative capability,  this problem was found in many parts of the world.

The basic data needed to calculate all-cause mortality in excess of expectations based on previous years is not available in all — or even most — countries. And where it is available, it is often published with a lag too great to allow for reporting in a crisis. Journalists’ work identifying and obtaining faster means to access local mortality data has been vital to enabling this type of reporting in many countries where ready access to this data is limited.

Rukmini S, a freelance journalist, was foremost among a number of reporters who undertook this type of analysis in India, where she obtained the mortality data for four states and and the city of Chennai. In a series of articles and op-eds for IndiaSpend, Scroll.in, and Dainik Bhaskar she explained the data quality issue to readers, brought together the available data from various local, state and national sources, and persuasively documented the evidence that India’s official Covid death toll was likely understated.

This was dogged reporting work by an independent journalist using little more technology than a spreadsheet, an important reminder that data journalism does not necessarily require huge investment in technology or complex visualisation capabilities to have significant impact. 

Her work was published in English, Hindi, and Tamil and she made her data and methods available to other journalists and researchers, encouraging similar work in other Indian states and further afield. The work of India’s journalists reporting on this data — has been vital to our understanding of the true scale of the Covid-19 pandemic in the world’s second most-populous country, something now repeatedly acknowledged by academic researchers using the material they uncovered.""",Rukmini S - Indian excess mortality investigation,23/03/21,Investigation,"Microsoft Excel, Google Sheets, CSV","Since the beginning of the pandemic, I have been reporting that India's official death toll from covid is likely a severe underestimate, both on account of historical registration issues, and account of an overly strict definition of a covid death that kept number artifically low. By first building a case for and then accessing confidential all-cause mortality data in India, I was able to provide the first estimates of true covid mortality in India. I reported on excess mortality in the city of Chennai, and the states of Madhya Pradesh, Andhra Pradesh, Tamil Nadu and Kerala and found that in","First, my reporting produced the first estimates of excess mortality in India, and by extension, the first estimates of missed covid mortality. The reporting was immediately picked up by others news organisation in India and abroad, and the impact was magnified by the fact that my articles appeared not only in English, but also in Hindi (Dainik Bhaskar) and Tamil (IndiaSpend Tamil). I have had the opportunity to speak about my reporting on excess mortality at multiple fora including the UN World Data Forum. Second - and this is something that I'm particularly proud of - by putting all of my methodology and data in the public domain, as well as throwing out an open invitation to journalists across India (no matter what language they worked in) to contact me if they had all-cause mortality data that I could guide them in fashioning into a story, I was able to set off a domino effect. Journalists across Indian newsrooms began reporting on excess mortality in their cities and states, and between us we have now produced data for 18 India states or over 700 million people. Lastly, since all of my data is on Github, researchers from across the world have had access to it to do scientific work on covid mortality in India. Multiple papers have been written using the dataset, and it forms part of global repositories. Where there would have been a large India-shaped hole in our understanding of global mortality from covid, reporting by me and then other Indian journalists was able to fill this gap. Additionally, I had dozens of bereaved people from across the country reach out to me to say that my work was helping honour the memory of the people they had lost, when official statistics were erasing them. I think that","I used Excel for data analysis and Google Sheets, Infogram and Datawrapper for charts. This was not a data journalism project that used much technology, and I had to rely chiefly on my investigative, journalistic, analytical and narrative skills. ","I am going to go out on a limb here and say that too much of data journalism now rewards visualisation rather than journalism; while visualisations are difficult to pull off for independent journalists and those in resource-constrained settings, this is also where some of the most vital jouralism is being done. This is not a project that used advanced tech or visualisation tools, but a project that required all of the best skills of investigative reporting to tell a data story. First identifying why this data was needed by specifically pointing out what India's official covid death toll was missing (instead of over-broad generalisations and suspicions about data suppression) through analytical articles like the ones I wrote for IndiaSpend was vital to create an understanding of what all-cause mortality data could do. Then accessing this data as I did for the pieces in The Hindu, Dainik Bhaskar, Scroll and IndiaSpend, required developing sources to access confidential data. After I wrote the articles, I wrote an op-ed for The Hindu explaining both the deficiencies and the advantages of using all-cause mortality data, and helping put what I had found in context. The hardest part of this work has been the consistent refusal of the Indian government to part with data. All of the all-cause mortality data had to be accessed using confidential sources which had to be developed at the level of each city and state government. The National Health Mission administrative data that I used to estimate rural India's excess mortality was pulled off the government's website while I was trying to use it (and is now no longer updated since I wrote the article). As the government continues to stonewall all attempts, putting this data out was all the more essential to know the true impact of covid","Hopefully, other single-woman teams like me can learn that being unable to produce impressive-looking data journalism doesn't mean that you can't produce data journalism that can change the world for the better. I used nothing more impressive that Microsoft Excel and had to do all of my reporting solo, but was, I hope, able to produce journalism that has altered how the world sees covid in India, push back against government stonewalling, and grant some dignity, even if it is in statistics, to the millions who died of covid but went unremarked on and uncounted.  I hope the domino effect that this project had can also help other solo journalists like me realise that we are not alone, and if we put our data and methodologies out in public, others can reach out to us, learn from us, and carry on the mission of speaking truth to power through data.   Finally, I hope that this project gives other journalists who are operating in environments of government suppression the courage to develop alternative sources and push back against the denial of data.","English, Hindi, Tamil",https://scroll.in/article/999888/covid-19-second-wave-india-recorded-3-lakh-more-deaths-in-may-2021-than-the-same-period-in-2019,https://www.indiaspend.com/covid-19/mortality-data-kerala-mumbai-too-soon-to-say-india-covid19-less-deadly-second-wave-737270,https://www.thehindu.com/opinion/op-ed/interpreting-deaths-in-chennai/article34645264.ece,https://scroll.in/article/997427/andhra-pradesh-saw-400-increase-in-deaths-in-may-tamil-nadu-saw-more-modest-excess-mortality,https://www.indiaspend.com/covid-19/deaths-first-wave-second-wave-pandemic-757701,https://www.indiaspend.com/covid-19/deaths-unknown-causes-national-health-mission-portal-covid-toll-760219,https://www.thehindu.com/opinion/op-ed/gauging-pandemic-mortality-with-civil-registration-data/article35157185.ece,,"Charts for the articles were made by the design teams at The Hindu, IndiaSpend, Dainik Bhaskar and Scroll.","Rukmini S is an independent data journalist based in Chennai, India. Rukmini has been a journalist since 2004, starting with the Times of India, and then working as Data Editor at The Hindu and HuffPost India. She is now a columnist for Mint and IndiaSpend and writes for many other Indian and foreign publications.  Rukmini won the Likho Award in 2019. Her pandemic podcast, The Moving Curve, won an Emergent Ventures India Covid-19 Prize in 2020. In December 2021, Rukmini's first book “Whole Numbers & Half Truths: What Data Can and Cannot Tell Us About Modern India"" was published by Amazon Westland.  ",
Switzerland,privat,Small,Shortlist,,A Song of Crowns and Tears,28/03/21,"Infographics, Video, Audio, Arts","CSV, Python","As a journalist, I work with Covid-19 data every day. The incidence is rising, more deaths, hospitalisations are increasing, ... But with each new wave of the pandemic, these figures lost a little of their horror for me. Victims became emotionless numbers. With «A Song of Crowns and Tears» I wanted to change this. The pandemic, the suffering, the victims should become audible. In this project, numbers should be freed from their emotionlessness and show what it's really about: people.","The video on Twitter, Youtube and Reddit spread surprisingly within a very short time. Newspapers from abroad reported on it, radio stations in the USA played the piece, I was asked for interviews and an artist even produced a dance performance based on «Crowns and Tears». But what moved me the most was all the feedback. People who wrote to me that they were touched by this project.","«A Song of Crowns and Tears» is based on the official figures of the Swiss state. These figures are used to automatically generate punch cards. The song is imported from a MusicXML and placed on the graphics. The transformation is done with Python. The song itself was written manually using the available data in the music notation software MuseScore (more about how the music was written in the next question). The punched cards were printed on the home printer, glued together and punched in painstaking work.","What does a pandemic sound like? What does data sound like? My first attempt: Each note represents a certain number of deaths. After writing a few bars, I had to realise: That doesn't work. . Intensity and emotion in music is not about stacking more notes together. Sometimes nothing can hit you more than a quarter break. Dissonancec, tempo, chords, dynamics - so many things bring music to live. So I changed my approach and tried to write a corresponding soundtrack to the data. On the right side of my screen I had the data, day by day. Ond the left the music notation software. I went through every day in the data and looked for the corresponding sound, that respresents the data and still follows the rules of music. This was probably the hardest part. Writing the scripts that transfer music and data to punch cards was straightforward. Producing the actual punch card, on the other hand, was challenging and took dozens of attempts until the instrument was able to play the four-metre-long punch card correctly.","There are so many good Covid 19 visualisations, there is not much to learn from ""A Song of Crowns and Tears"". Only perhaps: Sometimes it is worthwhile to present data in a different way than usual and to experiment with other modes of representation.","English, Language of Music",https://www.youtube.com/watch?v=DqfrOPs2pKM,https://github.com/simonhuwiler/crowns-and-tears,,,,,,,"Covid-19, Simon Huwiler",Simon Huwiler is a data journalist from Switzerland. He currently works for the Neue Zürcher Zeitung (nzz). He is a trained software developer and journalist and has a strong passion for experimental visualisations. He is convinced that data should not only be presented with diagrams. More about him: www.journalist.sh,
India,IndiaSpend,Small,Shortlist,,Data Gaps,15/02/21,"Investigation, Explainer, Solutions journalism, Long-form, Database, Open data, Environment, Women, Agriculture, Economy, Employment, Human rights","Microsoft Excel, Google Sheets, CSV","Our Data Gaps series is our effort to highlight the numbers that are not measured, or not shared publicly, and to question why. By shining the light on known or unknown gaps in public data, we hope to explain how this makes government policy less effective and inclusive, while limiting transparency and independent critique.","As data journalists, we routinely run into problems with data availability, accessibility, and reliability. It has long been considered a truism in data circles that ‘if you can't measure it, you can't fix it'. Today, when the Indian government is relying more readily on data to design, implement and evaluate its policies and programmes, the use of incomplete or unrepresentative data can perpetuate, and even create newer forms of, inequity.     By shining the light on known or unknown gaps in public data, we sought to explain how this makes government policies less effective and inclusive while limiting transparency and independent critique. This ongoing series has sparked many a conversation about the need for better, open, and accessible data in India's public sphere, and many stories have been republished in several national and provincial publications, and cited in academic journals (examples below). In 2021, we've written about the lack of data related to Covid-19 in India and the resulting underestimation of deaths, on the lack of data on transgender persons, on Dalit Christians and Muslims, on migrant workers, and women's land ownership. Other impacts (Citations) Devesh Kapur, Neelanjan Sircar and Milan Vaishnav Johns Hopkins University School of Advanced International Studies, Washington, October 2021 Journal Article (international) Gender, Social Change and Urbanisation in Four North Indian Clusters  cited: https://www.indiaspend.com/how-official-data-miss-details-on-half-of-indias-citizens Sharan Bhavnani, Prashant Narang, and Jayana Bedi Centre for Civil Society October 2021 research paper (national) Rights, Restrictions, and the Rule of Law COVID-19 and Women Street Vendors cited: https://www.indiaspend.com/how-better-data-could-help-prevent-custodial-deaths/   ","The nature of the series limits the use of technologies, and most stories used boots-on-the-ground reportage. We perused reports going back to Indian Independence, and examined smaller surveys and reports commissioned by the government. We also filed several requests under India’s Right to Information Act, but many of these requests were met with a standard response: “No such data are maintained.” For instance, the story on Dalit Christians and Muslims had to depend on a report commissioned by the government in 2008, even as more recent data should have been available through the 2011 national census, but wasn’t. We relied on experts and members of the community to give us access to reports and insights on the data gaps.  We used OCR tools to glean information from some of the scanned documents, while many handwritten documents had to be studied manually. Further, to analyse what data were available, we used Google Sheets and Tableau.  ","Since many of the issues covered in the series are politically contentious–including caste, human rights, and welfare entitlements–there was a lack of open data, and a reluctance on the part of government departments to provide the information. We filed several RTI applications and appealed against the non-disclosure of information for several months.  We looked at decades-old data, spoke to several experts to understand the issues, and used their work to shine the light on inadequate information.  ","As countries around the world restrict access to good quality data at the right time, this series serves as a guide to journalists across the world to highlight the gaps and initiate conversations in the public discourse towards a truly open data culture.  We hope that such conversations in Indian journalism will urge governments to make more data freely available, which in turn will help an engaged electorate participate in evidence-based policymaking.  ",English,https://www.indiaspend.com/data-gaps,https://www.indiaspend.com/data-gaps/how-unreliable-data-on-dalit-christians-muslims-expose-them-to-discrimination-734972,https://www.indiaspend.com/data-gaps/20-months-in-gaps-persist-in-indias-official-covid-19-data-772670,https://www.indiaspend.com/data-gaps/access-to-toilets-sanitation-swachh-bharat-abhiyan-data-surveys-762488,https://www.indiaspend.com/land-rights/why-we-dont-know-how-much-land-women-own-734247,https://www.indiaspend.com/gendercheck/denied-visibility-in-official-data-millions-of-transgender-indians-cant-access-benefits-services-754436,https://www.indiaspend.com/health/why-india-needs-village-level-data-to-target-malnutrition-in-children-752252,,"Shreehari Paliath,  Prachi Salve, Shreya Khaitan, Karthik Madhavapeddi, Madhur Singh, Anoo Bhuyan, Shreya Raman, Pranab R Choudhury, Gokulananda Nandan, Archita Raghu, Snigdhendu Bhattacharya, Rukmini S","Shreehari Paliath,  Prachi Salve - Journalist, IndiaSpend Shreya Khaitan, Karthik Madhavapeddi - Senior Editors, IndiaSpend Vishal Bhargav - Producer, IndiaSpend Gulal Salil - Former Graphic Journalist, IndiaSpend Madhur Singh - Former Managing Editor, IndiaSpend Anoo Bhuyan - Former Health Reporter, IndiaSpend Shreya Raman - Former Data Reporter, IndiaSpend Gokulananda Nandan, Archita Raghu - Former Interns, IndiaSpend Pranab R Choudhury - Founder and coordinator of NRMC Centre for Land Governance, Bhubaneswar. Snigdhendu Bhattacharya - Author and Journalist Rukmini S - Data Journalist",
Ukraine,Texty.org.ua,Small,Shortlist,,Fires in arable land,03/11/21,"Investigation, Database, News application, Map, Satellite images, Environment","Sensor, Scraping, D3.js, QGIS, PostgreSQL, Python","More than 30,000 fires in non-residential areas per year happen in Ukraine in average and this is largest number for Europe. To show a scale of a problem and to prove most of them are artificial we have created a program that automatically finds and shows satellite photos of places on fire. The visualization currently covers the years 2018-2021 and contains around one thousand such photos. Over time, each week the program adds new photos with fires.","The fire kills animals, insects, and bacteria, it destroys fertile black soils. Millions of tons of carbon dioxide and other harmful gasses from fires worsen the air we breathe and makes global climate crysis even worth. Ukraine have the largest number of fires among all neighboring countries. Checking out our interactive visualization users can see what these fires look like. Satellite images in our project show that many such fires are created intentionally (fires appear simultaneously in many parts of the field). Farmers who burn stubble are involved in targeted arson. In fact, this method is guaranteed to quickly kill blacksoils. In addition, these fires can often spread to nearby forests or settlements. And they are a direct threat to human life. Our visualisation clearly shows the scale of the fires, partly created intentionally, and so points out the harmful effect from fires on arable land. Together with visualisation we add to the article comments about legal grounds of such activity and comparison to situation in other countries. Ukrainian society have to pay more attention to the problem in order to avoid harmful and moreover irreversible effect on arable land, which provide a huge part of national economy.","We obtain data for fires using the FIRMS service, which uses the results of monitoring the earth's surface by NASA / NOAA satellites. The satellites fly over every point on the Earth's surface twice a day, and their infrared sensors can detect heat sources in an area of about 400 by 400 meters. After receiving the data, we group the points with fire and select only clusters larger than a certain size to cut off minor / accidental heat sources and sensor errors. We also reject heat sources in industrial areas, which are present there almost constantly. We use the coordinates of each selected cluster as the coordinates of the fire, along with its date. All processing is done with a python script. After that, knowing the coordinates of the fires, our program tries to find and download a photo for this area, taken at the right time. To do this, we use Sentinel-2 satellite data from the European Space Agency. If a needed data for a place and time is available, we process it with a special algorithm to ""highlight"" the areas with flames in the pictures and to create  composite RGB images to show in our project. All data are obtained, processed and served with a custom-build python scripts. Sentinel-2 data obtained thru Google Earth Engine cloud service. Processed images and metadata are served trough API created in Django web framework. D3.js library is used to create interactive visualization.  ","The hardest part of a project was a method to find and process thousands of files with raw satellite data from which images created later. Although to obtain one such ""image"" is a relatively easy task, it's much more harder to do it for such a batch processing as in our case. After many attempts -- we tried at least five different services and libraries, at last we choosed  Google Earth Engine which in our opinion currently is the best platform for custom mass processing of satellite data, ready to use by teams with limited budgets. Second problem: in the original data there are no such zones of flame that you can see on visualization. We use special algorithm to highlight flames on images to quickly locate the fire. However, the zone of flame, which is completed by the algorithm, clearly corresponds to the places where the combustion takes place (this can be seen in the pictures). As far as we know this is a first such a project with constantly updating large collection of recent satellite images with fires.    ","Using open source data (in this case from European Space Agency) and different open source tools with a little bit of creativity allow journalists to visualize almost any story where remote sencing is involved,  even on very low budget.","English, Ukrainian",https://texty.org.ua/projects/105282/ukraine-there-are-about-20000-fires-arable-land-yearly/,https://texty.org.ua/projects/104757/v-ukrayini-shoroku-blyzko-30-tysyach-pozhezh-yak-vony-vyhlyadayut-iz-kosmosu/,,,,,,,"Yevheniia Drozdova, Andrii Harasym, Nadia Kelm, Illia Samoylich, Anatolii Bondarenko","TEXTY data-journalism team: data-analysts Yevheniia Drozdova, Illia Samoylich, Anatolii Bondarenko, designer Nadia Kelm, journalist Andrii Harasym. Anatoliy Bondarenko: Co-founder and head of data journalism in Texty.org.ua, Kyiv, Ukraine. Physicist by education. Author of the course about data visualization for Prometheus (Ukrainian MOOC platform) and visiting lecturer in UCU, Lviv (“Practical introduction to data journalism”). Nadia Kelm: designer in Texty.org.ua, Kyiv, Ukraine. Author of the course about design in data visualization for School of Infographic (Internews Ukraine), winner of the national competition The best book design 2019 (the Arsenal Book Festival). Andrii Harasym: journalist from Kyiv with exerience in analytical work in environmental and civic engagement spheres. Illia Samoylych: Python developer, fullstack developer; joined TEXTY team in 2021 as analyst. Yevheniia Drozdova: data analyst of TEXTY with wide range of expertise; from data scraping to design; trainer of TEXTY's educational events online.",
United Kingdom,Airwars,Small,Shortlist,,Mapping and documentation of all civilian harm during May 2021 Israeli-Palestinian conflict,09/12/21,"Investigation, Explainer, Cross-border, Database, Open data, Fact-checking, OSINT, Crowdsourcing, Infographics, Map, Satellite images, Politics, Human rights","360, 3D modelling, Scraping, QGIS, JQuery, Microsoft Excel, Google Sheets, CSV, PostGIS, OpenStreetMap","This entry is built around an interactive online map and report at Airwars.org comprehensively documenting for the first time all civilians killed and injured in conflict during the May 2021 conflict between Israel and Palestinian militants. Published on December 9, the map and report were the result of six months of research, geolocation and mapping. The work represents the most extensive available public record of the 11-day conflict, telling the stories of human suffering in the voices of those affected, including granular details of incidents, images of the victims, and video footage of the strikes.","Labelled by a New York Times visual investigator as “one of the most detailed and complete databases of civilian harm I’ve ever seen,” the map and report were shared thousands of times on social media, while Airwars' social media accounts had hundreds of thousands of impressions. The work has also been shared by multiple stakeholders with large audiences, such as prominent journalists and commentators on foreign policy in the UK and elsewhere. The interactive map, and accompanying report and database - available permanently and for free in Arabic, Hebrew and English - received positive coverage in more than 25 major news outlets, including The Times (UK), The New Arab, The Jerusalem Post, a New Lines investigation by Airwars staff, and The Intercept. As such it is expected to have a lasting impact both in the Middle East and globally.  On a policy level, the resource represents a key advocacy tool during negotiations at the United Nations in February 2022 on limiting the use of wide-area effect explosive weapons in urban areas. The findings were for example discussed by civil society organisations and British and European parliamentarians in a recent panel discussion, highlighting civilian harm concerns. Previously, Airwars' extensive archive of civilian harm data has been used for public engagement; as a critical element of major investigations by others (for example the recent high impact New York Times series on civilian harm); as the basis of United Nations and international agency probes; and as part of wider efforts to seek justice and compensation for victims’ families. We expect that this will be the case for this project in 2022. Airwars will also seek to further engage with Israel Defense Forces, and with Palestinian factions, to better identify civilian harm events and trends, and seek future casualty reductions.","We used a variety of open-source techniques to build comprehensive datasets of civilian harm from Israeli strikes in Gaza and Syria, as well as from civilian harm in Israel from Palestinian rocket fire. This began with in-depth primary language research by our specialists in Arabic, English and Hebrew to create nearly 200 individual assessments, featuring 5,288 unique sources. This involves a complete review of all hyperlocal sources, including social media posts from affected communities, local journalism, and deeper research and academic reports. Each of these assessments were then reviewed multiple times, before the dedicated geolocation team mapped each individual incident often down to the exact location. After building the archive, Airwars conceived of and designed the neighbourhood map in conjunction with the team at Rectangle, our regular design partners. This was plotted on Mapbox with shapefiles designed via QGIS, in consultation with Palestinians from Gaza to ensure local knowledge was properly reflected. The map included 3D protrusions based on neighbourhood to immediately identify which areas had seen the most bloodshed. Each area was linked to a database of images and individual assessments, so viewers would be able to scroll through images and stories of each civilian casualty. We produced similar maps for victims of Palestinian rocket fire inside Israel, as well as of civilians killed by Israeli strikes in Syria since 2017. Additionally, for Gaza we also mapped every single Israeli strike and every civilian harm incident onto a map of population density in the Gaza Strip, overlapping different datasets to illustrate a clear story. This is proving to be a vital resource for those seeking to explain the high risks associated with wide area effect explosive weapons use in urban areas.","The most difficult part of this project was the challenge of ensuring accuracy of information on the civilian status of individuals, while also remaining consistent with our approach used in all conflict monitoring to archive all locally reported allegations of civilian harm. The Israeli-Palestinian conflict is a particularly sensitive and contested one and the lines between civilian and militant are often deliberately blurred, both by the belligerents and by observers. As such, wrongly identifying a militant as a civilian, or vice versa, could have significantly negatively impacted the credibility of the entire research and data set. In order to solve this problem, we carried out a series of measures, including: - Scraping all the ‘martyrdom’ statements from the Telegram channels of the military wings of Hamas and Islamic Jihad, and cross referencing these with every civilian reported to be killed during May 2021. - Reviewing all official Israeli military and government claims regarding the status of those killed, as well as other Israeli research organisations that made claims about them. - Regular contact with Israeli, Palestinian and international NGOs to check facts. - Multiple reviews of all assessments and comprehensive data quality checks to be certain all information was correct. - Ensuring that all information available was clearly archived to ensure transparency and encourage feedback. - Remaining consistent with our methodology used across all conflicts - whereby civilian casualty estimates are provided in ranges (lowest estimates, highest estimates) to take into account conflicting assessments of civilian status.  Although Airwars has a global reputation for the high quality of its conflict casualty reporting, the Israeli-Palestinian conflict is both a mature and highly polarised arena. It was therefore critical that our engagment bring both unique insights into the conflict; with the highest quality work. We believe we have succeeded in this.","The May 2021 conflict between Israel and Palestinian factions receieved high and welcome media coverage at the time - though this was necessarily episodic and limited, particularly when understanding casualty and violence trends. Checking with local and international NGOs, it became clear that no one organisation was planning to build a comprehensive model of the violence, and that this was an expertise we could bring on this occasion. The project yet again shows the potential power of online and remote research in cases where political powers seek to use border crossings or battlefield restrictions to limit transparency and accountability. Gaza has been all but closed to the world since Israel imposed a blockade more than a decade ago. Due to Israeli restrictions, it is near impossible to get foreign human rights researchers into the Strip. This is a trend across the globe, with governments and militaries increasingly closing off access to seek to suppress coverage and dissent. Yet the victims of the recent conflict deserved their individual stories telling in a comprehensive fashion. Our own researchers based in the United Kingdom, Turkey, Germany, the United States, Lebanon, Iraq and elsewhere worked together to overcome the physical distance challenges, and to document the stories of each victim. This again highlights the potential positive power of online research. Another key aim of our project was to examine the May 2021 conflict in the context of broader military trends - in particular the high risk to civilians posed by urban fighting. With its own extensive modeling of recent urban violence at Mosul and Raqqa, Aleppo and Tripoli, Airwars was, we believe, well placed to bring this additional key global perspective to the Israel-Palestine conflict.","English, Arabic, Hebrew",https://airwars.org/conflict-data/civilian-casualties-gaza-may-2021-map/?lang=en,https://airwars.org/wp-content/uploads/2021/12/Why-did-they-bomb-us-ENG.pdf,https://airwars.org/news-and-investigations/gaza-israel-syria-ewipa-report/,https://airwars.org/conflict/israeli-military-in-syria-the-gaza-strip/,https://airwars.org/conflict/palestinian-militants-in-israel/,https://newlinesmag.com/reportage/gaza-and-syria-a-tale-of-two-israeli-air-wars/,https://www.theguardian.com/global-development/ng-interactive/2021/jul/28/countdown-to-demolition-the-story-of-al-jalaa-tower-gaza-israel-palestine,,"Emily Tripp, Joe Dyke, Shihab Halep","Founded in 2014, Airwars is a London based international not for profit watchdog NGO, which specialises in tracking and understanding conflict civilian casualties at scale. Our high quality work monitoring belligerents in Iraq, Syria, Yemen, Somalia and Libya has won extensive plaudits, and has helped in particular to shake up US and European military understanding of civilian harm. The recent extraordinary series of New York Times investigations into civilian casualties from US actions drew heavily on Airwars research on Coalition actions in Iraq and Syria - including the building of an app by the Times that used Airwars assessments and locational analysis, to help their own field investigators to triangulate incidents of concern in Mosul.  The Israel-Palestine project submitted here represents many thousands of team hours. Our comprehensive monitoring of secretive Israeli actions in Syria - the most extensive public record available - builds on almost four years of ongoing monitoring. And our research and analysis of the May 2021 conflict was produced by the entire research and investigation teams at Airwars over a seven month period. It was the work of more than a dozen people including specialist investigators, researchers, analysts, data modelers, geolocators, advocacy officers, and web designers. In particular the project was led by: Emily Tripp - research manager Joe Dyke - senior investigator Shihab Halep - chief researcher",
Netherlands,"ORF, Apache, Deník Referendum, Mediapart, Der Tagesspiegel, AthensLive, Dublin Inquirer, IrpiMedia, Follow the Money (FTM), E24, Expresso, elDiario.es, Ctxt.es, Reflekt, Republik",Big,Shortlist,,Cities for Rent - Investigating Corporate Landlords Across Europe,28/04/21,"Investigation, Long-form, Cross-border, Multiple-newsroom collaboration, Quiz/game, Database, Open data, News application, Illustration, Infographics, Chart, Video, Map, Economy","Scraping, D3.js, Json, Google Sheets, CSV, Python","During a period of more than seven months, a team of over 25 investigative and data journalists and visualisations experts from 16 European countries, have been investigating the big players in Europe’s residential real estate. We found that the investments into rental flats increased with more than 700% between 2009 and 2020. We have visualised these trends and found that reports of negligence and abusive tactics by corporate landlords are consistent in the cities researched. We also found that often local governments don’t know much about the precise situation in their cities and the investment going into their housing markets.","Politicians, researchers, NGO’s, advocacy groups, and even real estate agents, architects and other housing professionals approached us forour methodology and data. Most notably, the Greens/EFA group at the European Parliament in preparation for a study about the financialisation of housing in Europe, demanding the introduction of transparency registries about corporate landlords. A Member of the Flemish Parliament currently working on this topic contacted our media partner Apache. Politicians also took our research into parliament. In Norway, there were reactions from politicians at the local and national levels, and the Finance Minister had to answer a question about the topic in Parliament. In Spain,an MP mentioned one of the articles in Parliament. Other colleagues were inspired by our research. The head of investigations at Al-Araby, a Qatar-based media outlet, contacted us when planning a similar research in Arab countries. A freelancer journalist in Atlanta, US, contacted us to tell us they are using our methodology as inspiration for research there. Dutch De Correspondent would like to follow up on our investigation and methodology to cover the situation in the Netherlands. Our work was cited in news reports and academic papers. In Norway, several media outlets published editorials about our project (Morgenbladet , Avisa Nordland, Kapital). The project was mentioned in the academic paper, “The value of the city. Rent extraction, right to housing and conflicts for the use of urban space”. The team members were invitedto talk about the research at (online) conferences and debates, among others Dataharvest - European Investigative Journalism Conference, VVOJ (Dutch-Flemish investigative journalism conference), or the Barcelona Housing and Renovation Forum. Our team members were interviewed for radio and podcast, notablyPUSHBACK talks with Fredrik Gertten and Leilani Farha, the former UN Special rapporteur on the right to adequate housing.","To collect the data we used multiple methods. Where scraping was needed, it was done with Python (requests & beautiful soup or selenium). Much information on the corporate landlords was sourced from yearly reports by a collaborative data collection in spreadsheet templates. Data analysis and cleaning was done mostly in Python (pandas) and excel. Using open source software focused on the privacy of our data is important to us. The actual collections, as well as the collaborative spreadsheets were therefore hosted in a Next Cloud environment on a server administered by an Arena team member. Our regular meetings were done via open source video meeting software BigBlueButton. For the publication of more than 30 interactive customizable graphics, we built a framework using d3, mapbox.js and reusable vanilla JS UI components.We had to accommodate a wide range of use cases from single iframes to pages with many graphics embedded directly in the article. We used code-splitting and on-demand loading of libraries to make sure to load as little code as possible while reusing code across graphics. All graphics were translatable and customizable (colors, highlighting data points, headings and texts) in a purpose-build backend built with Django REST-API with a frontend built using Vue and a UI framework (Vuetify) was used for the backend. Despite this backend, we made sure that the graphics could be hosted statically to ensure reliable hosting for all participating organizations. We chose to stay with commonly used types of dataviz, to focus on details (such as adaptive legends for maps that take into account the data visible in the current view of the map) and a good localization (CMS embed, number formatting, idiosyncrasies such as ""mil milliones"" in Spanish, translatability even of the data, e.g. the transliteration of company names into the greek alphabet).","Finding all the relevant data was a big challenge, as the sector lacks transparency in ownership and consolidated data sets on the city level. Therefore, to build our datasets we had to use a wide range of techniques. Some corporate landlords publish the listings on their website – we have scraped those. Some publish amounts of apartments they own in their yearly reports – we have extracted this data manually. However, some hardly publish anything at all. We have collected dozens of reports on real estate and rental markets from various sources (consultancy companies, banks), but most of these did not cover the cities we needed. In the end, most of our data on the actual names and investments of corporate landlords on the city level from a lengthy negotiations with a private business that collects data on real estate markets (Real Capital Analytics). The context data proved to be equally tricky. Eurostat collects a lot of data that is comparable across countries, but little on the city level. As living conditions and demographics in cities differs from the average country data, we could not use country data. We have dug into all the local statistical offices for context data but not every data point was comparable across the different sources. Methodologies for even seemingly simple measures as average household income proved to be very tricky (median disposable, net disposable, gross disposable, per adult, ...). It was even difficult to compare average rental prices. For example in Berlin, rent prices are including water. In the end we managed to make some of these data comparable. We found a dataset on cross-border comparable rental prices for our cities literally the day before the publication. Translatability and context-specific adaptability of the shared visualisations was also a big challenge (see section above).","Two elements of this collaboration stand out. By making this reporting cross-border, we could capture an European trend on the local level. Tenants learn that they are not alone: they read about the abusive practices and other troubles they face in their own city, but they can compare to other tenants across Europe. This is crucial, since politicians on European level can step in with regulation measures. This contextualisation was highlighted by the shared visual language of all publications. The shared data visualizations made possible by the custom back-end built by the Innovation Lab at Tagesspiegel allowed for each graphic to be translated and adapted to match the language and the look and feel of the publication medium. As the back-end used the same data for all the graphics, even last minute changes in the source data were not a problem, as they would be if every newsroom would create their own graphics. We have also commissioned an illustrator to create a series of illustrations for all the articles. The colour scheme of the illustrations was also adaptable. – in harmony with the visualisations. The second element was the post-publication knowledge directory (see at the “main link to your project”) we have created. Here we collected all the articles that were published within the project (68 up to date), and we also share our methodology, limitations of the available information, and a data catalogue. Here we describe for example the data availability and comparability, what to pay attention to, what are the problems we encountered, but also share links to resources elsewhere. As we were getting many questions about our methodology, this directory makes it easier for any journalist,s or researcher or student of journalism to pick up the topic and further develop the reporting and research themselves.","English, German, Norwegian, Italian, Spanish, Czech, Greek, Dutch, French, Portuguese",https://cities4rent.journalismarena.media/,https://dublininquirer.com/2021/04/28/company-landlords-are-promoted-as-more-professional-but-what-are-tenants-finding,https://medium.com/athenslivegr/cities-for-rent-b9d1bb2bba02,https://collaboration.journalismarena.eu/s/pBgcR6kmMD575GB,https://collaboration.journalismarena.eu/s/GGzY5Ppfk6iNc93,https://collaboration.journalismarena.eu/s/ZdZrC6pWAW56tCd,https://cities4rent.tagesspiegel.innovationlab.berlin/,,list sent to keng@sigmaawards.org,"Editorial and overall coordination: Jose Miguel Calatayud, Spanish freelance journalist and project director with Arena for Journalism in Europe, based in Berlin in Germany Data coordination: Adriana Homolova, Slovak-Dutch freelance data journalist and data coordinator at Arena for Journalism in Europe, based in Utrecht in the Netherlands Data visualisation coordination: Hendrik Lehmann, German journalist, head of the Tagesspiegel Innovation Lab, based in Berlin in Germany Data visualisation programming lead: David Meidinger, German journalist and web developer, lead developer at the Tagesspiegel Innovation Lab, based in Berlin in Germany Team members by country: - Alexandra Siebenhofer, Austrian journalist and editor at ORF, based in Vienna in Austria – Steven Vanden Bussche, Belgian journalist and editor at Apache, based between Merelbeke and Antwerp in Belgium – Gaby Khazalová, Czech journalist and editor at Deník Referendum, based in Brno in the Czech Republic – Bo Elkjær, Danish investigative journalist at Information, based in Copenhagen (contributed, but did not publish) – Alexander Abdelilah, French-German freelance data and investigative journalist, member of the WeReport collective and publishing with Mediapart, based in Metz in France – Sotiris Sideris, freelance data and investigative journalist, co-founder of AthensLive and data coordinator at Reporters United and publishing with both those media outlets, based in Athens in Greece – Lois Kapila, British journalist, co-founder, journalist and editor at the Dublin Inquirer, based in Dublin in Ireland – Alice Facchini, Italian freelance journalist publishing with IrpiMedia, based in Bologna in Italy – Peter Hendriks, Dutch journalist at Follow the Money (FTM), based between Utrecht and Amsterdam in the Netherlands (contributed, but did not publish under Cities for Rent name) – Steinar Rostad Breivik, Norwegian journalist and editor at E24, based in Oslo in Norway – Micael Pereira, investigative journalits at Expresso, based in Lisbon in Portugal – Peter Sabo, Slovak investigative journalist at Aktuality, based in Bratislava (contributed, but did not publish) – Manuel Gabarre, Spanish freelance lawyer, researcher and journalist publishing with elDiario.es and Ctxt.es, based between Jaca and Madrid in Spain – Christian Zeier, Swiss investigative journalist and co-fouder and editor at Reflekt, and also publishing with Republik, based in Zurich in Switzerland – Catherine McShane, British freelance investigative journalist based in London in the UK (contributed, but did not publish)",
Brazil,Folha de São Paulo,Big,Shortlist,,Analyzing the efficiency of questions in Brazil's largest college entrance exam and confronting the president's criticisms about the adequacy of items with a supposed ideological bias.,11/11/21,"Investigation, Quiz/game, Chart, Politics","R, RStudio, PostgreSQL","Enem, the second largest university entrance exam in the world, is taken by more than 3 million participants every year. Conservatives, including the current President, Jair Bolsonaro, argue that the exam fails to measure a candidate’s ability because of items that they perceive to be biased. The criticized questions are often those regarding the country's military dictatorship, women's rights and LGBTQI+ themes.We analyzed billions of responses from millions of students over the past 11 years to verify if these test items effectively assess their abilities and we found that Bolsonaro criticism is unjustifiable. ","Jair Bolsonaro, current Brazilian President, has been trying to change questions present in the Enem test since taking over the government. This effort has a technical argument as its formal justification: that certain items would not measure the students' proficiency and would only be on the test to promote ideological indoctrination. To our knowledge, this is the first time a group – journalistic or academic – has put these claims to test. Thus, the findings presented here, that the justifications presented by the president are false, are unprecedented. Furthermore,  we have shown that other items, which were not targets of Bolsonaro's criticism, are inefficient in the assessment of a students' knowledge. This conclusion was corroborated when we found that correct answers on these items were not considered for the final grade of the students. In other words, the model used by the institute responsible for the test (Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira - INEP) identified that these items were unfit to the point that getting them right did not give any information of how good the student was, leading to no increase in the final grade. None of the millions of test participants were aware of this at the time, since the parameters for calculating the grade are not disclosed by the government.  This article was the headline of the Folha de São Paulo newspaper, one of the largest and most traditional newspapers in the country."," Techniques:  Item response theory (IRT): To make the theorical model of expected answer from the students according to their abilities. Which we compared the empirical data (using Bock's (1972) chi-squared method). IRT was also used to to ascertain the ability of test's items to discriminate students according to their proficiency; Factor analysis: To check if students with more knowledge had more chance of getting the right aswer or not (using Crit coefficient);  Biserial correlation: To check if getting the right answer fom one it was correlated to get the right answer in others items; Web scraping: To get the links to download the data and to obtain images of the test items;    Tools and technologies:  Postgresql: To store the data from the students and the tests;   R: Used to obtain the data, make the analysis and plots and format the data; Adobe Illustrator: To edit, layout and ready plots for print","The evaluation of the quality of items in a test, that is, if it is efficiently measuring the knowledge of students, is not a simple task.  We have based our analysis on the Item Response Theory (IRT), a complex statistical model used to estimate the ability of students in different areas. This model is the same used by Enem to calculate students' grades.  Hence, our analysis is based on the same assumptions that the test uses.  Moreover, one of  our journalists  took a college course on the subject, with one of the greatest specialists in the field in Brazil, in order to obtain the necessary knowledge for the analyses Structuring the PostgreSQL databases was a complex operation, as it was necessary to store 12 billion responses from 70 million students who took the tests between 2009 and 2019. This structure was further hampered by the lack of standardization of the files from different years provided by the government. This work took 3 months. After this step, it was necessary to create codes in R to assess whether the items were able to discriminate students according to knowledge, that is, whether, for each item, the chance of getting it right was related to the student's ability in the area of ​​knowledge. Finally, there was extensive effort on coming up with a visual solution to best demonstrate the complexity of the theory in an understandable way for the reader. This includes a quiz with the unfit questions from the test, so the reader could understand how the candidates react to these and the problems with it. ","We believe there's value in translating technical knowledge as a way of doing journalism and contesting authorities’ declarations. This project started when we tried to understand how the government calculated students’ grades. In doing so, we faced difficulty in understanding the extremely specific technical knowledge. Although this would often be reason enough to abandon a task, we did not let ourselves be intimidated by it and sought to understand the model in depth.  We understood that this knowledge provided the tools to assess the quality of the tests' items and that with it we were able challenge the narrative  that the current federal government tried to impose.",Portuguese,https://www1.folha.uol.com.br/educacao/2021/11/questoes-do-enem-na-mira-de-bolsonaro-sao-eficientes-em-testar-conhecimento.shtml,https://www1.folha.uol.com.br/educacao/2021/11/questoes-inadequadas-compoem-2-do-enem-e-nao-contam-para-nota.shtml,https://arte.folha.uol.com.br/educacao/2021/veja-questoes-mal-elaboradas-do-enem-e-teste-suas-chances/,https://github.com/deltafolha/enem/blob/main/metodologias/metodologia_itens_polemicos.md,https://github.com/deltafolha/enem/blob/main/metodologias/metodologia_itens_inadequados.md,,,,"Daniel Mariani , Diana Yukari , Paulo Saldaña, Flávia Faria and Fábio Takahashi","Daniel Mariani: Data journalist at Folha de São Paulo. Graduated in Biological Sciences from the University of São Paulo.  Diana Yukari:  Information designer at Deltafolha, Folha de São Paulo's data journalism team. Flávia Faria: Editor of DeltaFolha, the newspaper's data journalism team. Previously, reporter for local news and politics.   Paulo Saldaña: Education journalist at Folha de São Paulo. Graduated in journalism from Faculdade Cásper Líbero, founder and director of Jeduca (Association of Education Journalists). Fábio Takahashi:  Former editor of DeltaFolha,Folha de São Paulo's data journalism team. Founder of Jeduca (Association of Education Journalists).  ",
Netherlands,Pointer (KRO-NCRV),Small,Winner,"Pointer, based in the Netherlands, infiltrated Telegram groups to uncover a troll army that was spreading misinformation about COVID vaccinations.  The reporters went undercover for three months to identify themes and analysis on the Telegram messages, then used OSINT techniques to find the organizers of the group called The Troll Army.",The Digital Army,20/05/21,"Investigation, Database, Open data, Fact-checking, OSINT, Illustration, Video","Animation, Personalisation, Scraping, Json, R","Pointer infiltrated a Dutch troll army of over 750 members. In a Telegram group disinformation about the corona vaccine and anti-corona demonstrations is shared. Thanks to social media analysis, Pointer finds out which accounts are involved in this troll army. On a daily basis, they share the links that are shared through this group. Pointer went undercover in this group, discovered the accounts behind it and found out who is driving this troll army. Publications include an online scrolly production, a news article, a making-of and a TV broadcast.","With this project, we were the first to encounter a Dutch-speaking troll army and analyze it from within. We presented our research in a number of different forms to reach different audiences including an online scrolly, news article, making-of and a TV broadcast. The latter had the most impact because we showed on TV how these groups were organized. It did very well for Dutch Television, attracting 370,000 viewers with competition from the Eurovision Song Contest. The three largest Dutch news media took over our news including public broadcaster NOS, RTL and the nieuws site nu.nl.","For our investigations into misinformation, I keep an eye on several Telegram groups. Through some searches on the open web, I end up with a spider web of strange Telegram groups: pedo hunters, dark markets, expo's groups, and the group called The Digital Army.  We wanted to follow this troll army for three months to see how they were organized. During this period, we scanned all the posts in the group several times to accurately analyze the conversations. This gave us a good idea of who the main characters are in this group. Through the data analysis in R we also got an idea which topics were often discussed, and which urls were shared.  Often these were links to websites that had not been distributed before. We also scrapped Twitter to see which accounts were spreading this misinformation right after it was shared in the troll army. Again, you can compare the language used by the group and social media accounts side by side. Over time, you can link a number of accounts together, and estimate the size of the troll army. In addition to this data analysis, we conducted an extensive OSINT investigation of the three protagonists. This constantly revolves around linking information and data we find online. For each of the three protagonists, we created an extensive research document, in which we place each clue. Through OSINT investigation, we were able to link the founder, leader and most loyal follower of this troll army to individuals with 100% certainty.  Via the open web you can search for invite links on Telegram. Then it also becomes clear how Telegram groups are structured: you can find all kinds of dubious groups via a kind of Starter-page like channels with countless links. The Digital Army is one of them.","The OSINT investigation provided the most headaches. These individuals want to keep themselves anonymous, but unwittingly give away some information. For three weeks, I churned out countless leads. Sometimes it was about a specific photo: if I could find out where it was taken, you get closer to where that person lives. Or I could say with certainty what someone's social media profile is: then you can use that to find information about someone's email address or phone number. Each clue leads to the next (half) clue. And the ultimate goal is to find out someone's name, hometown and contact information. With a lot of sleuthing, dead ends and luck, we were able to unmask the three main characters. We spent about three weeks on this OSINT investigation. For The Digital Army, we went undercover in a Dutch troll army for three months. Some people will know troll armies as the Russian factories where dozens of people behind countless screens and social media accounts bombard the world with misinformation. However, this is a completely digitally organized troll army. The participants barely know each other beforehand, and try to feed each other new tactics and messages via the social media app Telegram. This is the period just after Twitter and Facebook started banning all kinds of conspiracy theorists from their platform. Not much had been written about Telegram and misinformation, and certainly not from within. Through an innovative way of storytelling we take the viewer into the chat group of The Digital Army. To experience how it's like to be a part of these kinds of Telegram groups. Especially now, during the pandemic we find it important to show which persons are behind the spreading of disinformation and misinformation.","In 2015, an office building in St. Petersburg was still the image we had of a troll factory: a physical office building where dozens of people with hundreds of fake accounts try to steer public opinion. But in 2021 that image is outdated. Coordinated troll armies are now much easier to organize digitally. And that includes The Digital Army. Pointer discovered this troll army in early 2021, and spent some time investigating how they spread disinformation about the corona vaccine. In addition to online campaigns, they also created and distributed actions to physically intimidate mental health professionals, politicians and scientists. With every big investigation Pointer publishes a making-of article. Other journalists will thus get an inside look at how we work at Pointer. Often with the described methodology and challenges we face during the research process. This time we exposed the tactics of these misinformation spreading groups. This may also give other journalists ideas for their own investigations. Although these 'behind-the-scens' articles are being read by very few people compared to our fancy online scrollytelling articles or TV broadcastings, we find it important to be transparant about how we work on our investigations.",Dutch,https://pointer.kro-ncrv.nl/digitaalleger,https://pointer.kro-ncrv.nl/het-digitale-trollenleger,https://pointer.kro-ncrv.nl/nederlands-trollenleger-verspreidt-en-coordineert-desinformatie-over-vaccin,https://pointer.kro-ncrv.nl/hoe-is-het-digitale-leger-opgebouwd,https://pointer.kro-ncrv.nl/system/files/2022-01/pointer_digital_army_en_translation.pdf,https://docs.google.com/document/d/1dkLphevOQ-dIVtyLrpldE-xXKtobkDS9f9QUbDOxjhE/edit?usp=sharing,,,"Jerry Vermanen, Wendy van der Waal, Marije Rooze, René Sommer","For The Digital Army data- and investigative journalist Jerry Vermanen did al research, investigation and text voor de articles. He was also featured in the TV broadcast.  Had his first job in data journalism at NU.nl. Jerry doesn't care much for opinions and gut feelings: that's for in the pub or on social media. He prefers to talk about what really matters: facts. With data analysis, OSINT and old-school journalistic skills, he found 82 Dutch child pornographers, found out where Dutch weapons end up after resale and corrected officers who published too much personal information on the Internet. Jerry got help from designer Wendy van der Waal. She designed the scrolly production, all the header images and created the design for the TV broadcast. Developer Marije Rooze brought the research, imagery and (UI) design together. Editor René Sommer was there to keep everything on track and edited all the text and TV broadcast.      ",
Brazil,Fiquem Sabendo,Small,Shortlist,,Revealing the Brazilian military pensionists for the first time in Brazil,28/06/21,"Investigation, Multiple-newsroom collaboration, Database, Open data, News application",R,"Brazil's FOIA came into force in 2011 to ensure the right of access to information from the federal government and the federative units, but only the salaries of active public servants were available on the Transparency Portal, excluding inactive ones. Fiquem Sabendo has been fighting to open this data in the last 4 years. In 2020 we've managed to open civil's inactive pensions and, in 2021, we've made the government open decades of data about military pensions in Brazil, a black box that has never been opened before. More than 70 exclusive stories were published in Brazilian and international media","More than 70 stories were published about military pensions based on our work, including articles and Opeds published in every major news outlet in Brazil. This data has been hidden for decades. This story explains in detail the importance of opening this data and how we did it: https://latamjournalismreview.org/articles/journalism-brazil-freedom-of-information-act-pensions/ The stories published based on the data we revealed show, for example, that the children of Brilhante Ustra, a famous Brazilian torturer, still receive military pensions - https://brasil.elpais.com/brasil/2021-07-06/governo-paga-12-milhao-de-reais-por-mes-a-herdeiras-de-militares-acusados-de-crimes-na-ditadura.html Another story revealed that many pensioners have their own millionaire companies but still receive money from the government - https://www.metropoles.com/brasil/registros-mostram-400-filhas-pensionistas-de-militares-como-socias-de-empresas-milionarias In another story, we show that more public funds are spent in military pensions that in social benefits for poor people in Brazil - https://piaui.folha.uol.com.br/as-pensoes-e-os-bilhoes-da-familia-militar/  Folha de Sao Paulo, Brazil largest newspaper, wrote an Oped based on the data we found saying that the pensioners payment  policy shouldn't exist anymore -  https://www1.folha.uol.com.br/opiniao/2021/07/custoso-anacronismo.shtml    ","We went to court (Tribunal de Contas da Uniao / TCU) with a lawyer to make the government open the data, filed dozens of FOIA requests every month to get updates on how they were working to open the data and, when the data was released, used R and Shinyapps to make hundreds of gigabytes easy to use and find pensionists names - https://fabdev.shinyapps.io/graphs_on_demand/ We've organized a pool of journalists from dozens of different Brazilian and international newsrooms to public the stories using the data we found.","There were two main problems: making the government open the data even after the court decision. They have tried many different ways to keep hiding information, like disclosing only pensions related to civil servants and hiding the military, saying that military servants don't have to disclose their benefits, postponing the disclosed of data every month, etc. The second problem was to understand the data and make it available to everyone in a way that journalists would immediately start asking questions and writing their stories instead of spending time to understand particularities from the dataset. There were lots of mistakes in the original database, so we had to speak with people from the govermnet many times, schedule meetings and make them fix everything before actually making the data available.",Collaborative journalism is a key to get important information and make it relevant to society. Sometimes the scoop is not the only (or the most) important thing in a story.,Brazilian Portuguese,https://fiquemsabendo.substack.com/p/nova-denuncia-da-fiquem-sabendo-obriga,https://latamjournalismreview.org/articles/journalism-brazil-freedom-of-information-act-pensions/,https://fabdev.shinyapps.io/graphs_on_demand/,https://fiquemsabendo.substack.com/p/pensionistas-devedores,https://www.metropoles.com/brasil/registros-mostram-400-filhas-pensionistas-de-militares-como-socias-de-empresas-milionarias,https://apublica.org/2021/07/governo-paga-r-12-milhao-por-mes-a-herdeiras-de-militares-acusados-de-crimes-na-ditadura/,https://www1.folha.uol.com.br/opiniao/2021/07/custoso-anacronismo.shtml,,"Bruno Morassutti, Maria Vitória Ramos, Leo Arcoverde, Luiz Fernando Toledo, Taís Seibt, Fernando Barbalho",https://fiquemsabendo.com.br/nossa-equipe/,
Australia,"101 East, AJLabs, Al Jazeera Media Network",Big,Winner,"Al Jazeera's 101 East team used forensic investigation techniques to put cases of alleged torture, mysterious deaths, disappearances and detention in Myanmar under a spotlight after the 2021 coup. Its unique testimonials from detainees, field reporting and satellite technology found one secret facility in a military compound outside Yangon. With its partner, Forensic Architecture, the team gained access to previously unseen footage of a politician who died in custody – evidence that was later shared with the UN agency investigating atrocities in the country. The team also published a memorial listing nearly 7,000 people arrested during the first six months of the coup.",This is Myanmar's State of Fear,10/08/21,"Investigation, Long-form, Satellite images, Politics, Human rights","3D modelling, Adobe Creative Suite, Google Sheets","A climate of terror has engulfed Myanmar since the army seized power in a coup during February, 2021. The digital interactive, “This is Myanmar's State of Fear”, produced by Al Jazeera’s 101 East team is a forensic investigation into the military machine behind the violence and fear sweeping the authoritarian Southeast Asian nation. The multimedia piece puts disturbing cases of alleged torture, mysterious deaths, disappearances and detention without charge under the spotlight.  State of Fear” features rare interviews with army defectors, people who lost loved ones in post-coup violence and politicians from the deposed government now in hiding.","Using rare detainee testimonies, field reporting and satellite technology,  the team uncovered a secret facility allegedly used for interrogation and abuse located in a military compound on the outskirts of Myanmar’s largest city Yangon.  Al Jazeera worked with investigative agency Forensic Architecture to locate and digitally recreate the centre. They showed how the facility was rebuilt weeks prior to the coup.    Before publication,there was scant knowledge of the exact location of the facility and its existence was little more than a rumour. This digital reconstruction informed society of what was going on inside this facility, where hundreds of people allege they have been detained and abused Some dissidents have been killed in mysterious circumstances after being detained by the authorities. The reporting team gained access to unseen footage of a politician who died in custody which revealed severe injuries to his body which was drenched in blood, undermining the military’s official account that he died of a heart attack in custody. Although rumors about his death were widely shared online, this investigation shed new light on the event with hard evidence. With the foreign media’s access to Myanmar severely limited, most media organizations have struggled to cover this challenging news story and been forced to rely on foreign academics or diaspora voices as commentators. The unique strength of this reporting prompted the UN agency responsible for examining atrocities in Myanmar, the IIMM, to ask the 101 East team to share their evidence and witness testimonies to help with their investigation.  Sky News UK, TRT World and other media outlets have also reported on the key findings of 101 East’s investigation.  The combination of compelling first-hand accounts and forensic investigation makes the team’s unique coverage of the Myanmar coup and its aftermath worthy of a Sigma Award.  ","This digital interactive piece featured audio interviews and eyewitness accounts captured by a reporting team based in Myanmar.  The project also features timelines, data visualizations and other multimedia which have been curated in a scrapbook style to mimic the nature of the investigation. This content and its presentation helps the user to understand the context of what is currently happening in Myanmar and the process of information gathering. The team chose to publish the names of  6,994 detainees and their occupations - a digital memorial of those arrested during the first six months of the coup. This choice of presentation urges readers to understand the scale of the issue, whilst also dignifying the individuals who have been detained.   Photos and videos sourced directly from families of victims and from across social media were used to create a digital memorial of people allegedly killed by torture. The team handled this graphic content with sensitivity, highlighting the impacts of the coup on individuals without gratuitous violence. This piece was coded in AMP, a lightweight language to ensure quick load times regardless of hardware, to ensure this project reaches Global South digital audiences with limited internet bandwidth. The interactive ‘This is Myanmar's State Of Fear’ has been widely shared on Myanmar social media platforms.   ","The 101 East team gathered the core evidence for this online investigation in an incredibly difficult and repressive reporting environment.  In the aftermath of the coup, the junta has sought to prevent media coverage of events in Myanmar. It has closed independent television stations, online news outlets and newspapers, and arrested scores of journalists. The authoritarian regime has already convicted at least six journalists for violating a new provision of the penal code that makes it a crime to publish or circulate comments that they claim spread “false news” or “cause fear”.  Producing this digital project was an ambitious, risky endeavour yet vital to informing the world about what is happening under military rule in Myanmar. This reporting was spearheaded by Ali Fowle, a correspondent who spent nine years in Myanmar and was on the ground in the country's biggest city, Yangon when the army takeover occured.  Using her extensive contacts, Ali interviewed deposed politicians in hiding, former detainees and their families as well as demonstrators who feared imminent arrest.  She documented army brutality as the death toll rose, faced the constant risk of arrest and navigated internet shutdowns to produce this unvarnished, yet nuanced coverage from the frontlines. The 101 East team won the trust of sensitive interviewees who feared jail or death for speaking out against the military. The production team found secure locations to ensure that these interviews could be conducted safely. Ali’s long-term connections with activists and intimate knowledge of the country allowed her to navigate this sensitivity and obtain access other foreign media were unable to manage.  ","This quick turnaround digital piece was created in less than two months and capitalized on reporting materials gathered by an investigative unit of television reporters who made two longform documentaries after the coup in Myanmar.  Journalists can use ""This is Myanmar's State of Fear"" as a blueprint to reimagine television content for a digital audience.  It should be noted that Myanmar is one of a number of repressive reporting environment in the world right now.  Other news organizations could use this digital interactive as a model to report in authoritarian nations where press freedom is non existent and media access is difficult.  ""This is Myanmar's State of Fear"" is also a model of important international reporting and clearly puts the current issues into context for an audience unfamiliar with the politics of the country using timelines and clear explainers.    With human rights abuses occurring daily across Myanmar, investigations such as those done by Al Jazeera's 101 East team are vital examples of public interest journalism, providing a voice to the voiceless and holding the military junta to account. Combining corporate and forensic inquiry with field reporting, this interactive is an agenda-setting piece of digital journalism which serves as a key investigation into human rights abuses following Myanmar’s coup.  With the crackdown showing no signs of abating, this piece of online journalism is a powerful examination of how civilians have vowed to keep fighting for democracy, whatever the cost.","English, Burmese",aje.io/myanmarcrackdown,https://interactive.aljazeera.com/aje/2021/myanmar-state-of-fear/index.html,,,,,,,"Rhiona-Jade Armont, Ali Fowle, Drew Ambrose, Nick Olle, Aun Qi Koh, Liz Gooch, AJLabs, Forensic Architecture, David Boyle, Jenni Henderson, Forensic Architecture","101 East is Al Jazeera English’s weekly current affairs programme about issues in the Asia-Pacific.  The documentary program launched in 2006 and operates out of Al Jazeera's Asia bureau in Kuala Lumpur, Malaysia. They have produced an impressive range of documentaries about Myanmar since 2011, when the Southeast Asian nation transitioned from army rule to a civilian government.   Aside from television programs, 101 East produce in-depth multimedia pieces, virtual reality videos, gamification, comic book storytelling and data visualizations to engage web audiences.  101 East’s digital journalism have won the SOPA award (Hong Kong), a Venice TV Award (Italy), two Overseas Press Club of America citations, a Society of Digital News Design Award of Excellence (USA) and was highly commended twice at the Drum UK Online Media Awards.    ",
United Kingdom,"The Courier, Press and Journal",Big,Shortlist,,"The 6,089 Dundee WW1 victims: Search their names, ages, ranks and addresses",03/11/21,"Investigation, Database, Open data, Chart, Map, Culture","Animation, D3.js, Adobe Creative Suite, Microsoft Excel, Google Sheets, CSV",For Remembrance Day we honoured the fallen by geolocating and mapping their home addresses and turning it into a scrollytelling experience telling their stories. We also included a map our readers can explore and a searchable table as well as a map of where they were laid to rest for any of our readers who wished to pay their respects. There were a number of side stories going into more depth on the tragic stories we uncovered in the dataset.,"It is now three months since this piece was published and we still regularly receive emails from family members thanking us and sharing the stories of their family members with us. We've gone on to publish follow up articles based on these including one where a family member provided us with copies of letters their fallen family member had sent during the war. We've also had correspondence with people in the education sector - inlcuding one teacher who works on a Ministry of Defense campus and sought our permission to use the piece as part of her curriculum. We are a very new data team (only formed in March 2021) and we have also found that this piece has had a significant impact internally in being more creative with use of data. Poppy Scotland, the official Scottish Remembrance charity had the following: “This map is a powerful reminder of how every single community has been impacted by the cost of conflict over the past 100 years. Each poppy symbolises the bravery and sacrifice of those who fought and died in defence of our nation.","The original data was sent to us in excel format but as it was hand transcribed data based on records that were a hundred years old it required a lot of cleaning. For this stage we utilised Open Refine to cluster and clean the most common misspellings. To geolocate the data we used a Google Apps Script pinging off the google map API. As a relatively small newsroom we generally try to do as much for free as possible so this was geocoded ion batches of 1,500 per day (the free limit). Due to the age of the address data we then had to sift through the data for streets that no longer exist. The original design idea behind the project was based on an offhanded comment that from afar it would look like 'a field of poppies'. The poppy icon was designed in figma and it was designed to have similarities between the Scottish and Canadian official poppy icons to honour the many Scottish men that served in the Canadian military. The stalk of the poppy was a deliberate choice to add on to the 'field of poppies' aesthetic. The maps and scrollytelling were done using flourish and javascript. The poppy SVGs for the pictogram were also designed using figma.","We have a motto in our data team - ""NYT dreams on a DCT budget"" and this project is the epitome of that. Between our titles we had more than 11,000 records to clean and geolocate and as stated previously we tend to try to avoid having to pay for resources where possible so the addresses were located in chunks of 500 per day (per data team member - so 1,500 per day). As a small data team that also provides a service to the rest of the newsroom to create charts for their stories, it was often difficult to allocate the time required for manual scrutiny of the data, but when we did we found ourselves sharing fascinating story after fascinating story. As a group of millennials we had never felt andy particular connection to the tragedy of WW1 but trawling through this data and ouring our hearts and souls into creating something that would truly honour them has brought that connection to us - and we hope that connection is felt in our readers in our presentation of the data.",Data journalism is not just about news. Every now and then it's good to take modern data visualisation and journalism principles and apply them to a more feature based project.,English,https://www.thecourier.co.uk/fp/courier-investigations/2703714/dundee-ww1-victims/,https://www.pressandjournal.co.uk/fp/pj-investigations/3598986/aberdeen-ww1-victims/,,,,,,,"Lesley-Anne Kelly, Emma Morrice, Joely Santa Cruz",The DCT Media data team are passionate about making data more accessible to our readers. We're dedicated to making our analysis transparent and will publish our full data and methods  wherever possible. We use a blend of traditional reporting  and data visualisation and strive to be at the cutting edge of data-driven storytelling methods. Lesley-Anne has been working with data for well over a decade. She was born in Glasgow and moved to Dundee when she was 17 and never left which means she has lived in Dundee for 51% of her life so far.Loves a good spreadsheet but prefers R and dabbles in python and javascript where needed. In 2020 she completed a masters course in data visualisation. Emma originally studied law but a love of writing got her into journalism instead and after completing a masters course she worked as a news reporter for four years before moving into data journalism. She has lived in Aberdeen her entire life.Her favourite part of the job is working on longer projects and seeing big ideas come to life. She also likes learning new ways to visualise data. Joely has a science and data background and recently made the jump to data journalism. She is most happy when getting stuck in to big spreadsheets and experimenting with data analysis. Joely is based in London but lived in Dundee as a child so has come full circle by joining the Dundee team. Outside of work you will find her playing tag rugby (enthusiastically if not well) and expanding her circle of dog owners to go out walking with.,
France,"Disclose,Interprt,Princeton",Small,Winner,,Mururoa Files,09/03/21,"Investigation, Long-form, Cross-border, Multiple-newsroom collaboration, Open data, OSINT, Satellite images, Environment, Health","Animation, 3D modelling, AI/Machine learning, Python","Disclose and Interprt, in collaboration with the Science & Global Security program at Princeton University (USA), investigated the consequences of atmospheric testing in French Polynesia for two years. With the help of thousands of declassified military documents, hundreds of hours of calculations and dozens of unpublished testimonies, this investigation demonstrates for the first time the extent of the radioactive fallout that struck the inhabitants of this vast territory as the 'Europe.","After the publication of our investigation, demonstrations took place in French Polynesia to demand an apology from the French state. A summit was organized in Paris to bring together representatives of the French state and Polynesia to improve compensation for victims and to facilitate access to military archives during this period. The President of the French Republic visited Polynesia in July and acknowledged that France had ""a debt"".  The State has announced the opening of the military archives on nuclear testing. A commission has been set up to provide assistance with the preparation of compensation files for victims of nuclear testing, the clean-up and deconstruction of former nuclear sites  ","According to our calculations, based on a scientific reassessment of the doses received, approximately 110,000 people were infected, almost the entire Polynesian population at the time. Modelling toxic clouds to support, we also unveil how the French authorities have concealed the true impact of nuclear testing on the health of Polynesians for more than fifty years. The scientific visualizations presented on this platform were produced through the extensive data mining and analysis of declassified French documents and open access government sources.The reconstruction of fallout patterns and cloud trajectories from French nuclear tests was done using the US NOAA Hybrid Single-Particle Integrated Trajectory (HYSPLIT) particle transport and dispersion model and meteorological data from the NCEP/NCAR Reanalysis (1948 - present) project. Initial stabilized radioactive clouds were represented as vertical linear sources with activity adequately distributed among the cap, skirt and stem of the cloud. The cloud particle sizes were assumed to be log-normally distributed. Calculations were run on a 12-core linux machine and involved the release of 3,000,000 3D particles each.  The hypothesis and input data of these studies were crossed checked with information available from the declassified historical documents as well as information available from the scientific literature.","For two years, nuclear physics researchers, designers, architects and journalists worked together on this investigation.  This international project was carried out between Princeton University in the United States, the architects' collective ""Interprt"" working on ecocides in England and Norway, and the French investigative media Disclose. This collaboration was made extremely difficult because this team of about ten people could never meet in real life, due to the covid-19 crisis. The other main challenge was to develop a technological model to virtually recreate the French nuclear tests in the laboratory using the powerful computers at Princeton.  Once these scientific calculations were made, the designers and architects modelled in 3D the trajectory of the radioactive clouds and their fallout on the islands of French Polynesia. They also modelled the precise level of fallout on a village and the contamination of food and inhabitants.  On the other hand, the journalists investigated French state military documents, met with sources involved in the nuclear tests and went to the field to meet victims, find documents hidden by the state about the health impact. This project deserves to be selected because it is unprecedented in its method. It brings together the methods of scientific investigation, 3D modelisation, forensic architecture and journalism to strengthen the evidence. Science supports journalistic work to produce rigorous investigations. ","This investigative story shows how international collaboration on sensitive issues can be done completely remotely, using tools such as Slack to coordinate the progress of projects, organize videoconferences on Meetjitsi (encrypted video) and groups on Signal.  Above all, it teaches how different methods of investigation can complement each other: the search for evidence in science is stricter than in journalism. It also enhances the accuracy of an investigation, as journalists find themselves integrating the precise methodology of scientists. It is not easy to get scientists, architects, designers and journalists to work together. The working methods are not the same, so we have to agree on a common method.   The investigative hypotheses put forward by the journalists are in fact confirmed or not by the scientific experiment, then by the 3D modelling. On the one hand, journalists have access to sources to which scientists would not have had access. And the scientists have access to documents that are not accessible to journalists. This project will hopefully lead to more such collaborations in the years to come.","English, French",https://moruroa-files.org,https://www.youtube.com/watch?v=esN7mvqBP1Y,,,,,,,"Sébastien Philippe,Tomas Statius, Mathieu Asselin, Nabil Ahmed,Olga Lucko, Svitlana Lavrenchuk","Sébastien Philippe is an Associate Research Scholar at the School of Public and International Affairs with the Program on Science and Global Security. He is also an associate fellow (2019-2021) at the Nuclear Knowledges Program at Sciences-Po, Paris. His research focuses on the monitoring and verification of international agreements, the reconstruction of past nuclear weapon activities (nuclear archaeology), and the impact of emerging technologies on the safety, security, and vulnerability of strategic nuclear forces.  Tomas Statius is an investigative journalist working for several newsroom Libé, Disclose, Mediapart. Nabil Ahmed is a transdisciplinary scholar and writer. He leads INTERPRT, an environmental justice project that investigates and advocates for the criminalization of ecocide under international law. Olga Lucko leads INTERPRT’s spatial research and design projects. She has been responsible for developing visual forensic evidence for the past five years. She is an ARB registered architect with experience working on public building projects in the UK with renowned practices Tim Ronalds Architects and Clash Architects. Slitvana Lavrenchuk is responsible for geospatial data analysis and design. She has an MArch from the Royal College of Art. Her research interests include interdisciplinary responses to climate change in the arctic. Mathieu Asselin is a freelance photographer. Before working on Mururoa Files, he takes on the daunting task of exploring the controversial and infamous agricultural company, Monsanto, through investigative photography,     ",
Germany,Süddeutsche Zeitung,Big,Shortlist,,"Green, Yellow, Corona",10/12/21,"Illustration, Infographics, Politics, Health","R, RStudio","In the second pandemic winter, schools in Germany remain open, despite high infection figures. Students are crowded into classrooms, protected only by FFP2 masks and regular testing, because the vaccination rate among children is still low. How high is the risk of infection in classrooms, and how well can simple ventilation help reduce it? Journalists from the Süddeutsche Zeitung (SZ) measured the infection risk in classrooms, daycare centers and at family gatherings using CO₂ measuring devices. In charts and graphics, they illustrate how quickly the aerosol concentration rises and how only consistently opened windows can reduce the risk of infection.","The project is a visually narrated paid article and was very well received by SZ subscribers. Many non-subscribers also clicked on the article, which is why it also led to new conversions. ","The analysis of the CO₂ values of the different scenarios is based on measurements that the Süddeutsche Zeitung carried out with two different CO₂ traffic light models. They differ in design, but both have the same sensor installed. The SZ was provided with one model free of charge by the company ISIS IC. Measurements were also taken with a self-built CO₂ traffic light following instructions from the Umwelt Campus Birkenfeld. In addition, we received data donations from the Michaeli-Gymnasium in Munich. Students Simon, Felix and Samuel from the high school measured carbon dioxide concentrations in various classrooms as part of a ""Jugend forscht"" project and won the special ""Thinking Safety"" prize.   The CO₂ limit at 1000 parts per million (ppm) is based on assessments by the German Federal Environment Agency. Here, 1000 ppm of carbon dioxide in the room air is considered harmless and a room air with 1000 ppm to 2000 ppm of carbon dioxide is considered alarming.   To calculate the probability of having an infected child in a class, we assume that twice as many children are infected as officially known. Although regular rapid tests can detect some infected children, they cannot detect all, especially since children more often show few symptoms of illness and rapid tests have limited accuracy. The probability of infecting more children is based on an interactive calculator from the Max Planck Institute for Chemistry. In the scenarios presented, we assumed a classroom with one teacher and 25 students, 50 percent of whom are vaccinated or recovered, and who spend 25 hours together in a room over the course of a week.  ","To obtain the data, the journalists performed several measurements themselves over a period of one year and evaluated them together with data donations from schools and interpreted them with the support of experts. The Data team worked together with colleagues from the visual desk and science editorial team, which required cross-departmental planning, to present readers with a cleanly researched and well-visualized story. The evaluation underpins the current debate on mandatory vaccinations, student vaccinations and school closures with easy-to-understand scientific insights and offers guidance on safer indoor meetings through proper ventilation. ","This project shows how sensors can be used in a meaningful way to add scientific insight and data to a debate currently taking place in society. The story takes the reader through an everyday problem - worrying about children at school during a pandemic - but also shows why the risk of infection in a room rises quickly, even in small groups, and how this can be reduced quite significantly by proper ventilation. It also shows the great potential of collaborations between multiple departments and teams even within the same media house, because projects like these are made possible through working together.",German,https://www.sueddeutsche.de/projekte/artikel/wissen/mit-co2-ampeln-gegen-corona-e452127/,https://drive.google.com/file/d/1Ez3PG5rTa5qx4w5amlUZz2Yf3FZrcpsm/view?usp=sharing,,,,,,,"Sabrina Ebitsch, Maria-Mercedes Hering, Berit Kruse, Sophie Menner, Sören Müller-Hansen and Julia Schubert, also involved were Felix Hütten, Florian Peljack, Florian Kaindl, Christian Helten, Niklas Keller, Julian Hosse, Simon Groß and Jakob Wetzel","Süddeutsche Zeitung's award-winning Data Team was established in 2018 to focus on data-driven reporting across all topics, with close links to the Investigations team (known for the Panama Papers, among other stories). Although it is a standalone unit of the newsroom, the team always works very closely with specialist editors from the other departments, graphic designers and developers, combining their expertise and skills with their own to achieve the best result for readers. As the team sees it, data journalism is first and foremost journalism - its goal is to tell stories, explain complex issues, expose injustice and corruption. The team is committed to constantly learning, experimenting with new tools, sources and storytelling formats, and providing the best possible experience for Süddeutsche Zeitung readers - online and in print. Exchanging ideas, sharing knowledge and learning from each other are important pillars of the philosophy. Towards their readers, they try to be as transparent as possible - publishing detailed descriptions of their methodology, source code and raw data wherever possible. ",
Switzerland,Republik Magazin,Small,Shortlist,,She is pretty. He is strong. He is a teacher. She is kindergarten teacher,19/04/21,"Investigation, Explainer, Chart, Politics, Culture, Women","AI/Machine learning, CSV, Python","Google Translate used to translate texts in a way that cemented gender stereotypes. Then vowed to do better. We tested how well the algorithm improved using Finnish sentences (the grammar of which does not code for gender) and translating them to German.  Finnish «hän on kaunis» ('he/she is beautiful') becomes, for example, «sie ist schön» ('she is beautiful') in German. Profession names as well as adjectives were translated in a heavily stereotyped way. We explain how this comes to be (algorithms are trained in certain ways) and what could be done to amend it. ",The project was well-read on Republik Magazin's webpage (www.republik.ch) and particularly well-shared in social media.,"We hand-selected and hand-checked original sentences to be translated, then used Python to automatically translate them via Google Translate's API and then calculated the percentage of translations as he/she.","Designing the experiment and creating a well-balanced (but small, of course, since we hand-checked or hand-coded all the sentences) language corpus.","How broad the field of «data journalism» can be: Sometimes the interesting stories do not necessarily lie in huge datasets and do not have to be presented in fancy visualisations. Sometimes some knowledge, a good idea and a fitting experiment design produces an insightful story.",German,https://www.republik.ch/2021/04/19/sie-ist-huebsch-er-ist-stark-er-ist-lehrer-sie-ist-kindergaertnerin,,,,,,,,"Marie-José Kolly, Simon Schmid","Marie-José Kolly is a journalist at the online magazine ""Republik"" where she focuses on science and data journalism. After studying german language and literature and mathematics in Bern, she earned a doctorate in linguistics in Zurich and Paris. She teaches at the University for Education and spends most of her free time with books or in the snow. She lives in Zurich with her partner. Simon Schmid is a journalist. At the online magazine ""Republik"" he is Co-Head of the «Business, Science and Digital» desk. He studied sociology in Basel and economics in St. Gallen and completed further training in data journalism in New York. Schmid teaches data journalism at the MAZ in Lucerne and Storytelling with data at the FHNW in Brugg. He spends his free time hiking, biking and skiing. He lives with his family in Zurich.",
United States,"Grist, The Texas Observer",Small,Shortlist,,Waves of Abandonment,05/04/21,"Investigation, Long-form, Multiple-newsroom collaboration, Open data, Chart, Map, Environment","Animation, AI/Machine learning, Drone, Scraping, QGIS, Json, Adobe Creative Suite, Microsoft Excel, CSV, R, RStudio, Python","Volatile oil prices are setting the stage for fossil fuel companies to abandon oil and gas wells en masse, particularly in the Permian Basin straddling Texas and New Mexico. Those two states are already responsible for cleaning up about 7,000 such abandoned wells, a task that will cost at least $335 million. New statistical models developed by Grist and the Texas Observer predict the states could soon find themselves saddled with an additional 13,000 wells in need of cleanup — with a true cost that is closer to $1 billion. This project was supported by the Pulitzer Center.","The orphan well problem has received considerable attention at the local, state, and national level since the publication of our series. Regulators were receptive to our findings, and advocates and industry veterans alike sought to translate our methods to other geographic contexts. (Our open-source code makes this type of effort possible.) We saw our reporting cited in various public testimonies in state and federal hearings on well bonding. The recent infrastructure bill passed by Congress included almost $5 billion for old oil and gas infrastructure remediation — and while we wouldn’t claim to have unilaterally spurred that particular investment, we’re confident we contributed to the broader policy conversation in this space. Furthermore, in January 2022 (nine months after our reporting), the U.S. Interior Department drastically revised upwards their estimate of the number of orphan wells nationwide. The adjustment aligns with our predictions. With respect to the media ecosystem, our reporting was read by hundreds of thousands of people across a variety of platforms, and our feature was republished by The Guardian, among other outlets. Our Waves of Abandonment series received the Online News Association award in Investigative Data Journalism (Small/Medium Newsroom) earlier this year and contributed to Grist’s win in the General Excellence (Small Newsroom) category.","Grist partnered with The Texas Observer to harness their reporter Christopher Collins' expertise covering his native West Texas. Grist staff writer Naveena Sadasivam conducted archival research, interviewed policy experts, and filed dozens of public records requests to get the raw material for our data analysis. Grist data reporter Clayton Aldern then mined and combined these datasets and leveraged machine learning to create a statistical model of well abandonment. The model — which generates projections for every well in the Permian Basin — allowed us to predict which of the region's tens of thousands of oil wells are on the verge of orphanage. A separate survival analysis helped us understand producers' sensitivity to oil prices. Specifically, we created a series of cross-validated LASSO models to identify wells the states had not yet considered orphaned (but which were statistically indistinguishable from orphan wells). Because of our imbalanced classes (i.e. there were many more examples of merely ‘inactive' wells in the dataset than orphan wells), we also penalized models for incorrectly classifying orphan wells during training. This move prevented the models from simply learning to always guess 'inactive' — which would have led to high (but misleading) accuracy values for model performance. To forecast the public costs of these potential future abandonments, we used American Petroleum Institute identification numbers to perform lookups in state databases of plugging cost projections. We leveraged R/RStudio, Python, Tableau, Adobe Illustrator, and HTML/CSS/JavaScript. More information on our methodology is available in our methods write-up and on GitHub.  To help ensure the piece had the most impact, we added eye-catching imagery and drone videography and crafted interactive data visualizations (including, in our methods story, a web-app version of the model) that allow readers to see for themselves how the variables fit together to tell the larger story.","One challenge came from the disparate nature of the datasets in question. Some of our production data went back to the 1960s and had been saved in a variety of obscure formats, including those written for original IBM computers. Accordingly, sidestepping some particularly nasty data-ingestion tasks involved loading 62 million rows of well- and lease-level data into memory before cleaning. And not all datasets came to us easily. For example, we didn’t receive a complete dataset of New Mexico’s enforcement actions until we presented them with a BeautifulSoup-scraped version of their own public-facing database. A public records request sent to the agency was also rebuffed until we challenged their decision by submitting a complaint to the New Mexico attorney general’s office. Only after the attorney general’s review and decision to compel the agency to comply with our request did we receive the enforcement records. While other modeling efforts (at think tanks, for example) have sought to understand the orphan well problem at the national level, we believe they suffer from unrealistic assumptions about the abandonment rates of inactive wells in the country. Not all inactive wells will be orphaned. We believe our model is the first to operate at the level of the individual oil well (and produce realistic as opposed to sensationalist predictions). From inception to completion, our reporting process took approximately 11 months.","We believe this project is a good example of journalists leveraging statistical models to make predictions about the future (as opposed to exclusively describing the present or explaining the past). Certainly, talented data journalists at outlets like The Economist and Bloomberg engage in this kind of projection all the time. Our project offers a reminder that statistical models of public data can have lives outside the economic realm. Furthermore, we believe the models presented here represent a nice example of combining various subfields of statistical inquiry (in our case, statistical learning/machine learning, null-hypothesis falsification testing, and survival analysis) in order to paint a fuller, more reliable, portrait of real-world phenomena. Fundamentally, we think our project presents a case study in balancing advanced statistical models with archival research, public-records requests, interviews, and other tools of traditional reporting. When further combined with expressive visuals, interactive graphics, and drone videography, the package communicates important findings without deifying numbers over hard, qualitative reporting — despite being a piece of ""data journalism.""",English,https://grist.org/abandoned-oil-gas-wells-permian-texas-new-mexico/,https://grist.org/energy/scale-of-texas-new-mexico-abandoned-oil-wells/,https://grist.org/energy/fracking-oil-gas-well-inspection-in-permian-basin/,https://grist.org/energy/amy-townsend-small-study-methane-pecos-county-texas-abandoned-wells/,https://pulitzercenter.org/projects/looming-liabilities-abandoned-wells-permian,https://github.com/clayton-aldern/abandoned-wells,,,"Naveena Sadasivam, Christopher Collins, Clayton Aldern","Naveena Sadasivam is a senior staff writer at Grist. She previously covered environmental issues for The Texas Observer, Inside Climate News, and ProPublica. At ProPublica, she was part of a team that reported on the water woes of the West, a project that was a 2016 Pulitzer Prize finalist for national reporting. She is also a Livingston Award finalist and has been recognized by the Society of Professional Journalists and the Society of Environmental Journalists for her work. Sadasivam has a degree in chemical engineering and a master’s in environmental and science reporting from New York University. Christopher Collins is an associate editor at The Texas Observer. The Wichita Falls native graduated from Midwestern State University in 2012 with a degree in Mass Communication. He previously has worked as a reporter at the Abilene Reporter-News and the Wichita Falls Times Record News, along with running a freelance reporting business. Clayton Aldern is a senior data reporter at Grist. A Reynolds Journalism Institute fellow, his reporting and data visualization have appeared in a variety of outlets, including The Atlantic, The Economist, Logic, and on the floor of the U.S. Senate. He holds a master's degree in neuroscience and a master's in public policy from the University of Oxford, where he studied as a Rhodes scholar. Based in Seattle, he is originally from Minnesota.",
United States,BuzzFeed News,Big,Shortlist,,"The graveyard doesn't lie: Undercounted deaths from extreme weather, climate change, and failing infrastructure",26/05/21,"Investigation, Explainer, Solutions journalism, Environment, Health","Animation, CSV, R, RStudio","These two stories used excess deaths analysis to show that the winter storm and power outages that hit Texas in February and the extreme heat wave that hit the Pacific Northwest in June caused hundreds more deaths than we counted for by the affected states. The second story broadened the narrative to show that the failure to accurately count deaths from extreme weather is a general problem that is leaving government officials ill-prepared to respond to climate change and reduce its toll. It also discussed reforms in data collection, analysis, and disaster planning that would improve resilience against the single","The first story story drew widespread coverage from local and national media outlets, and prompted state Democrats to demand an investigation. This was a largely avoidable disaster: After an earlier winter storm in 2011, federal regulators warned Texas that its electricity grid, which is run independently from the two large interconnections that serve most of the rest of the nation, was vulnerable to failure in extreme cold.  Bills to better prepare the Texas grid to handle extreme cold were already in process. They were passed by the Texas legislature shortly after our article appeared and were signed into law by  Republican Gov. Greg Abbott in June. However, critics argue that these measures fall short of what is required. The issue has moved to center stage in the 2022 Texas gubernatorial election, with the Democratic challenger, Beto O'Rourke, citing our reporting in his attacks on Abbott's record.    ","The CDC maintains counts of weekly deaths, for all and certain selected causes, at the national and state level, for 2014 to 2019, and for 2020 and 2021. To estimate expected deaths for each jurisdiction and week, not including deaths from COVID-19, we trained regression models on the 2015-2019 weekly deaths data, accounting for long-term demographic changes using a linear component and using a smoothing spline to account for seasonal variation. We then used these models to predict the expected number of weekly non-COVID deaths for each state, with 95% confidence intervals. For each jurisdiction and week, we calculated weekly death anomalies minus deaths for which COVID-19 was given as the underlying cause, again with confidence intervals. We found significant spikes in non-COVID deaths in Texas in the week ending Feb 20, immediately following the winter storm and when the power outages occurred, and in Washington and Oregon in the week ending Jul 3, at the peak of an unprecedented heat wave, that greatly exceeded the official tallies in the affected states. Three independent academic experts in excess deaths analysis reviewed the findings, methodology, and analysis code. We also looked in more detail and individual causes of death in the CDC data, finding notable spikes for cardiovascular disease and diabetes in Texas for the week ending Feb 20, and for Washington in the week ending Jul 3. The analyses were performed using R and RStudio. Graphics for the stories were made with Datawrapper and ggplot2 R package, showing the time course of the power outages by county in Texas and neighboring states by animating frames using ImageMagick.","Finding family members of those who died in the Texas winter storm to reveal the harrowing human stories of these uncounted deaths. Aldhous looked for specific causes of death listed in the CDC data that were associated with the overall spikes, which flagged cardiovascular disease and diabetes as the causes listed on death certificates that were most likely to be associated with the extreme weather. He then filed public records requests to medical examiners in the largest Texas counties, asking for complete lists and cause of death for those who died after the winter storm and during the power failures. This allowed Lee and Hirji to triage their reporting by focusing on deaths that were attributed to these causes in the records obtained from medical examiners — turning what would otherwise have been a guessing game into a targeted search for medically vulnerable people whose deaths were triggered by the weather extremes. This allowed their individual stories to be told.","The GitHub repositories with our methodology, data, and analysis code provide a recipe for other data reporters wanting to apply excess deaths analysis to their own work. Our analysis of specific recorded causes of death, when combined with medical examiner’s records, also shows how data can be used not just to provide a top-line finding, but also to triage reporting by helping to identify individuals affected by the issues raised, emphasizing the human stories involved.",English,https://www.buzzfeednews.com/article/peteraldhous/texas-winter-storm-power-outage-death-toll,https://www.buzzfeednews.com/article/peteraldhous/extreme-weather-climate-change-missing-deaths,https://buzzfeednews.github.io/2021-05-tx-winter-storm-deaths/,https://buzzfeednews.github.io/2021-12-extreme-weather-deaths/,,,,,"Peter Aldhous, Stephanie M. Lee, Zahra Hirji","Peter Aldhous is a science and data reporter with BuzzFeed News, based in San Francisco. He also teaches in the J-School at UC Berkeley and in the science communication program at UC Santa Cruz. Stephanie M. Lee is a reporter at BuzzFeed News, where she writes about science, health, and medicine. Before joining in 2015, she was a reporter at the San Francisco Chronicle. She lives in San Francisco. Zahra Hirji is a BuzzFeed News science reporter covering the climate crisis, based in Washington DC. She previously was a reporter at InsideClimate News and has a masters in science writing from MIT.",
United States,USA TODAY network,Big,Shortlist,,Uncounted: The hidden death toll of the COVID-19 pandemic,23/12/21,"Investigation, Multiple-newsroom collaboration, Database, Open data, Infographics, Map, Health","QGIS, Microsoft Excel, Google Sheets, R, Python","For our Uncounted investigation, journalists from five newsrooms worked together to analyze CDC mortality data and follow that data to where it originates at the local level, through death certificates. We compared official COVID death figures with models developed by the CDC, in coordination with a team of demographers at Boston University; we collected death certificates and other primary source documents and then had medical examiners and physicians review them for errors and omissions; and we interviewed more than 100 medical examiners, coroners, public health experts, families and policymakers","What we found: Short-staffed, undertrained and overworked coroners and medical examiners were all but unified in when and how to investigate a possible death from COVID-19. Some took the family’s word for what they believed was their loved one’s cause of death. Others didn’t review medical histories or order tests to look for COVID-19, but expected the state or family members to provide documentation. Death investigators and some physicians attributed deaths to inaccurate and nonspecific causes that are meaningless to pathologists, but closely resemble symptoms of COVID-19. Our investigation reveals the country’s central problem with tracking COVID-19 deaths: Where people live and die has a lot to do with the accuracy of their death certificate. Some deaths are investigated with state-of-the-art technology and expertise; others don’t go beyond a phone call with the family.  Much of the impact from this project, published in three parts in mid-to-late December, is still evolving. But the CDC said in a statement that our findings go beyond what they are able to provide, and said forthcoming working papers would seek to address some of the more common reporting errors. ""We’re trying to push out as much information as we can, but we don’t have the resources to go digging in all of these counties. So it’s great that you’re doing this,"" Bob Anderson, the CDC’s chief of mortality statistics, told us. ""The sort of information that you’re digging up can help us, potentially, to improve the quality of the data.""","The team at Boston University worked with the journalists on this project to provide them with models of expected deaths in every U.S. county, a level of granularity never before reported on a national scale. The team then identified states and counties that had highest rates of deaths that were both more than any normal, pre-pandemic year, but weren’t attributed to COVID.  This modeling data was created using 10 years worth of CDC mortality data and a regression analysis. We supplemented this data with our own point-in-time analysis of 2020 and 2021 CDC mortality data, from its newly-released WONDER API. In the areas flagged by models, reporters found an unusual increase in deaths from natural causes, especially at home, and spoke with local corners and medical examiners who would likely be investigating and certifying the death certificates in these cases.  We've made the data publicly available as a repo and embeddable Google Sheet, along with a searchable data viz by county:","Mortality data, at the local, state and federal levels, only tells part of the story. As our series found, many of the errors and omissions on death certificates reflect a long-simmering problem of conflicting or unclear CDC guidance, a lack of training and resources or general apathy or politicization of the pandemic. As a result, we struggled to reconcile these data discrepancies and had to rely on on-the-ground reporting to shore up the differences. Many of the answers we received about the number of unexplained excess deaths in a particular county, specifically from Louisiana, Missouri and Mississippi, were revealing and not surprising. But they had not been fully explained before and elected coroners have yet to be held accountable for these errors and the incompleteness of their data. We plan to continue this work throughout 2022, in an attempt to expand on these findings.","As part of the project, Documenting COVID-19 is sharing the data used for the larger investigation to help local newsrooms investigate how COVID deaths are certified and counted in their community. We'll be updating this spreadsheet with new data and tools for reporters in the coming months. We also held a webinar on our reporting process and shared our slides here. We've received tips through our callout form from New York to Wyoming, and are already working with reporters in the USA TODAY Network in Georgia, Massachusetts, Wisconsin and five other states to replicate these stories locally.",English,https://www.usatoday.com/in-depth/news/nation/2021/12/22/covid-deaths-obscured-inaccurate-death-certificates/8899157002/,https://www.usatoday.com/story/news/nation/2021/12/22/deaths-elderly-couple-show-how-covid-deaths-can-go-uncounted/8989257002/,https://www.muckrock.com/news/archives/2022/jan/06/how-to-use-uncounted-cdc-data/,,,,,,"Dillon Bergin, Betsy Ladyzhets, Jake Kincaid and Derek Kravitz of the Documenting COVID-19 project; Sarah Haselhorst, The Clarion-Ledger; Ashley White and Andrew Capps, The Daily Advertiser; Rudi Keller, The Missouri Independent; Nada Hassanein, USA TODAY","Since the beginning of the pandemic, the Documenting COVID-19 project at Columbia University's Brown Institute for Media Innovation and MuckRock have worked to figure out how public health records and resulting data influences and shapes government policy. Death certificates are among the most influential records.  For this story, journalists from five newsrooms reviewed CDC mortality data at the county level. They compared those figures with models developed by the CDC and a team of demographers at Boston University, collected death certificates and documents and interviewed more than 100 medical examiners, coroners, public health experts, families and policymakers. The team at Boston University worked with the journalists on this project, providing models of expected deaths in every U.S. county and identifying areas of potential underreporting of COVID-19 deaths.",
South Korea,KBS(Korean Broadcasting System),Big,Shortlist,,"Land of the Extinct, how does the local city collapse?",04/04/21,"Investigation, Long-form, Documentary, Database, Open data, Fact-checking, Infographics, Chart, Video, Map, Satellite images, Audio","3D modelling, Sensor, Drone, D3.js, Json, Microsoft Excel, Node.js","This project is an in-depth analysis of the decline in population in local cities. It mainly dealt with the current address of the collapsing local city and the harmful effects of population concentration in the metropolitan area.  During the five-month production period, reporters covered 17 cities at home and abroad, and analyzed vast amounts of demographics with experts. Related content was produced not only on web pages but also on documentaries.","The coverage period is 5 months, 17 coverage sites, and 1,000km travel distance. Long-term on-site coverage has pointed out the problem of population decline in local cities. This is the first press release to cover the circumstances of individual cities in detail and implement them in videos.  Data analysis was also conducted. Based on the population data for special purposes that allow advanced statistical analysis, we looked at the problem of population decline in local cities with experts. Based on the analysis results, we also created ""a map of vacant houses across the country,"" ""a map of Korea's Catogram,"" and ""a 3D map of youth population movement."" This is a case of high expertise in accessing statistical sources and finding social implications.  We also paid attention to the web page. We have built a three-dimensional digital storytelling that integrates text, photos, graphics, videos, and 3D data maps, not just one-dimensional digital storytelling that puts text and photos. Furthermore, the web page was designed to move in response to its own actions such as scrolling and clicking, increasing the reader's experience. Writing was also adjusted in consideration of user interface.","This project used Create-React-App (CRA).  ① A map of vacant houses across the country This is a 2D interactive map that shows the stages of vacant concentration as of 2019 by dividing them into 'eup', 'myeon', and 'dong' units. Users can enter or select an area they are curious about to obtain an empty concentration rate in that area. It was made using Leaflet.  ② Local cities extinction Risk Map (2019) This is a 2D interactive map that shows the stages of the local cities extinction index as of 2019 divided into 'eup', 'myeon', 'dong' units. Users can enter or select a region they are curious about to obtain a local cities extinction index for that region. It was made using Leaflet.  ③ Korea's Cartogram Map (1966-2020) It is a catogram map that distorts the map area according to the population. Users can visually grasp how the population density in the metropolitan and non-metropolitan areas has changed with the times by selecting the year. It was made using D3.js. The number of people in each region is proportional to the size of the area.  ④ 3D simulation (2019) showing the movement of the youth population to the metropolitan area. There is a 3D simulation that shows the movement of the youth population during 2019. Users can choose the area they are curious about and get the percentage of the youth population who moved from that area to the metropolitan area. It was created using kepler.gl and redox.","It was the hardest to get the data that was the core of the coverage. More sophisticated and detailed data were needed to show detailed population changes in Korea over the past half century. However, the population data released by the National Statistical Office to the public were not detailed. Accordingly, reporters secured the necessary data through a microdata integrated service channel (special purpose data capable of advanced statistical analysis) provided by the National Statistical Office to experts.  Recruitment of team members was also difficult. Data journalism cannot be carried out by reporters themeselves. We need the help of many people, including data analysts and web page developers. However, at the time of production, there were no analysts to handle the data, no developers to create web pages, or designers to decorate the pages. The production cost was not enough, so we couldn't entrust the project to an outsourcing company. In response, we mobilized connections from college juniors to co-workers to find team members to participate in the project.","This page is a report on the problem of population decline in local cities. It contains specific social phenomena, causes, problems, and alternatives that can be derived at this point. Therefore, other media reporters can use this page to find out the current address and cause of the population decline in local cities in Korea, and how it negatively affects the whole country. Furthermore, we can see alternatives that can be derived at this point.  It is also worth referring to data visualization techniques. The most effort I put into this time is the map. I tried to design it so that I could see the changes in the population over half a century at a glance, and I also increased the user experience so that I could experience the changes in the population of the region where I live. In particular, the catogram technique is an example of a problem in which the population is too concentrated in a specific area as a data visual. It would be nice for other media reporters to refer to these cases and visualize them with population data in the future.",Korean,http://somyeol.kbs.co.kr,,,,,,,,"HYUNG GWAN LEE, EUN HEE MO, HA WOO LEE, DONG HYUK CHOI, DA HYE KO, SO HYUNG CHOO, SU HONG PARK, DABIN LEE Lee","KBS has set up a special coverage team to carry out this project. Reporters specializing in exploration reports, data analysts with high expertise, and web developers and designers with excellent senses participated. I was consulted by academia while carrying out the project.",
Germany,"Funke Mediengruppe (Berliner Morgenpost, Hamburger Abendblatt, WAZ, Thüringer Allgemeine, Braunschweiger Zeitung and many more)",Big,Shortlist,,2021 Bundestag Election in Germany - the Election Dashboards of Funke Mediengruppe,18/01/21,"Breaking news, Open data, News application, Mobile App, Infographics, Chart, Map, Elections, Politics","D3.js, Json, CSV, R, RStudio, Node.js","2021 was a super-election year (bumper election year) in Germany, culminating in the Bundestag elections in September. Beginning in  January, Funke Mediegruppe’s 18 newspapers provided election-related figures and information in one online dashboard. While the dashboard remained, its content adjusted to three phases:   before the elections the dashboard showed current polls and historical election results   in the election night, maps and new-tickers publishing automated written updates were live   after the elections, the dashboard shared results and analyses using maps and various charts    Additionally, there were five regional variants of basically the same dashboard, but presenting results for smaller regions.","Our dashboards were the backbone of all election coverage of 18 different newspapers. With the national dashboard published in January 2021 already, we were the first data journalism team in Germany to prepare the elections and overall, had almost one million visits across the different regional dashboards before, during and after election night in 2021. On top of that, many more articles of the company’s different newspapers either based their reporting on our work or even embedded parts of the dashboards as modular iframes into online articles, increasing the reach of our work. Our graphics and data were also adapted to multiple print graphics, for example the maps we created from very granular and hard to convert historical election results (link 7). Visualizations from our dashboards were featured by multiple blogs (e.g. datawrapper) and data visualization professionals.","The project is based on a state-of-the-art frontend technology stack using React with next.js, Mapbox for maps, emotion for CSS-in-js, and d3 for data visualization. As most of our users visited the page on their smartphones, we paid extra attention to a good user experience on small devices with low bandwidth. Additionally, we split parts of the story into smaller pieces that could be embedded on other sites using iframes. We created a database with consistent structure that various scrapers (coded in node.js) fed data from different sources (json’s, csv’s, but also emails with spreadsheets that were parsed automatically) and formats into. This one database was then behind all the different localized dashboards. For the historical as well as analytical parts of the dashboards, large amounts of data needed to be wrangled. In order to be able to display historical election results in comparable geometries, historically different election district geometries had to be refactored. For analytical elements as for example the comparison of election results based on district’s populations (e.g. by age or income) or how rural/urban they were, a lot of additional data was gathered and matched to categorize election result districts in order to be able to make these comparisons automatically (to save time / manual capacities) once the results were complete.","The overall challenge of the election-coverage was the scope of work considering our small four-person team. The difficulty is exacerbated by both the chaos in available data caused by German federalism, as well as catering to 18 different local newspapers that belong to Funke Mediengruppe. As our product included national-level results, in an ideal world, we would have simply created smaller derivatives for more local regions but based on the same data and source. Unfortunately, the data the federal German election office published, were not granular enough for small-scale maps of e.g. Berlin, Hamburg, or the municipalities of other federal states. Therefore the more granular local data for the regions our newspapers are covering had to be collected from various local offices, each of whom had differing formats and different interruptions during the election night (e.g. data being formatted differently than previously communicated), which needed a lot of fixing and adjusting of numerous scrapers for different locations in real time during election night, while results were flooding in already. The regional variance in data availability also impacted our production flow, as one of our approaches to make the workload manageable was to basically clone the dashboard we had created for the national overview to create regional derivatives by altering some location parameters but otherwise keeping the (data) structure and functions the same. However, we had to make revisions and adjustments for each location as data was not available consistently across these places. Where in one federal state there were only the election districts available (299 across all of Germany), other federal states had data on the level of municipalities (a few hundred per state) or even over 1000 different neighborhoods, which allowed for a very detailed map in the case of Berlin.","The project shows that an efficient set-up and preparation make it possible to cover a lot in a short amount of time, even with a very small team. Strategies that helped us achieve these goals included:   using a template structure, which can be copied as a blueprint to various elections and/or geographies and allows for multiple localized editions of content in the same structure (and structuring the data consistently as well)   a modular set-up, that allowed for more flexibility when handling, updating, fixing or maintaining multiple pages at the same time. For example, erroneous modules could be commented out if not high-priority at that very moment of the election night, allowing to attend to more urgent issues without having to shut down a whole page or keep errors online   modules also made it possible to create stand-alone widgets from parts of our dashboards that could in turn be used in other articles and front-page teasers across the company. Without a modular set-up, with the small team we have, it would have been impossible to offer and create interactive visualizations for as many articles and pages as our widgets were used in / embedded into.   using a central data repository with a strict data structure (including polling results, election results, results of individual candidates as well as parties, for various geographies), into which scrapers fed data from various sources. This made it possible to keep the upper hand over complicated data situations where we had to switch between 7 different dashboards within minutes or work on them in parallel for months.   we created non-public, self-service web interfaces for colleagues from other desks to export custom charts with our election data, e.g. for specific municipalities or maps as printable svg’s",German,https://interaktiv.waz.de/bundestagswahl-2021-umfragen-ergebnisse-wahlkarte/,https://interaktiv.morgenpost.de/bundestagswahl-berlin-2021-ergebnisse-wahlkarte/,https://interaktiv.thueringer-allgemeine.de/bundestagswahl-thueringen-2021-ergebnisse-wahlkarte/,https://interaktiv.waz.de/bundestagswahl-nordrhein-westfalen-2021-ergebnisse-wahlkarte/,https://interaktiv.abendblatt.de/bundestagswahl-hamburg-2021-ergebnisse-wahlkarte/,https://interaktiv.morgenpost.de/abgeordnetenhauswahl-berlin-2021-umfragen-ergebnisse-wahlkarte/,https://www.morgenpost.de/infografik/#/grafik/6038f23e767e213d20deade1,,"Marie-Louise Timcke, André Pätzold, Angelo Zehr, Sebastian Vollnhals, Ida Flik, Moritz Klack, Christopher Möller","Funke Mediengruppe's Interactive team develops interactive applications and data-driven stories for the Group's various news brands. It acts like a small, interdisciplinary task force of data journalists, designers and programmers within the newsroom, is very visually driven and user-focused, and covers various topics ranging from elections to climate change or social inequalities. ",
United States,Streetsblog,Small,Shortlist,,"Ignored, Dismissed",21/10/21,"Investigation, Database, Open data","QGIS, Microsoft Excel, CSV, PostgreSQL","I spent two months analyzing data on more than 26 million complaints received by New York City’s 311 system to reveal that the city police department chronically ignores reports of reckless driving, illegal parking, and abandoned vehicles. This analysis of more than a decade of data showed how the NYPD’s neglect of resident complaints has fostered a culture of lawlessness on city streets that has gotten worse as traffic deaths in the city climb to their highest point in years.","As a result of this story: - The city Department of Investigation has launched an investigation, which is ongoing, into one of the story’s findings: that city police officers may be harassing and intimidating frequent users of the 311 system.  - The city inquiry was demanded by then-Mayor Bill de Blasio, who expressed alarm about the story’s findings and vowed to improve the NYPD’s response to 311 complaints. - The NYPD has launched its own internal review. - The city Department of Information Technology released previously withheld data about city standards for 311 complaint response times. - The city Civilian Complaint Review Board said it would investigate allegations that the NYPD had falsified responses to 311 reports. - City council members criticized the NYPD’s handling of 311 in a hearing in which the story was frequently cited. - Other, larger news outlets--The Atlantic and The New York Post—wrote follow-up stories. - City lawmakers released statements condemning the NYPD’s handling of 311 complaints.","I used PostgreSQL to examine and manipulate the city 311 data. I used QGIS to join it with a police precinct boundary shapefile to determine which precincts were the most neglectful of 311 reports. This revealed one precinct was a major outlier in the city, closing unusually large numbers of complaints impossibly fast and with false justifications. And I used Tableau to map the complaints, which revealed further patterns and damning anecdotal findings in the data. For example, this revealed that a high-profile cyclist death in 2018 was preceded by more than a dozen complaints about unsafe conditions for cyclists on the street where she died, which the NYPD had ignored.   I combined these data techniques with more traditional reporting that drew on hundreds of pages of public records and interviews with dozens of city officials, former city police officers, safe-streets advocates, attorneys, and residents who have filed years of 311 reports.","The challenges of this project included working with such a large database, prying information from stonewalling city officials, and finding current or former NYPD officials to speak candidly about the department’s chronic mishandling of these complaints. Further, Streetsblog is a very small organization with few resources. This is the first project of this size that we have ever carried out.   The NYPD’s neglect of 311 has long been a source of ire for city residents, but reports on the problem had only been anecdotal. This story was the first to thoroughly document the NYPD’s sheer disregard for resident complaints and the failings of this crucial city service.","This project demonstrated the value of a largely overlooked public dataset in assessing the performance of city agencies. It points to many additional stories that journalists could pursue with the same data—like examining the 311 response of other city agencies or joining this data with other datasets to scrutinize other aspects of the NYPD’s 311 response. (I’ve already conducted one such follow up, cross-referencing the 311 data to city summons data to show that the NYPD may be lying about the summonses it claims to write in response to 311 complaints.)",English,https://nyc.streetsblog.org/2021/10/21/ignored-dismissed-how-the-nypd-neglects-311-complaints-about-driver-misconduct/,https://nyc.streetsblog.org/2021/10/26/council-member-calls-for-probe-of-alleged-311-harassment-as-colleague-reveals-dead-of-night-calls-from-cops/,https://nyc.streetsblog.org/2021/10/27/amid-multiple-investigations-nypd-defends-its-311-response-at-council-hearing/,https://nyc.streetsblog.org/2021/10/28/mayor-vows-to-investigate-alleged-harassment-of-311-users-after-recent-streetsblog-investigation/,https://nyc.streetsblog.org/2021/11/24/analysis-missing-parking-tickets-raise-new-questions-about-nypd-handling-of-311-complaints/,https://nyc.streetsblog.org/2021/12/21/city-investigating-nypd-311-harassment-allegations-after-streetsblog-report/,,,Jesse Coburn,"I am an investigative reporter at Streetsblog, a small news organization with journalists across the United States reporting on the movement for safe streets. Prior to Streetsblog I spent five years reporting for Newsday, a newspaper in New York, where my investigative work sparked law enforcement investigations and legislative reforms. I have also reported for the New York Times, the Baltimore Sun, Foreign Policy, and other publications.",
Germany,"Funke Mediengruppe (Berliner Morgenpost, Hamburger Abendblatt, WAZ, Thüringer Allgemeine, Braunschweiger Zeitung and many more)",Big,Shortlist,,Vaccination monitor,02/02/21,"Explainer, Open data, News application, Infographics, Chart, Map, Politics, Health","Scraping, D3.js, Json, CSV, Node.js","Since the early stages of Germany’s vaccination campaign, the vaccination monitor tracks the progress both nationally as well as in the different federal states, by age groups, vaccines and in international comparison. A modular concept allows us to react to new developments in the pandemic, newly available data or new questions by adjusting or adding visualizations. For example, in fall 2021, substantial parts of the page were revised to account for the new stage of booster vaccinations.","As our corona coverage is based on the concept of a web of applications, bundled in a compact form in the main coronavirus monitor dashboard and going into different applications and specific pages from there, it is hard to draw clear lines between individual projects. However, the vaccination monitor was and is a crucial element of the interactives team’s continuous effort and aim to provide all relevant information related to the pandemic and its developments. The vaccination monitor alone has been visited over 15 million times in 2021, and the coronavirus monitor, which includes a stand-alone embed widget of the vaccination monitor, continues to be the most successful article in the company’s history.","Data is regularly scraped from different sources with automated node.js scripts and written into csv files. The frontend uses these files and is based on a state-of-the-art frontend technology stack using React with next.js, Mapbox for maps, emotion for CSS-in-js, and d3 for data visualization. As most of our users visited the page on their smartphones, we paid extra attention to a good user experience on small devices with low bandwidth. Additionally, we split parts of the story into smaller pieces that could be embedded on other sites using iframes.","As a page designed with a monitoring function, the project poses a number of technical and conceptual challenges. On a technical end, displaying new up-to-date figures every day means that we automated data-collection processes with various scrapers, often with data sources not especially friendly to scraping (or barely machine-readable in general). On top of that, the issuing authorities frequently change formats, locations or content of the tables they publish without prior notice, so the vaccination monitor also requires continuous and often very quick maintenance or fixes. On a conceptual level, changes in policy, updated recommendations from health authorities as well as progress in what was originally meant to be a linear process of vaccinating most of the population meant constant reshuffling of priorities, old visualizations becoming obsolete and/or that the need for new visualizations arose, especially as more data was made public or more granular data became available. Major events that made us rethink and rework large parts of the monitor include the moment vaccinations were available to everyone rather than to those belonging to a priority group, changes in the communicated national goal (% of the population vaccinated) as more infectious variants arose, the moment booster vaccinations became necessary and kids could get vaccinated as well. The quality and validity of data was also a constant question accompanying us, as the methodology behind recording data was not always consistent across time and space (e.g. sometimes vaccinated people were assigned to multiple priority groups, other places just one) and in other cases, the was a discrepancy between reality and the official plans or recommendations that made interpreting data difficult (e.g. officially only people above 12 should be vaccinated, but there already were frequent reports about younger kids receiving shots as well).","As with all our Covid-19-coverage, this project taught us about creativity in data-driven journalism: even when the overall topic is the same for a long time, it is still both possible and crucial to develop innovative formats that are tailored to users’ specific needs at a specific moment in time - and these change constantly. Modular set-ups provide a lot of benefits with this: they allow creating go-to places for users which can be updated, adapted module by module, expanded and so on, and therefore stay an optimum source of information and stable source of information while adapting to dynamic developments in the pandemic. An important approach in constantly finding new interesting angles to public data was to match it with other information in order to generate new insights. For example, in the early phase of Covid vaccinations in Germany, the official data included indicators on the professions or age groups of those vaccinated. Rather than just visualizing the published data (how many percent of those vaccinated work in healthcare), we researched data on the different federal state’s approximate numbers of healthcare workers, pensioners, pensioners living in care facilities a.s.o. so we could give an idea about which percentage of these groups had received vaccinations. We also aim to continuously question standart visualizations and adapt or find new forms of depicting information. With the vaccination monitor, we for example created an abstract queuing line showing the different groups of prioritized people (when vaccinations weren’t available to everyone yet) and depict the numbers of vaccinations in a calendar-based tile plot and bar-chart, making it more accessible and relatable than abstract standard graphs.",German,https://interaktiv.morgenpost.de/corona-impfungen-deutschland-bundeslaender-weltweit/,https://interaktiv.morgenpost.de/corona-virus-karte-infektionen-deutschland-weltweit/,,,,,,,"Marie-Louise Timcke, André Pätzold, Angelo Zehr, Sebastian Vollnhals, Ida Flik","Funke Mediengruppe's Interactive team develops interactive applications and data-driven stories for the Group's various news brands. It acts like a small, interdisciplinary task force of data journalists, designers and programmers within the newsroom, is very visually driven and user-focused, and covers various topics ranging from elections to climate change or social inequalities. ",
Argentina,LA NACION,Big,Shortlist,,"Yeah, neighborhood, drugs and luxury. This is how trap music sounds on playlists and what centennials talk about",17/09/21,"Explainer, Database, News application, Mobile App, Infographics, Chart, Audio, Arts, Culture","AI/Machine learning, JQuery, Json, R, Python, Node.js","Trap is one of the most popular music genres among young people in Argentina: of the most listened artists last year, ten of them swung between Rap, Trap and Reggaeton. We used Natural Language Processing (NLP) tools to analyze their lyrics, demystify the concepts around the genre and understand what these artists, who bring about such a furor among the new generations, are talking about. The purpose of this project was focused on attracting and creating an experience for this type of audiences, unusual for LA NACION and also dealing with this topic in a different way for non-specialized audiences.","This special project was undoubtedly risky because we went completely out of our comfort zone. It was our first special using machine learning and dealing with a topic that is not typical in the newsroom. On the one hand, we set a precedent in using Machine Learning for content production in a large mass medium in Argentina; and on the other hand, we analyzed the lyrics of one of the most important music genres recently in a way never done before in the country. This was very well received by the community specialized in the subject, especially because we worked together with an independent medium specialized in freestyle and trap called El Estilo Libre and with journalists from the renowned music magazine Rolling Stone. We published this piece together and thanks to them we could have greater reach and dialogue with the community. The special was highlighted and valued by some freestyle personalities in rap battles. In addition to this news article, different pieces were made in social media: TikToks, Instagram Lives, Instagram TVs and different graphic pieces. These reached more than eighty thousand views in total and the piece was watched by an audience between 17 and 25 years old. The piece was also mentioned in LatAm Journalism Review, as one of the examples of an IA use applied to journalism in Latin America.","The whole information process was made in Python. Visualizations were made in Vue.JS. A total of 692 song lyrics were scrapped of the top 20 artists of the genre of Genius platform. After this step, tokenization and lemmatization techniques were applied to calculate the frequency of words used in the songs. Named Entity recognition was performed for brand recognition with the SpaCy library in Spanish and a manual check was performed. A sample of songs was taken with a 95% confidence interval and it was manually classified word by word under three labels: places, people and brands to compare the entities that were manually labeled against those recognized by the model. After several repetitions,EntityRuler, a library functionality for adding entities and improving entity recognition, was used. Different Wikipedia pages with lists of tags were also used as model training input. Thus, the model was run once again on the sample to perform a confusion matrix and determine the F1 score, which was 0.61 in its latest version. Finally, the universe of all song lyrics was processed and, after the output was achieved, a manual correction was performed on it to achieve an even higher level of accuracy. Moreover, a technique known as Topic Modelling was applied with the Top2Vec model to recognize the topics covered by the songs. The Spotify API was used to perform a rhythmic analysis of the genre. Variables such as danceability, tempo, energy and acoustic level were used as input. Based on these variables, the technique known as ‘clustering’ was applied to the KMeans model in 3 groups.  ","One of the great challenges of this project was the application of techniques from texts written in prose to texts written in verse, with phrases in different languages (English, Spanish, Italian, among others) and words and abbreviations specific to the genre or not found in dictionaries. In addition to this, we found that Natural Language Processing (NLP) models are not as powerful in languages other than English. This resulted in much more manual work and a team of more than 10 persons to be involved in the checking and supervision processes of the different techniques applied for more than 4 months. This project was one of the first projects of the team with these technologies, and therefore, it involved a process of learning and testing different models. As regards the editorial work, it was a twofold challenge: ensure that the piece is aimed at the specialized Trap community and understood by those who are not familiar with the subject. But installing such a disruptive and young issue in a traditional centenarian newsroom, where political or economic issues predominate and where the average age range of the audience is over 40 years old, was not an easy task. So, we worked with different teams and persons of different ages to find the best strategy for the project and achieve a vast reach on different platforms and audiences.","The main lessons learned are hidden behind the challenges mentioned above. One of the main lessons learned, that can be passed on to other journalism teams, was the work done with Natural Language Processing (NLP) models applied to texts in languages other than English. That was hard work, trial and error, several repetitions and the search for other solutions to help a better performance of these models. The key was not to discard the idea, but to look for other solutions and connect with other people who had had a similar problem. This could be done thanks to the interdisciplinary work done between developers, data scientists, designers, network and music specialists. The work between different teams, not only from the same media, was key to have precision in the algorithms and an immersive experience for users. But it was also a great learning experience for the newsroom to take the risk of creating an experience directly aimed at new generations, with a different language and format to what we were used to.",Spanish,https://www.lanacion.com.ar/sociedad/yeah-el-barrio-drogas-y-lujo-asi-suenan-y-de-esto-hablan-los-traperos-en-la-playlist-de-los-nid16092021/#/,https://blogs.lanacion.com.ar/projects/data/yeah-neighborhood-drugs-and-luxury-this-is-how-trap-music-sounds-on-playlists-and-what-centennials-talk-about/,https://www.lanacion.com.ar/data/analisis-de-letras-de-trap-con-procesamiento-de-lengaje-natural-nlp-metodologia-nid14092021/,,,,,,"Gabriela Bouret, Delfina Arambillet, Felix Ramallo, Gabriel Alonso Quiroga, Martín Pascua, Florencia Abd, Giselle Ferro, Paz Azcárate, Gabriela Miño, Florencia Rodríguez Altube, Nicolás Cassese,, María Inés Arán, Nicolas Caffarini,","The project members are part of LA NACION Data, Graphics, Social Media teams. And Rolling Stone Argentina Magazine journalist.",
Japan,Nikkei,Big,Shortlist,,China's global vaccine gambit,12/10/21,"Explainer, Long-form, Infographics, Chart, Business, Health","Scraping, D3.js, JQuery, Adobe Creative Suite, Google Sheets, CSV, Python","This project is follows a landmark moment in China's attempts to establish itself as a pharmaceutical powerhouse and a global health player. The work used a wide range of sources to tell the story of the economics, politics and diplomacy behind Beijing's push to become the world's biggest exporter of COVID-19 vaccines. It is the first in a data-driven series exploring China’s vaccine strategy, capabilities and role in the global medicines supply chain. But the effort has also attracted skepticism and questions, and observers wonder how China’s initiative fits into preparations for the next pandemic. ","We wanted to explore how and why Chinese vaccines have been exported all over the world despite most western countries eschewing their use. We tried to tell a nuanced story, highlighting how Chinese jabs have become a crucial tool in fighting the pandemic. But we also covered criticisms in areas including efficacy, transparency and the political demands made by Beijing of some recipient countries. Whatever the balance sheet of China's role, the pandemic shows Beijing has become an important presence in global public health in a way it was not before. We received comments, especially from people in the developing countries where the vaccines were in high demand. Readers from India, another big pharmaceutical country, were also interested in the follow-up piece.","We mainly used Python to scrape, refine and visualize the data used in this piece and a follow-up published in December on China's push to become the world's number one pharmaceuticals maker (https://asia.nikkei.com/static/vdata/infographics/chinavaccine-2/). We visualized all of the charts with a package called Altair so that we could easily edit them in Adobe Illustrator. For the mapping, we geocoded and visualized with Google Data Studio and Geopandas. For some national data, such as that of China, Mexico, and Indonesia, we scraped press releases in local languages. We could easily find documents released by the Chinese government, as one of the team members could read Chinese. For Mexico and Indonesia, we mostly relied on translation and colleagues who speak the relevant languages. For the research process, we used Selenium to scrape several Chinese official web pages, including those of the national drug regulator. We had to use a Chinese VPN to see some websites.","One of the most challenging parts of the work was telling the story of the spread of China's vaccines from various angles. We collected data on every aspect of Chinese jabs, such as distributed doses, product pipelines, and manufacturing plans. The flow of vaccine delivery was complicated, so we had to spend time calculating how many vaccines each country received.  The other challenge was China's lack of information disclosure. It was very difficult to access people and data in the country - especially from our base in Japan. It was also difficult to achieve the global coverage we wanted. We had to gather data and observations across continents for the comprehensive sweep required by the project. ","The COVID-19 pandemic has triggered a tidal wave of disinformation and propaganda. This is true of both China's self-promotion and some of the criticism levelled at its vaccines. The main lesson of the project is that it is important to dig into details to try to tell a sophisticated story. Chinese vaccines have been very helpful in combating the pandemic, particularly for poorer countries with little or no access to western-made vaccines. In this sense, Beijing and the Chinese pharmaceutical companies played a crucial role in saving lives. But at the same time, Chinese jabs are neither the best nor the cheapest available. Part of the reason they struggle to gain acceptance in some places is a lack of published trial data. And some of the vaccine shipments China has made around the world have come with a political price for recipient nations.","English, Japanese",https://asia.nikkei.com/static/vdata/infographics/chinavaccine-1/,https://asia.nikkei.com/static/vdata/infographics/chinavaccine-2/,https://vdata.nikkei.com/newsgraphics/chinavaccine-1/,,,,,,"Anna Nishino, Akira Shimizu, James Hand-Cukierman, Shohei Yasuda, Kentaro Watanabe, Michael Peel, MinJung Kim, Michael Tsang, Kazuhiro Kida, Tetsuya Abe","Nikkei is the largest economic and financial paper in Japan and it is now expanding its presence worldwide (its English-language media, Nikkei Asia and the Financial Times, are also part of the Nikkei group). The project was a joint effort of a multinational team that included experienced international editors and data journalists from Nikkei and Nikkei Asia, as well as a visualization expert who won a prestigious journalism award.",
South Korea,Donga Science(Kids Donga Science team),Small,Shortlist,,Our Neighborhood Zoo Guards Project(Korean : Udong Guards),01/01/21,"Investigation, Open data, Crowdsourcing, Illustration, Infographics, Chart, Environment","Animation, JQuery, Json, Adobe Creative Suite, Microsoft Excel, Google Sheets, CSV","The Zoo Guards is a citizen-participatory data journalism project in which citizens and experts investigate zoos across the country.   In Korea, the number of zoos as well as their welfare status is unknown. This is due to the system in which anyone can easily establish a zoo without a license.    So, we started the Project to investigate animal welfare at the zoo. Experts created simple questions to gauge the welfare state of the zoo. Citizens surveyed the welfare of 10 common animals in their neighborhood zoos and posted their data on our website. The data was corrected analyzed by experts.","A large number of citizens took part in the project. As experts, two veterinarians specializing in animal welfare, and as citizens, 760 teams of families including children and 40 veterinary college students gathered.   We collected data for a large number of zoos. Through citizens’ reports, we knew that there are about 200 unregistered zoos in addition to about 100 government-registered zoos. Citizens visited 246 out of 345 zoos in Korea (71.3%), and the welfare score of 10 animals averaged 53 out of 100. This was far below the score (88 points) at which it is presumed that the animals would live their life expectancy. Among the habitats surveyed, only 7% were presumed for the animals to be able to live their life expectancy. There were no statistics that investigated zoos on a large scale like ours in Korea. The government and some civic groups have surveyed a sample of 10 to 20 zoos.   It had an educational effect on citizens. They learn what conditions the animals in the zoo need to be happy and healthy. The children who participated in our project said, ""In the past, when I went to the zoo, I was busy looking at animals, but now I first check to see if the animals are healthy.""   Finally, our educational materials will be helpful to small zoo operators and keepers. Previously, they had little opportunity to learn about the needs of each animal species.   We reported our results as three articles in our magazine. On November 15th, a summary was released in the form of interactive content on our website. Our project won a special award at the Korea Data Journalism Awards.","1. PHP, CSS, mySQL, Excel It was used to develop a website for collecting zoo welfare survey data. On the website, citizens can answer 17 welfare measurement questions prepared by experts and post evidence photos and videos. Journalists and experts could download them as a single excel file when they want. With this excel file, they corrected and analyzed the data.   2. GPS location information Citizens usually used mobile devices at zoos to upload their findings. At this time, GPS location information of citizens was recorded along with the survey results, allowing experts and data journalists to determine the location of the zoo.   3. Google Docs, Google Maps We uploaded the zoo list in Google Docs and shared it with the public. Citizens added their neighborhood zoos to the list if it is not on the list (a zoo not registered with the government). With this, we discovered new zoos to investigate. Also, if citizens checked out a zoo welfare, they could write their name in the box next to the zoo’s name in the document. With this, we filled in the blanks on our zoo list. Also, we made it easy for citizens to find zoos close to home by uploading zoo address information to Google Maps.   4. JSON It was used to visualize statistical data analyzed with Excel as graphs in interactive content.   5. JQuery At the beginning of the project, to attract the public's attention and educate the ecology of the animals to be investigated, the 'Find Animals That Look Like Me' test was serviced. JQuery was used for user interaction in this test.   5. Javascript It was mainly used to implement motion in interactive content. In addition, it was used for all services for citizens.","Our biggest challenge was filling the data gap. We wanted to know exactly the welfare status of Korean zoos. In Korea, cases of abuse at several zoos have been exposed, but we did not know how widespread this was in zoos across the country. (The roadside zoos in the Netflix documentary 'Tiger King' raise and harass tigers for commercial purposes. In Korea, this is happening with raccoons and meerkats.) The problem was that no one had zoo statistics. According to Korean law, if someone has less than 50 animals or less than 10 species, they are not required to register with the government as a zoo. Also, when registering as a zoo, there is no need to report whether it meets the environmental conditions related to welfare. Without basic data, it was impossible to analyze the overall well-being of the zoo.   We decided to solve the problem with 'children'; VIP customers of the zoo. After educating children on what kind of zoo is good for animals, if children can do research when visiting the zoo, it is possible to educate zoo visitors and collect data at the same time.   The problem that children may lack professionalism has been addressed with 'citizen science'. We cooperated with experts in the field of animal welfare to prepare a method so that even children can easily do welfare research. Any errors that may have occurred during the investigation were also considered by experts. Experts are writing their thesis based on the data collected by us.   In summary, our innovations are: First, the reliability of the results is high. Experts set the welfare survey method sophisticated enough to submit papers to academic journals and corrected any errors. Second, data that did not exist in the world was collected. Third, ethical zoo visitors are growing..","Other journalists can learn about the structures that engage citizens in data journalism through our project. Everyone who participates in our project realizes their own desires. Journalists can gather data that never existed and write articles. Experts can also collect data that never existed and write academic papers.   The same goes for citizens. Children can satisfy their curiosity about animals and visit the zoo. Parents can relieve the guilt of visiting a non-educational zoo. (Parents who participated in our project often professed, ""Our son/daughter loves animals so much, so we have no choice but to go to the zoo. But when I saw the poor environment of the zoo, I felt guilty. I am glad that our child can critically enjoy the zoo.”) Veterinary College students gain the experience they need to become zoo veterinarians and meet mentors (experts).    The Zoo Guards was born based on their needs. The Korea Data Journalism Awards presented us with a special award and evaluated that it showed the potential of citizen-participatory data journalism. Previously, data journalists had been collecting, processing, and analyzing scattered data. This has made excellent articles, but there is a limit for the areas of ‘no data’ to stay in the dark. I dare to say that our project showed that even areas without data can be targeted by data journalism with the help of citizens and scientists.",Korean,https://drive.google.com/file/d/1GSdTc0akGuXy7ONly8U7ODZQXtwdJoOU/view?usp=sharing,https://zooreports.dongascience.com/raccoon,https://kids.dongascience.com/zooguard/main,https://docs.google.com/document/d/1AJuGUp9ckjF55lQIveuVmSAnfXVnjzCN8N05Oa4-AWs/edit,https://img.dongascience.com/kids2016/zooguard/article.pdf,https://img.dongascience.com/kids2016/zooguard/article_new.pdf,,,"Dasol Lee, MyeongJu Lee, Jeong Kim, Kee Kyeon Cho, Won Jae Song, Jaeyeon Lee, Eun Young Choi, Haein Jeong","Dasol Lee is working as a science reporter at Donga Science. She is now part of a team that makes a Kids Donga science magazine. In this team, she planned the Zoo Guards project, covered it, and wrote the articles and interative content.  MyeongJu Lee is working as a manager of Kids Donga Science. The manager plans and runs services for the readers of the magazines. In this project, she was responsible for running a service for citizens. In addition, the editor-in-chief and the designers of the Kids Donga Science team, and the development team worked together on this project.",
United States,Bloomberg,Big,Shortlist,,The Chinese Companies Polluting the World More Than Entire Nations,25/10/21,"Investigation, Long-form, Fact-checking, Infographics, Chart, Map, Satellite images, Environment, Business, Economy","QGIS, Json, Adobe Creative Suite, Microsoft Excel, CSV, R","China is the biggest unknown element in the fight against climate change. The world’s top polluter has more sway than any other country over whether the world succeeds at slowing the rise in global temperatures. Yet few companies in China publish data on their emissions, and those that do don’t say how they arrived at those figures.    Karoline Kan and Jin Wu spent months poring over corporate financial statements and sustainability reports to produce the first-ever emissions estimates of some of China’s biggest corporate polluters. The results were remarkable: some state-run companies emit as much as entire developed nations. ","The story was the first attempt to estimate how China’s emissions breakdown at the company level. What stood out was the huge role that the construction sector, including steel and cement firms, have in driving China’s pollution. These industries will have to deliver the bulk of reductions needed to achieve China’s climate goals, requiring a drastic shift in the economy so it no longer relies on property development and heavy industry as major growth drivers.    After laying out the scope of the problem, we produced one of the most detailed sectoral breakdowns of China’s greenhouse gases to show how China can tame its biggest emitters. This calculation required a whole different collection of datasets including multiple government statistics yearbooks, academic papers and industry reports. Interviews with experts and additional visual elements helped illustrate specific solutions that can be deployed.    The project garnered positive feedback from climate experts and other journalists, who called the research “stunning,” a “fantastic investigation,” “beautiful visualization and “important work.” The data added to debate heading into COP26 climate talks in Glasgow about what countries should do to cut their emissions. ","We worked with researchers from the Centre for Research on Energy and Clean Air to come up with our own methodology in order to calculate greenhouse gas emissions from major corporate polluters in China. With help from the researchers, we decided what were the best published data out there that can help us reach best estimation.   Data was collected into Google sheets for smooth collaboration with the researchers. We used R for formatting and analyzing data as well as generating drafts for some of the charts, then later polished them in Illustrator and exported them with ai2html for better accessibility on the web.   To make invisible emissions visible, we created moving particles with HTML5 Canvas to help our audience better connect to the topic. In addition to that, for the top as well as the grid graphic comparing company emissions, we used a multi-layer visual treatment to add motion to static photo/graphics with ai2html and photoshop, making the style consistent throughout the piece.    The scrolly top was built with HTML, CSS and JavaScript. GSAP, a Javascript animation library was used to create the smooth transitions, visually guiding our audience into the more data-heavy part of the story.   For the maps showing expanding construction in China’s cities, we used QGIS to map the built-up areas in different time periods, exported them as transparent png, then overlaid on top of satellite images. ","The biggest challenge of the project was compiling the datasets from scratch,and figuring out how to drive home the scope of China’s true emissions through the visual elements.    Our goal was to collect enough data to be able to make a best-possible estimation of greenhouse gas emissions. Sources of pollution vary from industry to industry, requiring us to mine different base data and make independent calculations. The researchers we worked with reviewed academic papers, industry reports and government statistics to determine the best way to translate the data we could find (such as tons of steel used or number of cars produced) into their equivalent carbon footprints.    Once we had those numbers, we had to find a way to get across to readers just what they meant. We experimented with various ideas, eventually settling on using different equivalences, such as the emissions for countries, or barrels of oil and numbers of trees that have to be planted to absorb that CO2. ","When there’s an important story we want to tell but there’s no published ready-to-use data on it, a close collaboration between journalists and domain experts (in our case, the researchers from CREA) to design a specific “study” could bring all sorts of possibilities.   Most of the time, we work with data that already exists, which is also true when it comes to research data. As journalists, we constantly have ideas that we hope there’s data for, but we lack the expertise to either collect that data, or do meaningful analysis and calculations by combining different datasets. So it’s becoming increasingly effective for a group of journalists to bring an idea to domain experts, brainstorming what could be done.   Be cautious with companies’ claims on “low carbon”, “carbon neutrality” and “sustainability” as ESG has become an area where a lot of greenwashing is happening. Many companies are not serious about their ESG reports, and often nobody will hold them accountable for whatever they reveal or not reveal in their ESG reports. Be careful with company emission data, and always try to be clear what scopes are included. ",English,https://www.bloomberg.com/graphics/2021-china-climate-change-biggest-carbon-polluters/,,,,,,,,"Jin Wu, Karoline Kan, Sharon Chen, Dan Murtaugh, Jane Pong","The journalists responsible for this story are reporters, editors and data journalists at Bloomberg News.",
United States,"The Markup, Süddeutsche Zeitung (for link 6)",Small,Shortlist,,Citizen Browser,19/01/21,"Investigation, Long-form, Multiple-newsroom collaboration, Database, Open data, Infographics, Chart, Elections, Politics, Business, Health","AI/Machine learning, Personalisation, Scraping, D3.js, QGIS, Json, Microsoft Excel, Google Sheets, CSV, R, RStudio, PostgreSQL, Python, Node.js","The Citizen Browser project is a flashlight inside the black box of Facebook's algorithms, allowing The Markup to monitor what content Facebook decides to amplify in people’s news feeds. In a year when Facebook is under unyielding scrutiny as whistleblowers come forward to unmask the realities of Facebook's intentions, Citizen Browser is an essential tool that delivers the ground truth and enables empirical research on platform accountability. It is one of the only windows the public has into the impact of Facebook's algorithms on its users. Our reporting series highlights Facebook’s failures to live up to its own promises. ","Following the publication of “Facebook Said It Would Stop Pushing Users to Join Partisan Political Groups. It Didn’t,” Sen. Ed Markey cited The Markup's work in a Jan. 26 letter to Facebook CEO Mark Zuckerberg questioning the company's broken promises regarding the promotion of political groups to its users. Leaked documents reveal that Facebook scrambled to address the issues raised by The Markup's article the day it ran. Internal teams investigated the “leakage” of political groups into recommendations, and the issue was escalated to Zuckerberg himself. Employees identified several technical issues that may have contributed to political groups being recommended to users, and an employee declared the problem had been ""mitigated"" by Jan. 25, six days after the story ran. As recently as June, our data suggests that Facebook's algorithms have continued to recommend political groups to its users. Our report “Credit Card Ads Were Targeted by Age, Violating Facebook’s Anti-Discrimination Policy” caught congressional attention as well. After Facebook pledged to purge its site of the discriminatory financial services ads that The Markup uncovered in its research, U.S. senator Mazie Hirono cited The Markup's reporting in her letter to Monika Bickert, Facebook's vice president of content and policy, calling the company's response ""evasive and inadequate.""  One of the companies referenced in this story, Hometap, used our research to prompt an audit of its ads with its ad agency. According to a statement from Hometap's head of marketing, Rachel Keohan, ""Following your outreach, we worked with our third-party digital agency to audit our ad campaigns, and determined that many of our Facebook advertisements were, in fact, still utilizing age ranges for targeting purposes. We’re in the process of updating all of our Facebook advertisements to no longer target audiences based on age.”","The Citizen Browser project is a pioneering investigation combining the infrastructure of national polling with modern data collection techniques. Our panel of users automatically shares data with us from their Facebook feeds, allowing us rare visibility into what content is pushed by Facebook's algorithms.  Citizen Browser is a standalone desktop application based on the open source Electron JavaScript framework. It's designed to run 24/7, remaining open in the background of the user's computer. The app performs Facebook captures one to three times daily, using custom software to talk to a Chrome browser using the Dev Tools Protocol.  Our panelists include more than 3,500 paid participants in the U.S. and 600 in Germany. The resulting dataset contains more than 20 million posts, 57 million recommended groups, and three million targeted advertisements. To ensure panelists' privacy, we built a data pipeline that automatically removes personally identifiable information (PII) from panelists' feeds before making their data available for analysis. Maintaining data quality and panelist privacy requires constant upkeep: As Facebook updates its software, we must monitor and update code as well.  The application, data-processing pipeline, and underlying cloud infrastructure were audited by third-party security research firm Trail of Bits.  Analyzing this vast dataset required keyword analysis, linear regressions, correlation analysis, classification, and ranking comparisons. We joined panelists' data with information they provided about their demographic and political affiliations.  We also built interactive tools for the public. In Split Screen, readers can see differences in news sources, hashtags, and group recommendations between groups of Citizen Browser panelists. In our Trending on Facebook Twitter bot, we provide daily updates of the content that appeared most often in the past 24 hours in our panel. This bot accompanies our report on how sensationalist, partisan posts are more popular in feeds than Facebook claims.","Attempting to independently monitor Facebook is a massive challenge. Many research scientists have tried and failed to overcome the technical and legal hurdles to providing oversight of the world’s largest social network. Facebook has a history of shutting down or dismissing attempts to monitor its platform. In 2019, the company made changes to obfuscate its code in a way that blocked ad collection efforts by ProPublica, Mozilla, and ad transparency group WhoTargetsMe. This summer, Facebook shut down the accounts of researchers working with the NYU Ad Observatory and then implemented new code that foiled automated data collection of posts—a technique researchers and journalists use to audit what’s happening on the platform on a large scale. We tackled these challenges in two ways. First, we built our system in a privacy-preserving manner that we hoped would blunt any legal argument from Facebook about our compromising its users’ privacy. We used an isolated browser profile on the panelists’ computers to store sensitive Facebook session information, and we built a data pipeline for redacting PII. We built our cloud infrastructure so that no unredacted data could be seen by a person. We had all our software audited by a security research firm to verify these measures. Secondly, we spend a lot of time adapting our software to the many changes in the tech platform’s software. Sometimes those changes happen when Facebook introduces a new feature, like the “flags” related to COVID-19 released this summer. The messages in these flags addressed the panelist by name, so we needed to update our redactors to strip that out. Other times—as when the platform modified accessibility attributes in its HTML to make it harder to rely on them to parse data from the page—it seems as if Facebook is updating its software to intentionally hinder our work.","The most significant takeaway from this project is the importance of integrating engineering and editorial in the newsroom. Unlike in most newsrooms, our engineers and reporters work together on the same team and report to the same investigative editors. That allows them to work hand in hand to peek behind the curtain of Facebook's algorithms. No one was willing to accept that those algorithms could forever remain opaque. We couldn't have told these stories without the technology, and we couldn't have built the tech without that shared mission. Relatedly, holding tech platforms accountable requires speaking tech fluently yourself. Not every reporter needs to be able to tell their C++ from their C#, but being willing to question assumptions, shake off a fear of numbers, and dive headfirst into data can go a long way toward walking the walk. We also hope other journalists recognize and emulate our prioritization of privacy. We were able to glean a tremendous amount of information from our panelists’ Facebook feeds all while honoring our promise to respect their privacy and never compromise their personal information. Yes, this required a bit of legwork to make possible, but we were willing to pay the ""privacy tax"" for a principle we hold so sacred. Finally: show your work. We publish everything on GitHub for two reasons. It enables other newsrooms to repurpose our data, slicing and dicing it for their own investigations and takeaways, and we hope they do. And we also believe in transparency: We hope other newsrooms follow suit with publishing methodologies to show their work, helping fight the ""fake news"" narrative that journalists face.",English,https://themarkup.org/citizen-browser/2021/01/19/facebook-said-it-would-stop-pushing-users-to-join-partisan-political-groups-it-didnt,https://themarkup.org/citizen-browser/2021/02/16/trumps-false-posts-were-treated-with-kid-gloves-by-facebook,https://themarkup.org/citizen-browser/2021/03/04/official-information-about-covid-19-is-reaching-fewer-black-people-on-facebook,https://themarkup.org/splitscreen,https://themarkup.org/citizen-browser/2021/11/18/facebook-isnt-telling-you-how-popular-right-wing-content-is-on-the-platform,https://themarkup.org/citizen-browser/2021/09/22/germanys-far-right-political-party-the-afd-is-dominating-facebook-this-election,https://themarkup.org/citizen-browser/2021/05/20/facebook-said-it-would-stop-recommending-anti-vaccine-groups-it-didnt,,"Surya Mattu, Angie Waller, Corin Faife, Julia Angwin, Rina Palta, Jeff Crouse, Ian Ardouin-Fumat, Simon Fondrie-Teitler, Mago Torres, Leon Yin, Micha Gorelick, Sam Morris, Alfred Ng, Thomas Pullin, Jon Keegan, Colin Lecher, Dara Kerr, and more","Citizen Browser is a full-year (and still ongoing) project led by Markup senior data engineer Surya Mattu with assistance from project manager Angie Waller, data reporter Corin Faife, and two dozen other staffers and contractors who contributed to its upkeep, analysis, and corresponding reporting. An engineer by training, Surya builds tools and gathers data to tell stories about how algorithmic systems perpetuate systemic biases and inequalities in society. Before The Markup, he worked on Gizmodo’s Special Projects Desk and ProPublica, where he was part of the team that was a finalist for a Pulitzer Prize for the series “Machine Bias.”",
United States,"The Markup, Gizmodo",Small,Shortlist,,Prediction: Bias,02/12/21,"Investigation, Long-form, Multiple-newsroom collaboration, Database, Open data, Infographics, Chart, Map, Crime, Gun violence","Scraping, D3.js, QGIS, Json, Microsoft Excel, Google Sheets, CSV, R, RStudio, PostgreSQL, PostGIS, OpenStreetMap, Python, Node.js","A Markup/Gizmodo collaboration, this investigation is based on more than eight million previously secret crime predictions that software developer PredPol left unsecured on the web. We conducted the first-ever independent analysis of actual PredPol crime predictions and found that they fell most heavily on low-income, Black, and Latino neighborhoods, while mostly sparing richer, White areas. Experts had feared the software was replicating police bias, but our unprecedented access to data allowed us to prove it. We also discovered that the company’s founders were aware of the inequities and developed a possible tweak, but the company didn’t change its algorithm. ","Published as the year came to a close, the investigation was well received by activists, and academics studying policing technology who said it provides needed transparency into this notoriously opaque universe of cop tech.  “No one has done the work you guys are doing, which is looking at the data,” said Andrew Ferguson, a law professor at American University who is a national expert on predictive policing. “This isn’t a continuation of research. This is actually the first time anyone has done this, which is striking because people have been paying hundreds of thousands of dollars for this technology for a decade.” Immediately after publication, Sen. Ron Wyden’s office asked the reporting team for a private briefing on the findings, often a first step to legislation or other action.","This investigation began when Gizmodo investigative data journalist Dhruv Mehrotra searched websites of law enforcement agencies using a tool he built and typed in “PredPol.” A page on the LAPD’s website linked to an unsecured server containing the motherload: millions of crime predictions PredPol delivered to dozens of law enforcement agencies across the country over years.  Dhruv downloaded the data, then partnered with The Markup’s Surya Mattu to analyze it. They converted more than eight million predictions stored on 42,000 individual files—small, red boxes drawn on street maps—into geolocation coordinates, and then joined them to demographic information from the U.S. Census Bureau and public housing locations from HUD. And then the really hard work began: What methods would we use? What thresholds? What about errors in Census data? Or the fact that we can’t get demographic information for areas as small as the prediction boxes? Getting the disparate impact analysis right and bulletproofing it took months. We used Python scripts and Jupyter notebooks to build the data sets for analysis. We used Kepler.GL and Observable Notebooks to build interactive maps that visualized the prediction data. We carried out our analysis using R, relying on the R Targets package to build a deterministic data pipeline that could be easily audited. And we used R Markdown to build data sheets that contained the findings for individual jurisdictions as well as maps showing where the predictions occurred. We also created choropleth and grid density maps using mapping software, showing predictions in their geographical contexts, which was invaluable for reporting The team filed more than 140 public records requests with 43 agencies, requesting data about stops, arrests, and use-of-force incidents. Surya and Dhruv wrote custom software to determine which incidents occurred in prediction locations.","One huge problem was confirming the legitimacy of the data, since it came from unsecured cloud storage rather than an official source. That involved dogging dozens of police and sheriff’s departments and local officials, searching public contracts, and scouring media reports.  Connecting the findings to real-world actions by police on the street was hampered by the agencies, who almost universally refused to share data on how officers responded to the crime predictions. So we filed more than 140 public record requests for data on arrests, stops, and uses of force. Most departments denied our requests, but we collected, standardized and examined more than 600,000 arrests, stops, and uses of force from those who fulfilled our requests.  The reporting was particularly challenging during COVID-19 travel restrictions. Reporters called hundreds of arrestees and spoke to defense attorneys and prosecutors, and none were aware crime software may have been related to their cases. In many cities, advocates weren’t even aware that crime prediction software was being used at all. The team also interviewed cops, academics, policing experts, and local officials. They were eventually able to visit some communities affected by police departments’ use of PredPol.   The data analysis required a lot of prototyping and iteration. We used regressions to look for correlation between different predictions and arrests, we carried out an exposure analysis to determine who was most likely to be exposed to policing due to predictions, and we calculated the demographic composition of the different neighborhoods based on how many predictions they received. Each approach resulted in similar conclusions. To ensure the report's findings were bulletproof, we reached out to subject-matter experts on predictive policing and researchers from Stanford, Columbia, the University of Pennsylvania, Oxford, and Human Rights Watch to review our methodology before publication.","First and foremost: Persistence pays. This data came not from a public records request—it would have been denied—but rather from unsecured cloud storage that an industrious reporter found by digging around. Sometimes the back door is the only door. Second, it shows the importance of reporting out the “why” behind the data findings. The data analysis revealed that the software targeted neighborhoods that were disproportionately inhabited by people of color and poor people. That’s a strong finding, yet those who believe algorithms are the cure to bias would question how this is possible. So we looked into it for them, finding a bevy of academic and government reports that most crime is not reported and that people of color and those living below the poverty line are more likely to file police reports when they’ve been victimized than White people or middle-class and rich people, who are more likely to handle the situation another way.   Lastly, transparency: We published an in-depth methodology showing precisely how we conducted our analysis and posted all of our data on GitHub, which allows anyone to download it and conduct their own research or reporting. As we in the media continue to battle a confused public’s contention that mainstream news is “fake” or biased, it’s never been more important to show your work, even your limitations. We do it every time at The Markup, and it’s heartening to see other newsrooms beginning to publish more robust methodologies as well.",English,https://themarkup.org/prediction-bias/2021/12/02/crime-prediction-software-promised-to-be-free-of-biases-new-data-shows-it-perpetuates-them,https://themarkup.org/show-your-work/2021/12/02/how-we-determined-crime-prediction-software-disproportionately-targeted-low-income-black-and-latino-neighborhoods,,,,,,,"Aaron Sankin, Surya Mattu, Annie Gilbertson, Dhruv Mehrotra, Dell Cameron, Daniel Lempres, Josh Lash, Evelyn Larrubia, Andrew Couts, Angie Waller, Joel Eastwood","Aaron Sankin reports on how technology can be used to harm marginalized people. He focuses on platform governance and online extremism, which he previously covered for the Center for Investigative Reporting. Surya Mattu builds tools to tell stories about how algorithmic systems perpetuate systemic biases. He previously worked at Gizmodo and ProPublica, where he was part of the ""Machine Bias"" team, a finalist for a Pulitzer Prize. Annie Gilbertson is an investigative reporter and audio journalist based in Los Angeles. Dhruv Mehrotra is a data reporter with Reveal. Additional reporting by Dell Cameron, Daniel Lempres, and Josh Lash.",
United States,Access Atlas,Small,Shortlist,,Access Atlas,15/01/21,"Breaking news, Database, Women, Health, Human rights","Animation, Json, Microsoft Excel, Google Sheets, CSV","As the first wave of  the COVID-19 pandemic peaked in the United States, abortion bans were instituted in many states under the guise of safeguarding public health. In response, Access Atlas was founded to track abortion bans and restritions set in place throughout the pandemic complemented by local narratives from abortion providers that illustrate their iniquitous impact. Since its inception, Access Atlas continues to develop resources to equip people with necessary and accessible information regarding the practical implications of confusing state laws and policies on abortion care. ","Our project provided a vital source of information for everyone from journalists to people seeking abortions during a time when reproductive healthcare was under attack, but not regularly covered in the news. While media outlets covered some high profile battles over abortion access, the availability of abortion and other reproductive healthcare was unclear in many parts of the country in the early months of the COVID-19 pandemic. In the months of April and May 2020, our website and maps were the only consistently updated source of this aggregated data.  Our maps and information provided timely, accurate, and accessible information to the public on three key questions: (1) Are abortion services available in my state during the pandemic? (2) What policy restrictions, if any, exist to limit access? (3) How were abortion services defined in initial COVID-19 executive orders and statements in my state? We designed these resources to assist both individuals looking for information about their healthcare options and researchers looking for accurate data. Beyond abortion availability, these resources identify the gap between technical laws and actual abortion accessibility in each state and arm viewers with clear information about statewide abortion restrictions designed to limit access.  When states began to loosen COVID-19 restrictions, we continued to track abortion availability and accessibility, recognizing the public confusion and uncertainty in this time period. By conducting interviews with abortion providers in 2021, we were able to identify the ways in which providers were uniquely impacted by state policies during this time and shared our findings through a series of blog posts to complement our collected data.  ","Access Atlas presents a set off three maps that capture and track the availability of abortion services in the United States during the COVID-19 pandemic. An excel database was developed using information collected from news articles, policy and legal documents and resources as well as clinic interviews. Analysis of this database yielded three important quantitative measures displayed within our maps that together communicate a digestible snapshot of abortion access in America.  The first map displays the COVID-19 Abortion Restriction Score. This score is a composite measure calculated according to the number of medically unnecessary laws enacted in each state whose restrictive impact on access to abortion care was amplified in the context of the pandemic. The two subsequent maps display how abortion was initially classified in executive orders implemented in March 2020 and the current status of abortion availability in each state. The website and maps were developped using HTML, JavaScript, and CSS. While this research exposed the uneven impacts of state-level responses to the COVID-19 crisis on abortion access, our analysis of their implications lacked local narrative. In January 2021, we called clinics in the 11 states that classified abortion as non-essential in initial government orders to understand the impacts of the pandemic on individual clinics. We collected both quantitative and qualitative data on over ten clinic level variables and published our findings in a series blog post that explored areas of commonality and difference in clinic experiences. The goal of this blog series was to highlight the unique experiences that clinics and their clients have had to navigate and overcome throughout the COVID-19 crisis and the need to support abortion providers now more than ever.","The challenge of developing Access Atlas fell into two main categories: the practical issues associated with culling data, and the theoretical uncertainties we had about presenting the results. As recent graduates working without funding or institutional support, we spent our limited time outside our newly-remote first day jobs brainstorming how best to determine and relay critical information regarding abortion availability and access across the US. Despite hours spent reading news articles, executive orders, and government press statements, it remained unclear where people were able to access abortion services, and how people were expected to monitor changes in their availability.  Given the highly nonbinary nature of the question we were trying to answer, we devised a digestible ""COVID-19 abortion restriction score"" to characterize the various ways in which abortion availability could be threatened, then went state-by-state gathering data from a diverse range of trusted sources. This kept results simple but robust in that they contained a great deal of information: for example, this allowed us to capture where abortion was available, where surgical abortions were banned but telemedicine was available, or where restrictions limited the ability of providers to delegate tasks. We continued to review and update this information once the website launched, especially where accessibility was uncertain; we also brought two undergraduate research assistants onto our team.  More broadly speaking, we struggled to identify our audience. We wanted this information to be available to people seeking services themselves, but recognized that, given our resources and limited background in app development, it was more feasible to focus on the academic and research community. Ultimately, all of these decisions paid off, and we were able to collaborate with current students from Wellesley College, Vanderbilt University, and Georgetown University (see their writeup at: https://www.hyasforchoice.com/single-post/2020/05/31/access-atlas-mapping-abortion-restrictions-in-the-time-of-covid).","The landscape of abortion availability and access in the United States is unstable and changes according to political will or global pandemics. In addition, the intricacies of access to abortion services vary from state to state and new legislation and restrictions are often difficult to predict and track. Our team was challenged by the fickle nature of abortion access across the country. Sudden changes in accessibility required rapid updates to our website and this conflicted with our mission to provide our audience with accurate and well researched information.  Our team dealt with this conflict via transparency. We informed our audience about the limitations of our data, measurements and analysis as well as provided them with our sources. As journalists covering abortion in the United States our responsibility was not only to provide our audience with data and information, but also the necessary resources for to think critically about and evaluate the evidence we are presenting. This responsibility is often forgotten or intentionally ignored within journalism, especially journalism covering politically frought topics such as abortion. We also believe that our project demonstrates the importance of engaging youth in data reporting and journalism. Our project engaged university students as research assistants and collaborated with reproductive justice university groups on social media campaigns. We hope to continue to provide an inclusive platform which nurtures the energetic and visionary voices of youth and provides them with leadership opportunities.",English,http://access-atlas.org/,http://access-atlas.org/blog.html,,,,,,,"Charlotte Evans, Io Jones, Tara Gallagher","Charlotte Evans is an Master's student in Population Health Sciences at the University of Cambridge. She grew up internationally and has a B.A. in Geography from Dartmouth College. Prior to graduate school Charlotte spent two years conducting health system evaluation research in the Northwest Territories of Canada in collaboration wih Indigenous governments. She is commited to ensuring equitible access to health care services and pursues this goal through the development of projects including Access Atlas and initiatives to ensure youth leadership in the design and delivery of mental healthcare.  Io Jones  is a student at the University of Virginia School of Law. She grew up in Brooklyn, New York ad has a B.A. in Geography and Public Policy from Dartmouth College. Prior to law school she worked as a litigation paralegal and conducted research on abortion access during the COVID-19 pandemic with the Access Atlas team. She has conducted a range of research projects on access to abortion, crisis pregnancy centers, and state, national, and global health policy. Tara is a first-year environmental science PhD student at Harvard University exploring connections between climate change and the water cycle. She grew up in Burlington, Vermont and has a B.A. in physics and music from Dartmouth College. Before graduate school, she spent two years working as a signal processing researcher on government contracts, and collaborated with both Harvard’s Center for Communicable Disease Dynamics and the Access Atlas team on studies related to infectious disease and reproductive health respectively.",
Peru,"Salud Con Lupa, Epistemonikos Foundation",Small,Shortlist,,Scientifically Proven: An analysis of the most talked-about covid-19 treatments,04/01/21,"Database, Open data, News application, Fact-checking, Chart, Health","D3.js, Microsoft Excel, Google Sheets, CSV, Python, Node.js","Several clinical trials have been carried out that offer scientific evidence to confirm the efficacy or harm of the use of a drug or therapy with various patients who have suffered mild, moderate and severe symptoms of coronavirus infection. Salud Con Lupa in collaboration with Epistemonikos analyzed the 45 most used COVID-19 treatments. We created seven classifications, ranging from ""standard treatment"" to ""unsupported by science"". We developed these categorization levels to assess the appropriateness of the covid-19 treatments up to this point in time. From September 2020 until now, we update this information weekly as new scientific evidence becomes available.","After two years since the COVID-19 pandemic started, there is already scientific consensus on which drugs under study work, and which do not, for the treatment of the disease. To follow the evolution of the evidence, in September 2020 Salud Con Lupa and Epistemonikos launched Scientifically Proven, a tool that announces the progress of the results of clinical trials of medicines against COVID-19 in the world. This effort, which is updated every week, presents 45 drugs, the most used, divided into seven categories (from ""Standard treatment"" to ""Science does not support it"") that are very easy to understand to bring science closer to more people. Our tool shows the medicines that can be trusted today for each stage of COVID-19 and those that lack scientific support, but are still recommended by some organizations or groups that validate conspiracy theories. While there are drugs that have shown some benefit (such as monoclonal antibodies), they are expensive, complex to administer, and, in poor nations, still very rare. Scientific consensus is always difficult to define because there are many entities and finally one looks at the main organizations. For example, when the first organization says that hydroxychloroquine should not be used until the last one stops using it, many months go by. We also have drugs that have been reaching a consensus that they work. Today we are very clear about the role of corticosteroids and monoclonal antibodies. The latter are antibodies that bind to SARS-CoV-2 and block it. The issue is its cost. Today there is a consensus that they are used as prevention in high-risk people who have had close contact with someone infected, but also to treat patients in early stages.","Build a database We built a database of 25 treatments that were evaluated. The project was launched in September 2020 and updated every week until the present, which allows to see a medical evolution of the treatments. Two years after, the list of medical evidence on the platform already gathers 45 treatments with the highest demand for use. The tool is presented in Spanish, English and Portuguese. Contrary to scientific research articles that have an academic or scientific structure, this tool was created with the aim of guaranteeing easy navigation access, with which visualizations and tabs were integrated with the details of the therapies. Multidisciplinary team The work team involved doctors, investigative journalists and scientific journalists, as well as computer specialists, responsible for the implementation on the website under a highly usable scheme that allows users to immediately know the classification , the evolution of the evidence and navigation by type of medication or therapy. Methodology and classification The scale we used was developed specifically for this project and seeks to answer two questions: How ready is an intervention for clinical use? What is the state of progress of scientific research on an intervention? In other words, the scale (seven categories) indicates what the available evidence tells us about the possible clinical use of a treatment and what is the nature, quality and possible evolution of this evidence. All the evidence used comes from the COVID-19 Living OVerview of Evidence (L·OVE) platform, an open access platform that is updated by searching 41 electronic databases, trial registers and other sources.","How to dive the sea of ​​evidence on COVID-19 to come up with the necessary answers before running out of air? This challenge that we face with our partner Epistemonikos. We created a solution that classifies interventions against COVID-19 according to their effectiveness, proven with the latest and best available evidence. This is how the “Scientifically Proven” repository was born. Our tool completed more than one year in operation, being updated every week to determine the effectiveness of the different interventions against COVID-19 and, in this way, contribute to safe and informed decision-making by social and political actors, health professionals, as well as the entire Latin American and world population. Currently, the repository continues to be updated without interruptions and, in addition, it is translated into the Portuguese and English languages, in order to reach even more people throughout the world. We have the purpose of bringing scientific evidence closer to where health decisions are made, which affect us all individually and collectively. The best way to prevent health and public health problems is by considering evidence in decisions, but for this, evidence must be properly organized and accessible. This is an example of what science and journalism can achieve together against  COVID-19.","Before the COVID-19 pandemic, the scientific community had concerns about poor-quality science being preprinted and then widely disseminated. But now, everyday people are reading them too and the media is covering them at a rate that far outpaces pre-2020.  In Latin America, we faced a big problem: most of the journalists don’t know how to find strong scientific evidence and sources and translate it into Spanish. That is one of the most important reasons that we launched Scientifically Proven, a tool that is helpful for them and for the public. It provides a snapshot of the research on the coronavirus, but does not constitute medical endorsements. What kind of evidence do we use? The primary source of information for estimating the effect of each intervention is randomized trials. Other evidence is used as a complement, especially when there is no evidence from randomized trials. We also always recommend consulting your doctor about treatments for COVID-19.","Spanish, English​, Portuguese",https://saludconlupa.com/comprueba/cientificamente-comprobado-un-analisis-de-los-tratamientos-mas-usados-contra-el-covid-19/,https://saludconlupa.com/comprueba/scientifically-proven/,https://saludconlupa.com/comprueba/cientificamente-comprovado/,,,,,,"Fabiola Torres, Gabriel Rada, Jason Martínez","Fabiola Torres is an investigative journalist from Perú. She is co-founder and director of Salud Con Lupa. She is an International Center for Journalists Knight Fellow and a member of the International Consortium of Investigative Journalists (ICIJ). Jason Martínez is co-founder & CTO of Salud con magnifying glass. He designs and builds apps with a focus on civic tech and data journalism. Gabriel Rada is Associate Professor at the Faculty of Medicine and director of the Evidence Center at the Pontificia Universidad Católica de Chile. He is the co-founder, president and CEO of the Epistemonikos Foundation, a non-profit organization whose mission is to bring independent, high-quality information to all those who make health decisions, through the use of information technologies.               ",
United States,Quartz,Big,Shortlist,,Fashion brands aren't keeping their Instagram diversity promises,16/03/21,"Investigation, Database, Infographics, Chart, Arts, Lifestyle, Business, Culture, Economy, Human rights","Scraping, D3.js, Python, Node.js","A year after fashion and beauty companies took to Instagram en mass to show support for the Black community and the Black Lives Matter movement, our analysis of 27,000 images posted by 34 brands showed that while many did increase the diversity of skin tones in their Instagram images, the increases were often only marginal. Light skinned models still prevail. We made this readily apparent with interactive and static data visualization.  ",The piece was one of the more widely read items on our site and was especially well read by members of the fashion and beauty industry. Researchers of inequity and company representatives reached out asking us to share our data and methods so that they could bring better accountability to their organizations and study it further. Influencers shared the story and graphics with their followers. Fashion influencer Bryanboy called it “very essential reading” Later in the year our data and graphics were included in an episode of an episode of the The BoF Show on Bloomberg TV.,"First we used custom built tools to collect and store Instagram posts using python and node. Then we constructed a database front-end that allowed us to evaluate and categorize every image we collected. That piece of software was written in node. We then analyzed our data using the python library pandas. Visualized the data using HTML, CSS, and D3.js and added interactivity using javascript.  The visualizations have three modes to allow readers to explore the data. A timeline view, a clustered gradient view, and a combination of the two—a view of two clusters, split by whether the post was from before or after Blackout Tuesday. These three modes deftly showed how long brands stopped posting to Instagram during the US unrest, the distribution of skin tone depicted on a brand’s account, and how that distribution changed after Blackout Tuesday. In all three views, dots can be tapped or moused-over to reveal the image it represents. We size-optimized the photographs using the command line tool imagemagick.  ","Collecting this data was extremely hard. Instagram does not have an API and the site will block IP addresses that it perceives as trying to harvest data. Nevertheless, we devised ways to both collect the data without violating the site's terms of use—and avoid being blocked. But that was just the start. We then used the software we wrote to evaluate each image by hand, establishing the number of people in the image, their skin colors, and whether or not the image was suitable for inclusion in our analysis.   ","Firstly, our project is a great example of how to hold organizations accountable through data. Second, it shows the opportunity for journalists to create data where none previously existed. There was no dataset of the skin tones of models promoted by fashion and beauty brands, despite the information being in plain view. We were willing to put in the work, and made a first-of-its kind dataset.   ",English,https://qz.com/1971689/,https://qz.com/1981716/,,,,,,,"Amanda Shendruk, Marc Bain, David Yanofsky","Amanda Shendruk is a visual journalist on Quartz’s Things team. She reports at the intersections of code, data and design. Marc Bain was Quartz's fashion reporter. He covered anything and everything related to clothes and footwear, whether sneakers or luxury, business, or design. David is the editor of Quartz’s Things team, the publication’s cohort of journalists who use code-based methods to originate and execute their stories.  ",
Netherlands,NOS,Big,Shortlist,,Social housing? Get to the back of the queue,24/04/21,"Investigation, Long-form, Database, Open data, News application, Podcast/radio, Illustration, Infographics, Chart, Video, Map, Audio, Economy","Animation, Personalisation, Adobe Creative Suite, Microsoft Excel, Google Sheets, CSV, Python","In The Netherlands the waiting times for social housing are long. Yet nobody knew how long exactly: somehow no part of the government keeps tabs on this. So we set out to find the exact waiting time for social housing, for every single one of the 355 municipalities in the country. We requested data from the 300+ social housing corporations in the country, to find that in at least a quarter of the municipalities you’ll need to wait for more than 7 years. In some parts of the country waiting times were as high as 20 years - or more.","Our investigation added to the national society wide debate about the current housing crisis, its consequences and possible solutions. Following our addition to the debate the housing crisis was high on the agenda of the newly formed political coalition. Who, when presenting themselves merely weeks ago, installed a minister of housing as part of the Ministry of Interior and Kingdom Relations. We published across different media - online news article, online interactive, radio reportage, tv-broadcast - on the last Saturday of april 2021. Come Monday the new insight on exact waiting times for every municipality dominated the frontpages of all newspapers - both national and regional, as well in local and regional broadcasts. As the national public news broadcast, our reporting reaches a great audience. Yet by sharing the outcomes and data with regional news outlets, our impact further increased. Our results can be seen in an interactive that can be personalized on a municipality level, an explainer-video on Youtube, and on our site and newsapp, reaching over hunderds of thousands unique visitors. The articles were viewed 400.000+ times, the interactive has had 400.000+ unique visitors, and the YouTube video has 280.000+ unique views. Together with our television- and radiobroadcasts, which usually reach millions of people.","Most of our research consisted of classic journalistic slog e-mailing the 300+ corporations responsible for social housing throughout the country. Many of them did not answer our e-mails, so we ended up calling those corporations, explaining what we were trying to do and asking them for data. Once corporations agreed, we had to get the numbers out of the files they sent us (PDF’s, Word documents, annual reports and emails) and into our spreadsheet. For data analysis purposes we mostly used Google Spreadsheets and Python Pandas - though most of it was spreadsheet based. The Netherlands is made up of a multitude of municipalities and in most of them the social housing is dealt with by multiple corporations. We therefore needed to calculate weighted average waiting times, to make sure we had a good understanding of the situation in every municipality. (We used a threshold of 75% of all social housing in a municipality; if we had less than 75% of results for a given municipality, we showed no data.) For our interactive we wanted to provide readers/visitors with more contextual data; these numbers, mostly open data, were also collected and edited in the spreadsheet. The interactive was built using the Nuxt.js and designed with Adobe XD. For graphics Adobe illustrator was used, and After Effects for animations.","The hardest part of this project was the sheer number of unknown unknowns we had to overcome, and the vast number of sources needed to gain insight into waiting times for social housing. The Netherlands is made up of a multitude of municipalities and in most of them the social housing is dealt with by multiple corporations. Every organization had a slightly different definition of waiting time, active and passive. We ended up resolving this with personal contact - explaining our needs to many organizations.","It’s always worth asking simple questions even when people think the answer is obvious. Everyone “knew” that waiting for social housing was a matter of years and years, instead of months. Yet nobody knew exactly how long the wait was. We found out why nobody knew soon enough: the information needed to get to an average waiting time on a municipality level was scattered among hundreds of organizations. Second lesson for others - one story begets many more. By building an interactive website where users need to fill in their municipality, we ended up with 355 different stories: one story for every municipality. So instead of spending months apparently working on one production, think of it as time well spent telling 355 different stories. And finally, note how this story did not start with a simple downloaded dataset. Some data-driven investigation require you to build your own dataset, e-mailing and calling hundreds of sources.",Dutch,https://app.nos.nl/op3/socialehuur/#/?gemeente=alkmaar,https://www.dropbox.com/sh/w64ubzr3fc9b15i/AACdGeO9qOeyxkT1hSvVJs6ga?dl=0,https://www.youtube.com/watch?v=7RBQ60O8SRs,https://nos.nl/op3/artikel/2377995-sociale-huurwoning-in-zeker-een-kwart-van-de-gemeenten-wacht-je-meer-dan-7-jaar,https://nos.nl/video/2378050-22-jaar-wachten-op-je-sociale-huurwoning-krijgt-emma-er-een-voor-haar-veertigste,https://www.nporadio1.nl/fragmenten/nos-radio-1-journaal/6fdc677b-2ea7-4d39-93a1-31068711486f/2021-04-24-langere-wachttijden-voor-een-sociale-huurwoning,,,"Daan Kool, Lars Boogaard, Cheuk-Ming Tang, Winny de Jong, Jurjen IJsseldijk, Emil van Oers, Leen Kraniotis, Wessel de Jong, Jeroen Schutijser, Emma Jackson, Jos Stolper, Stephan Vegelien en Hugo Janssen.","NOS is the national public news broadcaster of the Netherlands. NOS op 3, part of NOS, covers the news for an audience between ages 18 and 35 years. For this investigation researchers, (data) journalists and designers from both the NOS economics desk and NOS op 3 collaborated intensively:  Daan Kool: research Winny de Jong: research, data analysis Lars Boogaard: data visualisation, design and animation Cheuk-Ming Tang: development Jurjen IJsseldijk: editing and project management Emil van Oers: research, script, presentation and video-editing Leen Kraniotis: research, production Wessel de Jong: tv reportage Jeroen Schutijser: radio reportage additional support was provided by Hugo Janssen, Emma Jackson, Jos Stolper, and Stephan Vegelien.",
Singapore,The Straits Times,Big,Shortlist,,Saving Singapore's Shores,22/12/21,"Solutions journalism, Fact-checking, Chart, Video, Map, Satellite images, Environment","Animation, 3D modelling, Canvas, Adobe Creative Suite, Google Sheets, CSV, OpenStreetMap","To drive home the urgency of climate change, this story provides a simulation of how Singapore will be affected by rising sea levels, using the Merlion — Singapore’s landmark – as a scale. Using data from the latest IPCC report, we visualized the best- and worst-case scenarios for sea-level rise by 2100. The story also highlights the impact of climate change on extreme weather events, and the measures that are put in place to protect the low-lying island-nation – from nature-based solutions to infrastructures to funding climate research.","The latest IPCC report garnered much attention to the topic of climate change, but most of the contents of the report remain too complicated for many. We are making this topic more accessible through the use of the 3D simulation and animated graphics to explain the complicated sea-level processes. This project will also be displayed at an exhibition at the National Museum of Singapore from Jan 14 through Feb 8. The exhibition, called ""Through the Lens,"" is a celebration of the best in visual and interactive journalism. It explores the impact of global climate change on Singapore and showcases how even a small country can do its part to tackle the challenges of the crisis.","This project uses 3D modelling in Unreal, and it required scaling the models manually to make sure they are accurate. The 3D simulation is then exported as a video and we used our video scrolly template to display them on the web. The animated graphics were created on Adobe illustrator, then animated using Adobe AfterEffects. The 3D map was created in Blender and was also exported as a video. In addition, we also used datawrapper for some of the simple charts.","The hardest part of this project was making sure we are accurate. First of all, the IPCC report itself is complicated and not easy to understand. Terms such as ""ice-cliff instabilities"" were totally new to us. The sea-level projections also come with caveats to the data. We consulted with researchers at the Earth Observatory of Singapore, who helped us understand the complex sea-level processes and made sure we were explaining and visualizing them correctly. Another challenge in creating this project was striking a balance between showing all the scenarios – even the extreme worst-case scenario, while also delivering the message that these are merely projections, and it's not too late to minimise the damage. We strived for this balance through adding annotations in the 3D merlion scrolly, and also having the second part of the piece solely to highlight many of the existing and planned measures to protect Singapore's coastlines.","We think it's important make complicated topics such as climate change more accessible and understandable to readers who would otherwise not seek out information on this topic. While the 3D visualisation took a lot of work and back-and-forth to get it right and accurate, we believe this is the best way to visualize this data and drive this story home. We also think it's important to consult with the scientific community to make sure we are representing the data as accurately as possible. In addition, as with any other climate change-related topic, it's important to strike that balance between the problem and the solution.",English,https://www.straitstimes.com/multimedia/graphics/2022/01/singapore-protect-sea-levels-rise/index.html,,,,,,,,"Charlene Chua, Luo Mingxuan, Ryan Tan, Rachael Lee, Stephanie Adeline, Zachary Tia, Zeke Tan",Charlene Chua - Digital Graphics Journalist; Luo Mingxuan - Designer intern; Ryan Tan - 3D/Designer; Rachael Lee - Designer intern; Stephanie Adeline - Digital Graphics Journalist & project lead; Zachary Tia - Data journalist intern; Zeke Tan - Real-time Graphics Developer;,
Singapore,The Straits Times,Big,Shortlist,,"Supply chains, interrupted. Why a bicycle takes 40 days to reach Singapore",03/12/21,"Investigation, Illustration, Map, Satellite images, Economy","D3.js, Json, Adobe Creative Suite, Google Sheets","Over the past year, the world’s logistics network has seen unprecedented upheavals, affecting deliveries of everything from bottles of champagne to iPhones. The story uses a bicycle as a case study for the global delays in deliveries. We visualised a pre-disruption delivery timeline and contrasted it to the same delivery amid the global disruptions. The story then looked at the factors influencing the delays, as well as the results of them, with a focus on how congested ports had become since the beginning of 2021.","The topic of supply chain disruptions has been the subject of countless stories published over the past year but few have taken a predominantly visual approach to explaining the topic. It’s a complex subject that spans economics, politics, manufacturing, and a global pandemic. We simplified the idea by providing a scenario that most people can relate to — ordering a product online, making the subject and its impacts easy to understand.","For the introductory scolly section, we used our in-house scrollytelling component coupled with SVGs created in Adobe Illustrator. The SVGs contain multiple layers, which allowed us to manipulate them using CSS and Javascript. The ships and trucks are animated along SVG paths based on how far a reader had scrolled down the page, while other elements are animated using CSS keyframe animations. Elements like the wakes of the ships were made visible or hidden based on how far the ship had travelled since its starting position.  We created the second scrolly using satellite imagery sourced from SentinelHub. We manipulated the images in Photoshop and created SVG annotations in Illustrator and QGIS.  Lastly, the map component near the end of the story was created using D3.js with data from GoComet. The map is rendered as an SVG and is only visible on desktop. On mobile, it is replaced with a dumbbell chart.  ","The idea for the initial scrolly section took a long time to form. As a result, the ideation phase made up a significant amount of the overall time we spent on the project, and the introductory section went through several iterations. We started out wanting to create a globe and zoom into congested ports to display satellite imagery but decided we needed to ground the idea to make it more relatable.  We then settled on the idea of tracking a single product, though our initial thinking was just to have lines running down the page to show the relative trips of the bicycles before and after the disruptions. We eventually decided to go with illustrations to make a serious idea easier to approach and to move away from the abstraction of lines running down a webpage to something more literal that included actual modes of transport.  Another issue we encountered early on was dealing with the sheer amount of data needed to create a visualisation of ships traversing the globe, but as mentioned above we eventually dropped this idea.","Most interactive stories you see are visually quite serious, but it’s acceptable to lighten a serious topic with visuals that a child would understand. This is something we discussed a lot when putting the piece together. Our senior editors were worried about the seriousness of the topic and whether it would resonate with our readers. We brainstormed how we could go about engaging readers and decided to go with more “cutesy” animations as a way to draw readers into the story.",English,https://www.straitstimes.com/multimedia/graphics/2021/12/global-supply-chain-problems/index.html,,,,,,,,"Alyssa Mungcal, Carlos Marin, Charles Tampus, Christopher Udemans, Joyce Lim, Rachael Lee, Rodolfo Pazos, Xaquín G.V",Alyssa Mungcal: UI/UX designer Carlos Marin: Data visualisation developer Charles Tampus: Web developer Christopher Udemans: Graphics journalist and project lead Joyce Lim: Reporter Rachael Lee: Graphic design intern Rodolfo Pazos: Interactive graphics editor Xaquín G.V.: Data and graphics editor,
Singapore,The Straits Times,Big,Shortlist,,Remembering the 5 million lives lost to Covid-19,30/10/21,"Infographics, Chart, Health","D3.js, Json, Adobe Creative Suite, Google Sheets, R","Covid-19 was set to claim five million lives around the globe last October. However, the constantly growing death counts had left people numb and confused. This story took a unique visualisation approach in the hope to break the numbness. We connected the visual metaphor of condolence flowers with death counts in an attempt to humanize the number and serve as a reminder for lives that have been cut short by the virus.  ","The piece was one of the most well-read interactives of the month on our site. Given that it’s almost two years into the pandemic, it was quite a challenge for a repetitive theme to attract this much attention from the audience. In addition, this project was one of the first nontraditional charts our graphics desk has been published so far. It introduces more visual possibilities our newsroom can offer for our readers.","1) We conducted visual research and mood boarding in Miro to collect and organise references from publications that had put up visual pieces when the global Covid-19 death count passed the one million mark. And then we came up with the flower visualisation concept as a digital memorial to pay tribute to the lost lives.  2) Data wrangling and visualisation prototype are both done in Observable notebooks. The notebooks allow us to work collaboratively and transparently to easily iterate the design of a nontraditional look of a chart. We first used Arquero, a JavaScript library to perform dplyr-like data queries, to transform Covid-19 data sourced from Our World in Data (OWID). Then we imported the data from one data-wrangling notebook to another to test out different types of flowers with D3.js to encode time series data with animation.  3) We exported SVGs from Observable notebooks and refined the layout in illustrator. The top scrolly made with Vue.js then animate different layers in SVGs with CSS as users scroll through. The bottom exploratory tool adapted D3.js code from Observable notebook and turned it into a Vue.js component.  4) To engage our readers throughout, we relied on the classic Martini glass narration structure to lay out the key moments and details almost two years into the pandemic. It stats with explanatory scrolly to take readers through ebbs and flows of the pandemic globally. Next, it narrows down to profile five countries of interest and lastly it offered an animated exploratory tool for readers to learn more about the situation in each continent.  ",The most challenging part is conceptualisation since the topic has been covered extensively with all types of charts when the world first cross one million deaths. It took us quite some effort to come up with a visualisation solution that is both refreshing and meaningful. Another challenge lies in turning the intricate SVGs performant as they are rendered in a browser. It took trial and error to optimise the file size of our SVG with several layers.  ,The main takeaway from this project is how to report a topic that has been covered extensively. We deliberately stayed away from charts that other publications have already used. We also drew inspirations outside the visualisation world - the Japanese funeral floral arrangements - for an organic look of the layout.  This time we have also incorporated Observable into our workflow from data wrangling to visualisation. This allows us to make quick updates as live data came in from OWID. We were able to put together an interactive demo for our editors to facilitate the discussion.,English,https://www.straitstimes.com/multimedia/graphics/2021/10/covid19-5million-deaths/index.html,,,,,,,,"Arvind Jayaram, Charles Tampus, Christopher Udemans, Leonard Lai, Rodolfo Pazos, Spe Chen, Xaquín G.V.","Arvind Jayaram: Assistant foreign editor Charles Tampus: Web developer Christopher Udemans: Graphics journalist Leonard Lai: Assistant digital editor Rodolfo Pazos: Interactive graphics editor Spe Chen: Data visualisation designer and project lead Xaquín G.V.: Data and graphics editor Correspondents: Timothy Goh in Singapore, Debarshi Dasgupta in India, Arlina Arshad in Indonesia, Wahyudi Soeriaatmadja in Indonesia, Nadirah H. Rodzi in Malaysia and Elizabeth Law in China",
United States,"The Marshall Project, published in partnership with Louisville Courier-Journal and USA Today Network - The Marshall Project took the lead in all the reporting and data analysis.",Big,Winner,"This award combines two entries, recognizing the outstanding data and engagement journalism of Weihua Li specifically and of the team as a whole. We were blown away by the consistent excellence in data analysis and reporting, across a wide variety of stories. We very much agreed with the citation letter: The work ""challenges mainstream myths about crime and punishment and empowers community members to use data to hold the powerful accountable."" It is particularly impressive that such strong work was done on the subject of the incarcerated, a population that can be challenging to cover using the customary tools and techniques of journalism. The Marshall Project's effort to create a community of the incarcerated in order to center stories around those voices and experiences is an example for us all.",Millions of People with Felonies Can Now Vote. Most Don't Know it.,23/06/21,"Investigation, Open data, Infographics, Elections, Politics, Crime","AI/Machine learning, D3.js, Json, Google Sheets, CSV, Python","After several battleground states reinstated the right to vote for people formerly incarcerated for felonies, we undertook a complex data investigation that revealed that no more than 1-in-4 of them registered to vote in time for the 2020 election. We used text messaging to directly engage with people who were newly able to vote in Nevada, Kentucky, Iowa and New Jersey. Many of the people we spoke with didn’t know they were eligible to vote; our investigation showed that states do little to notify them of their restored rights.","This reporting opened the door to a legislative resolution in Nebraska to study voting among formerly incarcerated people. As the resolution states, “data and system errors have impermissibly disenfranchised eligible voters from participation in the election process.”  The resolution exists in large part because of Nicole Lewis’s nuanced reporting in the story, and Andrew R. Calderón’s detailed description of how to unearth and contextualize such data  that demonstrated to civil society groups like the ACLU that such a study is feasible and explained how to conduct it efficiently.  The nature of the system means impact will be slow to materialize, but as the pending legislation in Nebraska shows, we believe that over time the exposure of the ways voting participation is still stymied can affect tens of thousands of people. Our story was co-published by the Louisville Courier-Journal and USA Today Network, appearing in the Des Moines Register, the Reno Gazette-Journal and several Gannett newspapers in New Jersey. That helped bring the issue to the forefront nationally and in the states we analyzed. The project also raised awareness of the issue through media appearances on CBS News, NBC/Peacock on Zerlina Maxwell's show along with two more NBC Now appearances, and NPR and WBUR’s Here And Now.  It was also covered by Politico and Talking Points Memo.","Entity matching: We used Python to join the datasets. Early iterations of the project used natural language processing tools like Dedupe, but we were happily able to simplify the problem to the point where we could use more straight-forward techniques.   Surveying: We used Typeform to design and build a custom survey to embed on our site.   SMS (text messaging): We used the Twilio API to run our direct survey of formerly incarcerated people in Kentucky.    Cloud computing: We tracked and logged our SMS survey using Docker containers running on Amazon Elastic Cluster Service (ECS). We used Terraform to manage the stack, allowing us to quickly deploy serious computational resources but only pay for what we actually used and avoid long-term maintenance debt.   Observable notebooks for sharing results with the editorial team. These included editorial analysis, but also helped show the status and results of the SMS survey for internal use.   D3.js to visualize the percentage of potential voters who registered.","Our nuanced approach to understanding the data behind this project made this project unusual for a newsroom. Despite the fact that nearly every state purges people from the voter rolls once they go to prison, few states keep track of how many formerly incarcerated people re-register once they are released. This makes assessing the success of the re-enfranchisement laws incredibly difficult. We had to figure out how to do our analysis with imperfect and ambiguous data, using only publically accessible records.  We developed a methodology for joining voting records with release records that was conservative but accurate (“at least one in four”). By using complex logic based on release date, age at release, and name, we were able to set an accurate but fair minimum value. Another challenge was building relationships with formerly incarcerated people who had been re-enfranchised. When we did our survey, we got negative responses from people who it seemed didn’t want to talk about their experience with the criminal justice system. There is a stigma associated with incarceration . Upon release, people often want to focus on re-entry rather than their time behind bars. Fortunately, we came in contact with a source through our SMS campaign in Kentucky who was willing to share her story, and her anger that the Kentucky government had not done more to inform her that she had the right to vote in the 2020 election.","Generally speaking, this project demonstrates that entity resolution problems in datasets from multiple sources can sometimes be solved with simple techniques available to many newsrooms. It also demonstrates how journalists can answer a question that the government has not answered using the government’s own data from multiple agencies. And it shows how journalists can use release records and data about prisoners to answer questions about a topic like voting, and similarly how voter rolls are a powerful source of useful information for many kinds of journalistic inquiry.   More specifically, journalists can learn from Andrew R. Calderón’s open methodology on how to measure voting patterns of people released from prison. For practical reasons, we did not consider all states that re-enfranchised people with felony records. In addition, the analytical techniques we used are not limited to states that recently gave people with felony records the right to vote – these techniques could be applied to other questions about the voting patterns of formerly incarcerated people.",English,https://www.themarshallproject.org/2021/06/23/millions-of-people-with-felonies-can-now-vote-most-don-t-know-it,https://www.themarshallproject.org/2021/07/01/how-many-people-convicted-of-felonies-are-registered-to-vote-in-your-state,,,,,,,"Nicole Lewis, Andrew Rodriguez Calderón, David Eads, Susan Chira","Story and engagement reporting: Nicole Lewis, Andrew Rodriguez Calderón Data and engineering: Andrew Rodriguez Calderón, David Eads Editors: Susan Chira and David Eads",
United States,ProPublica,Big,Shortlist,,Sacrifice Zones: Mapping Cancer-Causing Industrial Air Pollution,02/11/21,"Investigation, News application, Map, Environment, Health","Personalisation, D3.js, QGIS, Json, Adobe Creative Suite, Microsoft Excel, Google Sheets, CSV, R, PostgreSQL, PostGIS, OpenStreetMap, Python","In a groundbreaking interactive-first investigation that the EPA’s own staffers praised as “a wake-up call,” ProPublica revealed more than 1,000 hot spots of cancer-causing industrial air pollution that the agency allowed to take root across America. These are “sacrifice zones.” Residents pay the price so that consumers can enjoy products made there. We captured the ways EPA has failed to protect the public, not just through weak policies, but through deliberate choices recounted to us on the record by insiders. This project was conceived by journalists on our interactive data team and grew to include expertise from across our newsroom.","The project, which the EPA’s own staffers praised as “a wake-up call” and “a huge bucket of cold water in the face,”  led to the kind of impact environmental advocates said they had been working for decades to achieve. Two days after the first parts were published, the EPA announced that its administrator Michael S. Regan would visit the communities we featured; on his tour, he said the agency had “looked very carefully” at ProPublica’s reporting and was “incorporating much of it” into plans for reform, which include increasing air monitoring and enforcement and reexamining the way the agency assesses cancer risk. New cumulative risk assessment guidelines are expected to be released in early 2022, along with an updated “more robust” analysis of air pollution. In response to our reporting, officials launched air monitoring efforts in Laredo, Texas and Pascagoula, Mississippi.  The investigation, which we distributed to impacted communities through an unprecedented engagement effort, also led to a groundswell of activism among residents, many of whom said they had been unaware of the dangers they’d faced. Residents lobbied for air monitoring, packed town halls, circulated petitions, started neighborhood health surveys, and called for the CDC to conduct blood testing on schoolchildren.  More than 60 local TV stations aired segments about our analysis; at least 16 local newspapers did the same. Their stories extended our impact — an article in a Michigan newspaper led state officials to investigate a polluter that had never been permitted; a Missouri television station’s report prompted such outrage that the EPA called a meeting in Verona, Missouri in response to community outrage over the extreme cancer risks we revealed. “As soon as I saw that report, I knew I needed to come down here tonight,” Greg Winters told us at the meeting. “It pissed me off.”","We used five years of the EPA's Risk-Screening Environmental Indicators (RSEI) database, along with EPA's Toxics Release Inventory to generate estimated additional cancer risks from industrial emissions down to 810 x 810 meter grid cells for the whole country. We averaged values over the years 2014-2018 to get a better idea of long term exposure. We used US Census data to determine racial disparities in cancer risk from industrial emissions. We obtained all of this data through online government servers.    At around 7 billion rows, our data was too large to use the analysis tools we’d normally use, so we turned to  Google BigQuery. Using BigQuery, we were able to compute cancer risks at an incredibly high resolution -- each grid cell in our analysis represents a quarter of a square mile of the country. We used Ruby and Python to write a clustering algorithm that generated ""hot spots"" around areas that represented estimated lifetime cancer risks from industrial emissions of above 1 in 100,000. We used R to do our race analysis. We used Ruby, Python (with rasterio) and Photoshop to generate static maps of high risk zones. To make our interactive map, we took the result of our analysis and compressed it into mbtiles format with tippecanoe. We then designed the interactive map with Mapbox Studio and wrote a JavaScript web application using Vue.js and the Mapbox API to geocode user input, query the data and surface it for readers. The web application also included d3.js charts for individual results.","Five years of EPA modeled industrial chemical concentrations added up to about 7 billion rows of data. Turning the disaggregated concentration data into cancer risks required learning how big data systems worked, and then learning how to distill the outputs into something that could be served in a web application and queried on the fly. It took the better part of a year to develop that pipeline. But once we had initial findings, we ran into another issue: the quality of the government’s data. Because the EPA doesn’t directly monitor the air, it accepts self-reported emissions estimates that companies often derive using flawed formulas. The EPA does little to check the accuracy of these numbers and failed to catch major errors that our reporters began to spot. To publish an analysis we could trust, the entire reporting team undertook a vast, weekslong data quality scrub that the agency had never bothered to do. The scrub led more than two dozen facilities to correct their data with the EPA and for agency officials to admit that the EPA needs to do a better job of ensuring data integrity. We then wrote software to reintegrate those updated submissions into our overall analysis. From the very start, we recognized that all of this technical work would amount to little, however, if it neglected to serve the people in these hot spots. We launched the most ambitious and far-ranging community engagement endeavor ProPublica has ever undertaken to make sure our work reached those most impacted by the risks we’d uncovered. We reported on the ground in 10 states and mailed postcards to 8,800 homes. In the end we heard from more than 1,000 impacted residents across 34 states, many of whom had been unaware of the dangers posed by nearby facilities.","Cross-newsroom collaborations yield incredible results. The custom analysis undertaken by our newsroom’s data journalists for over a year laid a foundation upon which we could tell uniquely authoritative stories. Seven reporters then joined in the effort to illuminate how and why these hot spots came to be. Together, the team distilled our findings— powered by billions of rows of data, countless records and scores of interviews— into lucid language with a clear presentation. Our visuals team, for instance, developed an interactive graphic to teach readers about cumulative risk—a concept they needed to see to understand. We learned that giving readers such an intimate and personalizable look at a problem makes for effective storytelling. Readers had strong emotional reactions to being able to plug in their address and get a precise view of the estimated industrial cancer risk where they live for the very first time. Especially since no one else had ever compressed, processed and made this data accessible in this interactive form. Our map quantified a problem in many places that was previously anecdotal, allowing residents in the most marginalized communities to point to hard data when discussing the risks in their neighborhood.  Visualizing and publishing a government’s agency data in such a granular way can also drive change and conversations among policymakers. EPA employees told us that our presentation has led the agency to improve its own data analysis efforts.  Finally, the careful way in which we approached our analysis helped it be taken seriously by experts who might otherwise dismiss work by non-academics. Prior to publication, we invited air toxics scientists to give us feedback during map demonstrations and went over our detailed methodology with them, word for word. This was hard and unglamorous work, but it demonstrated to key stakeholders that our approach was sound.","English, Spanish",https://projects.propublica.org/toxmap/,https://www.propublica.org/article/toxmap-poison-in-the-air,https://www.propublica.org/article/how-we-created-the-most-detailed-map-ever-of-cancer-causing-industrial-air-pollution,https://www.propublica.org/article/the-dirty-secret-of-americas-clean-dishes,https://www.propublica.org/article/whats-polluting-the-air-not-even-the-epa-can-say,https://www.propublica.org/article/they-knew-industrial-pollution-was-ruining-the-neighborhoods-air-if-only-regulators-had-listened,,,"Lylla Younes, Al Shaw, Ava Kofman, Lisa Song, Max Blau, Maya Miller, Kiah Collier, Alyssa Johnson, Ken Ward Jr., Jeff Kao, Lucas Waldron","Lylla Younes and Al Shaw are interactive data reporters and news applications developers. Ava Kofman, Lisa Song, Max Blau, Maya Miller, Kiah Collier, Alyssa Johnson and Ken Ward Jr. are reporters. Jeff Kao is a computational journalist. Lucas Waldron is a visual investigations producer.",
United States,ProPublica,Big,Shortlist,,Unchecked: America's Broken Food Safety System,29/10/21,"Investigation, Explainer, Long-form, Database, News application, Crowdsourcing, Business, Agriculture, Health","D3.js, JQuery, Json, PostgreSQL, Python","We analyzed genomic sequencing data and used phylogenetic tree visualization software to show that even after the CDC ended its outbreak investigation into salmonella infantis, the dangerous and drug-resistant strain was still running rampant through the chicken industry and sickening tens of thousands of people. The unchecked spread of this strain is emblematic of America’s baffling, broken food safety system, which is ill-equipped to protect consumers or rebuff industry influence. We also built an interactive database using the USDA’s microbiological sampling data to allow consumers to look up the salmonella rates of the plants that produced their chicken or turkey.","A week before the first story was published — after about a month of interview requests about our findings — the USDA announced that it was rethinking its approach to salmonella. In November, the department asked a key advisory committee for suggestions on how to improve its testing program to focus more on public health risks. In particular, the USDA said it wanted recommendations on how to focus on the riskiest types of salmonella, how much salmonella was present and how to better control the bacteria on farms — all vulnerabilities highlighted by ProPublica's reporting. And in early December, the USDA asked poultry companies for project proposals to test new strategies for reducing contamination. ProPublica's reporting also spurred one of the country's leading food safety lawyers, Bill Marler, to threaten to sue the USDA if it didn't respond to his long-pending petition to ban the sale of raw meat and poultry tainted with certain types of salmonella — including infantis. Our Chicken Checker app engaged thousands of consumers across the country who, for the first time, were empowered to make more informed shopping decisions. And through the app, we received nearly 900 submissions from people who collected information from packages of poultry, which allowed us to see which supermarkets had received poultry from the most problematic plants. Several readers wrote in to say that they had sought to avoid industrial poultry processors and were surprised to learn through Chicken Checker that the organic, free-range chicken they paid a premium for was actually processed by a big chicken company. One Maryland reader called our investigation ""eye opening and upsetting piece."" ""Congress is supposed to be protecting us, the consumer, and yet they are constantly letting us down by siding with the very industry they are supposed to be protecting us from,""","Data reporter Irena Hwang used a combination of command-line tools, DB Browser for SQLite and various Python open-source libraries. Hwang used command-line tools to obtain data from the NCBI Pathogen Detection Browser’s (https://www.ncbi.nlm.nih.gov/pathogens/) public API and DB Browser for SQLite to convert raw TSV files into query-able SQL databases. Then, Hwang used the Jupyter Lab user interface to write Python scripts for combining and analyzing data from public APIs and state and federal information requests, using Python packages including pandas and sqlite3. Hwang also used the Interactive Tree of Life (https://itol.embl.de/), software developed by researchers in Germany, to visualize phylogenetic data.   News applications developers Andrea Suozzo and Ash Ngu combined 15 datasets, including the USDA’s list of registered poultry processing plants and corresponding salmonella sampling data, into a PostgreSQL database. They adapted federal regulatory methodologies used to evaluate salmonella prevalence in plants to focus on the types of salmonella most likely to cause human illness, then built a front-end searchable interface to surface and visualize that data using Ruby on Rails and D3. The Chicken Checker app showed consumers how to find the plant code, which may appear in several places on raw poultry packaging, and presented information about the plant’s salmonella record in an easily understood format.","The project depended on analysis of data that was new to ProPublica, particularly genomic sequencing data. Given the highly specialized nature of this data, much of our reporting focused on gaining a thorough understanding of the origin and scope of the data, identifying which analyses were most informative and useful, finding and familiarizing ourselves with the right software for analysis, interpreting our results and verifying those results with federal agencies and nearly a dozen outside experts. ","ProPublica is excited to bring to the attention of other journalists underutilized databases like the NCBI Pathogen Detection Browser and USDA Food Safety and Inspection Services Laboratory Data. We believe that these databases can and should be used for additional reporting on food safety and public health, and accountability stories about the federal agencies that gather and review this data in order to regulate industries. We also believe that this story can help expand the definition and scope of data journalism to a field that can leverage even the most esoteric datasets from academic science. Investigative stories are often data-driven, and we believe that our story and Chicken Checker news application help expand that definition to include “science-based” and “public-service oriented.”",English,https://www.propublica.org/article/salmonella-chicken-usda-food-safety,https://projects.propublica.org/chicken/,https://www.propublica.org/article/how-propublica-used-genomic-sequencing-data-to-track-an-ongoing-salmonella-outbreak,https://www.propublica.org/article/your-free-range-organic-chicken-may-have-been-processed-at-a-large-industrial-poultry-plant,https://www.propublica.org/article/when-dangerous-strains-of-salmonella-hit-the-turkey-industry-responded-forcefully-the-chicken-industry-not-so-much,https://www.propublica.org/article/the-low-and-slow-approach-to-food-safety-reform-keeps-going-up-in-smoke,https://twitter.com/propublica/status/1454886464433659905?s=20,,"Irena Hwang, Andrea Suozzo, Ash Ngu, Michael Grabell, Bernice Yeung, Mollie Simon, Maryam Jameel","Irena Hwang is a data reporter at ProPublica. Andrea Suozzo is a news applications developer at ProPublica. Ash Ngu is a reporter, designer and developer with ProPublica’s news apps team. Michael Grabell is a ProPublica reporter who writes about economic issues, labor, immigration and trade. He is a two-time Pulitzer Prize finalist. Bernice Yeung, who covered business with a focus on labor and employment for ProPublica, is managing editor of the Investigative Reporting Program at UC Berkeley. Mollie Simon is a research reporter at ProPublica. Maryam Jameel is a ProPublica engagement reporter working on community-sourced investigations.",
United States,ProPublica,Big,Shortlist,,What Parler Saw During the Attack on the Capitol,17/01/21,"Breaking news, Documentary, News application, OSINT, Video, Politics","D3.js, QGIS, Json, Google Sheets","Shortly after the attempted insurrection on Jan. 6, we were the first news organization to publish the majority of these videos, which had been uploaded to the then-defunct social media service Parler. Although that system had been taken down by its hosts, we received and combed through a trove of thousands of video files collected by an online group that had archived them. The result of our work was a harrowing interactive and social media-like experience that let users experience the riot as though they were in the midst of its participants.  ","Readers responded strongly, making it one of the most-viewed features on the site in 2021. The Department of Justice cited the videos we published dozens of times in documents charging  insurrectionists with crimes committed that day, and the videos were played countless times in Congress during former President Trump’s impeachment trial.  Perhaps most crucially, because these videos were made inaccessible when Parler’s web host took it off the Internet, if it weren’t for our project, all of this documentary evidence might have been lost.","This project was a huge technical undertaking. The initial cache of videos was over 30 terabytes, a truly enormous amount of data. We had to use metadata and write code to narrow down the videos to a reasonable number to review.  We put out a call to the rest of the newsroom and asked for volunteers to review the videos so we could surface the  germane and newsworthy videos from the day. ProPublica journalists watched and tagged hundreds of videos in a spreadsheet. We also needed to think through the experience we wanted readers to have. We wanted it to be easy to navigate and tell a gripping, unfolding story, but also let them specify which parts they wanted to see. We color-coded the videos and organized them by time, creating a timeline scrubber that is its own data visualization: because the timeline is color-coded, you can see at a glance how over time the videos go from outside of the Capitol complex, to inside the building itself.  Further, video is not easy or cheap to deal with. We had to transcode all of the videos to create versions of the files that we could serve to users, including those on mobile data connections. What’s more, some browsers crash when you load too many videos at once, so we had to create technical workarounds to make it possible for browsers to handle that many videos. ","The sheer size of the original dataset — 30 terabytes comprising many hours of footage — made it a complex project from the start. The data we got from our sources included the full EXIF metadata, we were able to narrow the trove down to using timestamps (starting from Trump’s speech through the end of the day) and geographic coordinates (in or near the Capitol).. However, the EXIF data was messy and inconsistent across different devices so we needed to be careful to avoid missing pieces of video evidence. More than  35 ProPublicans contributed to this project. They watched and tagged videos to augment what we knew about each of them past what the metadata could tell us, which helped  narrow down to just videos we wanted to publish. Corralling dozens of colleagues into a Google Sheet together on such a tight timeline was hard work.  Really, the main challenges here given the scope of information we were working with was speed. We’re not a breaking news organization, but we sprinted and worked together to make sure we got these videos to the American people as soon as we could. We consider it a public service.","Sometimes an event comes around that is so momentous you need to drop everything to cover it. Most journalists know this, but knowing how you can make an impact on a national, fast-breaking story is hard.  We stuck to our strengths: Computational journalist Jeff Kao found a source with a huge cache of data, and collaborating with our news apps editors created a way for the entire organization to pitch in and help. The news apps team then got to work immediately sketching out how we could present videos to people in a compelling and meaningful way.  While other news organizations went with curated walk-throughs of the day, we realized there was power in a minimally-filtered and immersive piece. Giving people the “Parler-eye view” of the day gave people the ability to experience the day through the eyes of those who posted videos from it. That way of looking at it turned out to be powerful.",English,https://projects.propublica.org/parler-capitol-videos/,https://www.propublica.org/article/inside-the-capitol-riot-what-the-parler-videos-reveal,https://www.propublica.org/article/why-we-published-parler-users-videos-capitol-attack,https://www.propublica.org/article/capitol-eugene-goodman,,,,,"Lena V. Groeger, Jeff Kao, Al Shaw, Moiz Syed, Maya Eliahou, Alec MacGillis",Lena V. Groeger is a Deputy Editor on the News Apps team at ProPublica. Jeff Kao is a Computational Journalist at ProPublica. Al Shaw is a Deputy Editor on the News Apps at ProPublica. Moiz Syed was a News Apps Developer at ProPublica. Maya Eliahou is a Visual Producer at ProPublica. Alec MacGillis is a Reporter at ProPublica.,
United States,"The Palm Beach Post, ProPublica",Big,Winner,"This data Journalism team produced changes that will have a direct effect in people´s lives and health. The experts advice and citizen participation building a dataset from scratch , together with a beautiful presentation full of maps, evidence and stories , makes this investigation inspirational and proves the power of open collaboration as a step forward for real impact journalism.",Black Snow: Big Sugar's Burning Problem,08/07/21,"Investigation, Explainer, Solutions journalism, Long-form, Multiple-newsroom collaboration, Documentary, Crowdsourcing, Infographics, Chart, Video, Map, Satellite images, Environment, Business, Agriculture, Health, Economy, Employment","Sensor, Scraping, D3.js, QGIS, JQuery, Json, Adobe Creative Suite, R, RStudio, PostgreSQL, PostGIS, Python","For years, residents living amid Florida’s sugar fields have complained about cane burning, a harvesting method that chokes communities of color with smoke and ash. Yet sugar companies and regulators have reassured people that the air is healthy.  The Palm Beach Post and ProPublica tested that proposition, using our own monitors to produce a first-of-its-kind analysis tying the burning to spikes in pollution, which experts said posed health risks. We also analyzed hospitalization records and even traveled to Brazil, where São Paulo officials have largely phased out burning after residents there voiced concerns similar to those of Floridians today.","Our investigation revealed that regulators depended on an unfit air monitor and measured pollution in a way that failed to capture the impact of cane burning. After we started asking questions, officials replaced the monitor, and federal lawmakers pressed to tighten the nation’s pollution standards.  Citing the reporting, Sen. Jeff Merkley, a Democrat from Oregon and one of the upper chamber’s leading voices on environmental justice, called for greater federal oversight to make sure a similar situation does not happen again. “What the predominantly Black and Hispanic communities living near cane fields in Florida have been put through is completely unacceptable,” said Merkley, who serves as chair of the Environment and Public Works Committee’s subcommittee on environmental justice.  Changes to the nation’s air-monitoring framework are necessary, Merkley said, “to make it harder for industries to bury evidence of the dangerous pollution levels they’re causing.” Moreover, the investigation prompted new research that will add air sensors in the sugar-growing Glades region and examine health trends this year — something Florida has failed to do. The study, funded by NASA, will be the most comprehensive effort of its kind in the area. The Post/ProPublica project is also reshaping the political debate in Florida, where both parties have long supported the sugar industry. In December, Democratic lawmakers introduced legislation to roll back a law that protects farmers from lawsuits over air pollution. And U.S. Rep. Charlie Crist, who previously served as governor and is now a contender for the post again, has pledged to push for “a shift away from burning and towards a cleaner harvesting process” if elected governor this year. He called for action in response to our reporting, saying “we cannot continue to turn a blind eye to the air pollution and health hazards this community is experiencing.”","To collect real-time air-quality data in the Glades, ProPublica and The Post collaborated with residents to set up low-cost sensors outside their homes in Pahokee, one of the towns that dot the area. These PurpleAir sensors constantly recorded air pollution levels over four months of the cane-burning season. The air-quality data was analyzed in tandem with burn permit logs and smoke plume projections from the state Agriculture Department. We sought to discern the relationship, if any, between cane burns and increased air pollution at residents’ homes while taking into account wind and other atmospheric variables that affect how smoke travels. Armed with the state’s smoke projections, we used mapping software to categorize each day based on whether smoke from the burns was projected to reach Pahokee. Our analysis of more than 100 days of data found repeated spikes in particulate matter, or PM2.5, on days when the state authorized cane burns and projected smoke would blow toward instead of away from town. Our sensors reported more spikes in PM2.5 between 9 a.m. and 8 p.m. — the hours when cane is burned and the resulting smoke may linger. In addition to the analysis of the air-quality data, we also gathered qualitative data about the effects of cane smoke using a text bot that surveyed residents whenever our sensors detected a spike in pollution. The residents were asked how the air smelled, how much smoke they saw in the air and what health-related reactions, if any, they had. To sign up community members, we called every sixth person on the voter rolls; we designed flyers and posted them on bulletin boards and community gathering spots; we attended a Zoom church service to discuss the effort; and we contacted local leaders and knocked on doors.","Until our investigation, reporting on cane burning in the Glades largely rested on anecdotal accounts and political spin. This project combined original records research and reporting, deep community engagement and a novel citizen science experiment using low-cost air-quality sensors to produce a first-of-its-kind analysis showing the link between cane burning and air pollution near residents' homes. The effort underscored why officials’ claims of safety have persisted for so long. Sugar cane burning is uniquely challenging to track because the burns last roughly 15 to 40 minutes and occur in fields across a 400,000-acre region. Air pollution analyses often rely on hourly or daily data from a regulatory monitor that can miss or mute short bursts of pollution. Our project, however, used PurpleAir sensors to measure air-quality levels in real time. The data allowed us to identify the existence of short spikes in pollution during cane-burning hours on days when the smoke was projected to blow toward Pahokee. The project was made possible through a deep level of community engagement. But outreach was particularly complicated because many of the same people who are affected by the seasonal burns also benefit from the industry’s role as one of the biggest employers in the region. Residents in the area also often have unreliable Wi-Fi, which made it challenging to find residents who wanted to and were able to host a PurpleAir sensor. The stakes for the billion-dollar sugar industry were high, and it challenged the project’s reporting and methodology at every turn. We published an explainer to address criticisms and walk readers through the complex science and regulation. Even then, one of Florida’s largest sugar producers mounted a public relations campaign in the Glades to discredit ProPublica, which it called an “activist, agenda-driven, online-only website.” The project has no corrections or clarifications.","PurpleAir sensors are a relatively recent innovation; the first version of the sensors was created in 2015. In the past few years, though, the sensors have undergone enough development and testing by universities, air-quality groups and regulators like the EPA that they are able to be used in a project where reporters work with residents to do their own monitoring of air pollution in places where the government's air-monitoring system is lacking. Since publishing “Black Snow,” the PurpleAir staff has emailed the reporters to say that “tons of groups have been following suit” and that “it really gave lots of folks who struggle with localized pollution ideas and courage to set up sensors and do their own research.” Journalists can do the same.  Additionally, the project’s reporters have already begun sharing the tools they used for community engagement that made this project possible. At the Investigative Reporters and Editors conference last year, Ramadan and Miller detailed how to use free text bot tools to get real-time feedback from dozens of sources at once. They also shared tips on conducting community research to then generate and launch tailored outreach strategies to reach communities where they are (this can range from letter-writing to door-knocking to hosting events, among others). Reporters from other news organizations have since reached out and said they have deployed these tips and tools in their own journalism. We expect that will only continue.","English, Spanish",https://projects.propublica.org/black-snow/,https://www.propublica.org/article/a-complete-failure-of-the-state-authorities-didnt-heed-researchers-calls-to-study-health-effects-of-burning-sugar-cane,https://www.propublica.org/article/burning-sugar-cane-pollutes-communities-of-color-in-florida-brazil-shows-theres-another-way,https://www.propublica.org/article/after-years-of-complaints-florida-improves-pollution-monitoring-near-burning-sugar-cane-fields,https://www.propublica.org/article/sugar-companies-said-our-investigation-is-flawed-and-biased-lets-dive-into-why-thats-not-the-case,https://www.propublica.org/article/we-reported-on-pollution-from-sugar-cane-burning-now-federal-lawmakers-want-the-epa-to-take-action,https://www.propublica.org/article/they-deserve-to-be-safe-candidates-call-on-florida-to-investigate-the-health-effects-of-sugar-cane-burning,,"Lulu Ramadan -The Palm Beach Post, Hannah Morse - The Palm Beach Post, Ash Ngu - ProPublica, Maya Miller - ProPublica, Nadia Sussman - ProPublica","Lulu Ramadan was an investigative reporter at The Palm Beach Post, where she worked from 2015 to 2021. She’s now an investigative reporter at The Seattle Times. Hannah Morse joined in 2018 and currently covers county government for The Palm Beach Post. Ash Ngu is a reporter, designer and developer with ProPublica’s news apps team. Maya Miller is an engagement reporter with ProPublica focusing on health, environment and housing. Nadia Sussman is a video reporter with ProPublica.",
United States,"NPR, KQED",Big,Shortlist,,Who Will Pay To Protect Tech Giants From Rising Seas?,27/07/21,"Explainer, Long-form, Multiple-newsroom collaboration, Video, Map, Environment, Business","Drone, QGIS, Google Sheets, Node.js","Coastal cities need billions of dollars to build defenses against sea level rise. Tensions are growing over where that funding will come from: taxpayers or private companies with waterfront property? This immersive project is a deep dive into a complicated subject, using interactive maps and drone photography to help convey the scope of the issue and the challenging terrain.","Our story is being used a tool for community engagement among several environmental justice groups in the Bay Area. They are using it to help their public understand sea level rise and how it affects them, relative to their large corporate neighbors.  After publication, Google and Facebook employees tweeted about the story publicly. Google and Facebook are both aware of our reporting. ","This project is a combination of original drone footage and photography, maps and reporting. The maps were made using QGIS, leaflet.js and lots of code to allow integration with Google Sheets. This project was built with NPR’s own in-house developed scrolly-telling based interactive template. The template makes it easy to integrate photo, video, interactive elements, and text.   ","Creating the data layers for Facebook and Google’s parcels was extremely difficult and confusing. Lauren Sommer and I (Daniel Wood) collected detailed parcel information from several different counties to create the data sources at the heart of this story. Some of the data was extremely large and hard to work with. Some of the parcels were held by third parties but rented to the companies. Other parcels were owned by the companies but as-yet undeveloped. Creating this layer with a cohesive methodology from messy public sources was very challenging.  Building a slippy map that flew and zoomed on scroll was essential to locating this story within a place. Working on a tight budget, we opted to use a free web mapping library called leaflet.js, rather than spend extra for something like Mapbox. But this came with several difficulties. One downside of this was that we had to populate the imagery with our own custom labels, and we had to have them work well on desktop and mobile. Another challenge was loading and painting the geo data while keeping the load times low. While Mapbox would allow you to preload this data into their tiles, our system doesn’t pre-bake tiles, making this impossible. On the plus side, loading the geo data in the browser allowed us to easily manipulate the layers with javascript and css. This made the water effect possible and the timeline of loading data later on. ",Animated maps such as these can be a good way to step laypeople through the factors and impacts of issues that otherwise can be very dry and difficult to grasp with text and photos alone. This project offers a couple examples of how one might approach it.,English,https://apps.npr.org/sea-level-rise-silicon-valley/,,,,,,,,"Lauren Sommer, Ryan Kellman, Ruth Talbot, Daniel Wood, Duy Nguyen, Alyson Hurt, Neela Banerjee, Lee Smith, JJ Haris, Kevin Stark, Meredith Rizzo, Marissa Leshnov","Reporting by Lauren Sommer Visual editing and production by Ryan Kellman Design and development by Ruth Talbot, Daniel Wood, Duy Nguyen and Alyson Hurt Map asset collection and editing by Daniel Wood and Ruth Talbot Editing by Neela Banerjee, with copy editing by Lee Smith Drone video by JJ Harris/Techboogie/KQED Additional reporting by Kevin Stark, KQED Additional production by Meredith Rizzo Additional photography by Marissa Leshnov for NPR",
Philippines,Rappler,Small,Shortlist,,Duterte government's ‘rubbish' files stall SC drug war case,24/02/21,"Investigation, Database, Chart, Map, Crime, Human rights","Microsoft Excel, Google Sheets, CSV","The series is an unprecedented look into the police’s files on drug war operations, including reports of deaths, and which took two years of litigation before the Supreme Court was able to compel them to submit. Rappler obtained a copy of those files and pleadings, revealing the documents were no better than rubbish reports whose only value was to delay a court decision on whether President Rodrigo Duterte’s drug war should be voided. The case is pending. A subset of the files revealed that killings were already endemic in a province outside Manila with little to no solution.  ","Four months after Rappler published the series, the former prosecutor of the International Criminal Court (ICC) requested an investigation into Duterte’s drug war and other extrajudicial killings. Providing context to the series was five years’ worth of Rappler’s reporting into the drug war, parts of which were cited 49 times in the ICC prosecutor’s request for authorization to probe Duterte ","The team used both Microsoft Excel and Google Sheets to serve as repositories of the data we collected from the drug war documents. We analyzed the data using the same set of tools available within these softwares. The team plotted the data using Flourish, in addition to using Google Maps, to give us the big picture of what we are working with. ","Getting the files was our first big hurdle, as providing us copies of the documents would get sources charged with contempt of court. We obtained the copy in 2020 just before the pandemic hit, meaning we had to sort through thousands of unorganized files as we navigated the challenges of reporting in the time of COVID-19. Transferring confidential and big files in the time of the pandemic was also difficult. Ultimately, it was the uselessness of data in the files that proved the most difficult to deal with – how do we find the story in the data after we found out there was scarce data after all? Parts 1 and 2 tell the story of what lack of data meant, while Part 3 was a result of finding the trend in a small subset. ","Rappler’s reporting demonstrates what a local newsroom under attack can achieve with collaboration among its journalists for a long-term investigative data project. The combined expertise of the reporters involved in the story provides an intimate yet system-spanning look at the failure of criminal investigation and judicial action in President Rodrigo Duterte’s blood-soiled central platform, the war on drugs. The project also signals the urgent need for technology in telling impactful stories, all while keeping the fundamental principles of journalism intact – the necessity of data journalism.",English,https://www.rappler.com/newsbreak/investigative/duterte-government-rubbish-files-stall-supreme-court-drug-war-case-part-one/,https://www.rappler.com/newsbreak/investigative/incomplete-submissions-supreme-court-show-poorly-documented-drug-war-part-two/,https://www.rappler.com/newsbreak/investigative/vigilantes-drug-war-bulacan-killings-part-three/,,,,,,"Lian Buan, Rambo Talabong, Jodesz Gavilan, Michelle Abad, and Pauline Macaraeg","Lian Buan covers justice and corruption for Rappler. She is interested in decisions, pleadings, audits, contracts, and other documents that establish a trail.  Rambo Talabong covers the House of Representatives and the local government sector for Rappler. Prior to this, he covered security and crime. He was named Jaime V. Ongpin Fellow in 2019 for his reporting on President Rodrigo Duterte’s war on drugs. In 2021, he was selected as a journalism fellow by the Fellowships at Auschwitz for the Study of Professional Ethics. Jodesz Gavilan is a writer and researcher for Rappler and its investigative arm, Newsbreak. She covers human rights and impunity, and also hosts the weekly podcast Newsbreak: Beyond the Stories.  Michelle Abad is a researcher-writer at Rappler. Possessing the heart and soul of a feminist, she is working on specializing in women's issues in Newsbreak, Rappler's investigative arm. Pauline Macaraeg is a digital forensics researcher at Rappler. Her work involves studying the digital landscape and finding ways to counter online efforts that undermine democratic institutions and values. She writes about the spread and impact of disinformation and harmful online content.",
Philippines,Rappler,Small,Shortlist,,TRACKER: The Philippines' COVID-19 vaccine distribution,01/04/21,"Database, OSINT, Chart, Map, Health","Microsoft Excel, Google Sheets, CSV","This is a regularly-updated tracker that shows the progress of the national vaccination program in the Philippines, which kicked off on March 1, 2021. It contains data on the number of vaccine doses administered to Filipinos so far, the number of doses per vaccine manufacturer delivered to the country, and maps on the status of the vaccine rollout per region.","The tracker complements official information from the government's national vaccination dashboard by providing details like the percentage of the Philippine population already vaccinated, the number of vaccine doses administered each day, and the country's speed of inoculation as compared to other countries in Southeast Asia.","We use Google Sheets to encode the data, then connect it to Datawrapper and Flourish for the graphs and maps. We also use data from Our World in Data to track the vaccination progress in ASEAN countries.","The hardest part is actually collecting the data, because data people like us would have to manually encode the numbers daily from the government's dashboard, which doesn't allow users to select and copy texts. Some figures from previous dates also get updated from time to time on the dashboard, so we would have to review past entries and match our figures with the government's numbers. In addition, there are varying sources for reports on each vaccine delivery arriving to the country, so the curation of this data is more difficult.","Because vaccinating the population against COVID-19 is a major effort to fight the disease, there should be more eyes on the progress and speed of the country's vaccination program. Yes, the government provides regular figures, but we should add more context to those numbers, to show our readers how fast or slow the vaccination is, where the program is doing well or not, and where the government may be lacking in attention and where there is opportunity for intervention by the private sector.",English,https://www.rappler.com/newsbreak/data-documents/tracker-covid-19-vaccines-distribution-philippines/,,,,,,,,Michael Bueza,"Michael Bueza is a data curator under Rappler's Tech Team. He is an IT graduate who joined Rappler in 2013 after working for a top IT company. He was part of the Research Team before he took on his current role. He usually works on data about elections, governance, and the budget.",
Germany,Bayerischer Rundfunk,Big,Shortlist,,OBJECTIVE OR BIASED – On the questionable use of Artificial Intelligence for job applications,16/02/21,"Investigation, Long-form, Chart, Video, Business, Employment","AI/Machine learning, Json, Adobe Creative Suite, Microsoft Excel","Software programs promise to identify the personality traits of job candidates based on short videos. With the help of Artificial Intelligence, they are being advertised to make the selection process of candidates more objective and faster. In the US more and more companies and their HR departments are working with this kind of software. Also, startups in the UK and EU begin to enter the market with similar products for recruiters. We scrutinized one of those products and wanted to find out whether it makes the recruitment process more objective and fairer. ","As an interdisciplinary team that works at the crossroads of journalism and computer science we think it’s important not only to write about the phenomenon but also include data analysis that would show if such AI systems were able to deliver on the product promises. Also, algorithmic accountability reporting and AI as an investigative topic still is quite an underreported field in Europe and Germany. Furthermore, we wanted to contribute to the general debate if and how we want to use those algorithms in the recruiting process as a society.   The investigation triggered a discussion in the media and policy making landscape about the use of AI for recruiting purposes: The Markup’s Julia Angwin addressed the issue in a detailed twitter thread, the weekly magazine Der SPIEGEL and also Business Punk – a partner magazine of the political magazine stern – quoted the investigation in a longer piece about recruiting software. Also the MIT Technology Review cited our work. A tweet by one of our colleagues was largely echoed, especially in the US. In the aftermath of our investigation scientists from universities all over Europe reached out to learn more about our method and results. Also, unions got interested in the issue and refer to our findings. Policy makers who are working with and around the EU AI Act, where such AI Systems are assessed as “high risk”, regularly quote our investigation. ","Based on insights from scientific research in the field of face and personality recognition on the basis of images or video materiel we developed an experimental setup and several hypotheses to test the software. Together with test persons several hundred video clips were produced. The goal: To find out whether a range of factors would affect the artificial intelligence of the software and hence the personality assessment of the candidates. The experiment was performed in two different ways: On the one hand, a professional actress wearing different outfits would answer the various job interview questions, always using the same text and way of speaking. On the other hand, video producers technically modified a considerable number of recorded videos of a diverse group of test subjects. That way, it was possible to make sure for both scenarios that only a single factor would be purposefully changed in each experiment. ","Since we entered terra incognita – there was almost no reporting and only few research on the use of AI-driven personality prediction based on video-snippets in the context of Human Resources – it was challenging to verify the results. We did that by consulting and discussing our results with experts from relevant fields such as Business Psychology and Computer Vision / Machine Learning.  We gained access to an application that promises to evaluate the mimic, gestures and voice of job candidates on the basis of short videos. No other data journalistic team - as far as we know - has conducted such an experiment on a software that claims to use Artificial Intelligence on job interviews before. Getting access to the application and testing it under “real circumstances” – without being uncovered – was a challenging and time-consuming process. We have good reason to assume that the investigation and especially the results of our experiments can contribute to an urgently needed discussion if and how we want to use such AI driven software for recruiting at such an early stage as a society. ","As an interdisciplinary team that works at the crossroads of journalism and computer science, we find it important to not only write about the phenomenon but also include data analysis that shows whether such AI systems are able to deliver on the product promises. Algorithmic accountability reporting and AI as an investigative topic is still an underreported field. And each investigation requires an individual approach. Thus, every story adds to a better understanding of the field and algorithmic accountability in general.  Our method draws on the idea of investigating an algorithm by holding the input constant – except for a single factor under consideration – while evaluating the differences in the output afterwards. This gives you an idea of the importance and the influence of an input variable without knowing the inner details of the algorithm. These (pairwise) comparisons can and should be repeated for various factors in different contexts. That also helps in order to assess whether the impact of certain factors happens by purpose or hints to technical flaws. This approach can be applied to other AI-related investigations and black-box algorithms in general. ","English, German",https://interaktiv.br.de/ki-bewerbung/,https://interaktiv.br.de/ki-bewerbung/en/index.html,,,,,,,"Elisa Harlan, Oliver Schnuck, Steffen Kühne, Sebastian Bayerl, Benedikt Nabben, Uli Köppen, Lisa Wreschniok","Elisa Harlan works as data journalist and reporter at the German public broadcaster Bayerischer Rundfunk (ARD). She graduated from the German School of Journalism and studied data journalism at Columbia University in the US. She was a fellow at the investigative newsroom Correctiv and was one of the ""Top 30 under 30"" journalists in 2019, awarded by Mediummagazin. Her work was awarded with the Grimme Online Award and nominated for the Reporter:innenpreis 2021. Oliver Schnuck Computer and social scientist by training, he works as data journalist at BR Data / BR Recherche (Bayerischer Rundfunk). Interested in the numbers behind the words and the graphics next to them. Previous work was awarded with the Philip Meyer Award.  Steffen Kühne Tech Lead AI + Automation Lab and BR Recherche / BR Data at Bayerischer Rundfunk. Data journalist and interactive developer. Specialized in data analysis, visualization and storytelling. Sebastian Bayerl  Full stack developer AI + Automation Lab and BR Recherche / BR Data at Bayerischer Rundfunk. Creator of user-friendly web applications and immersive interactive experiences. Uli Köppen is Head of the AI + Automation Lab and Co-Lead of the investigative data team BR Data at German Public Broadcaster Bayerischer Rundfunk. In this role she’s working with interdisciplinary teams of journalists, coders and product developers specializing in investigative data stories, interactive storytelling and experimentation with new research methods such as bots and machine learning. As a Nieman Fellow 2019 she spent an academic year at Harvard and MIT and has won several awards together with her colleagues.",
United States,"International Consortium of Investigative Journalists, The Washington Post, SVT, Miami Herald and 147 media partners around the world",Big,Shortlist,,Pandora Papers,03/10/21,"Investigation, Long-form, Cross-border, Multiple-newsroom collaboration, Database, Corruption, Money-laundering, Business, Economy","AI/Machine learning, Scraping, Microsoft Excel, Google Sheets, CSV, Python","The Pandora Papers investigation lays bare the global entanglement of political power and secretive offshore finance. Based on more than 11.9 million records, containing 2.94 terabytes of confidential information from 14 offshore service providers, the investigation reveals the secret deals and hidden assets of more than 330 politicians and high-level public officials in more than 90 countries and territories, including 35 country leaders. The files were obtained by the International Consortium of Investigative Journalists and shared with 150 media partners around the world. They also reveal secret holdings of more than 130 billionaires from 45 countries including 46 Russian oligarchs.","The publication of the Pandora Papers has generated reactions around the world, among them:  Within hours of publication, authorities around the world vowed investigations. Officials in Pakistan, Mexico, Spain, Brazil, Sri Lanka, Australia and Panama, among other countries quickly promised inquiries while global watchdog groups demanded action in the wake of stories revealing how billionaires, politicians and criminals exploit a shadow financial system that covers up tax dodging and money laundering. Parliaments, including the European Parliament, and those in Malaysia, Colombia, Ecuador, Brazil, among others opened discussions about the Pandora Papers. US lawmakers proposed a legislation that experts say represents the most significant reform of anti-money laundering rules since 9/11. The chairman of a Czech Senate commission called for investigations into the offshore deals of Czech Republic Prime Minister Andrej Babis’ exposed in Pandora Papers reporting. Czech prime minister’s party narrowly lost re-election days after Pandora Papers revelations in a surprise outcome. A Denver museum promised to return looted relics to Cambodia after US moves to seize them. The repatriation of the ancient statues came weeks after Pandora Papers reporting identified dozens of Khmer antiquities linked to an accused trafficker in the collections of major art institutions. Chilean legislators voted to impeach president Sebastián Piñera after Pandora Papers revelations. Proceedings advanced to the Senate, where the Senate voted 24-18 in favor of removing him from office, but the vote fell short of the required threshold. Ecuador’s president Guillermo Lasso survived removal efforts after a majority of the country’s legislature voted against a recommendation to dismiss him following Pandora Papers revelations. In Sri Lanka, President Gotabaya Rajapaksa ordered an investigation into the Pandora Papers findings, including those showing members of his family used shell companies to buy luxury property and artwork.","The 11.9 million records were OCRed, indexed and shared using Datashare, a secure research and analytical open source tool developed by ICIJ’s technical team. To explore and analyze the information, ICIJ identified files that contained beneficial ownership information by company and jurisdiction, structured it and generated lists by country. In cases where information came in spreadsheet form, ICIJ removed duplicates and combined it into a master spreadsheet. For PDF or document files, ICIJ used programming languages such as Python to automate data extraction and structuring as much as possible. ICIJ used machine learning and other tools, including Fonduer and Scikit-learn, to identify and separate specific forms from longer documents. Some provider forms were handwritten, requiring ICIJ to extract information manually. SVT extracted data from passports. After structuring the data, ICIJ used graphic platforms (Neo4J and Linkurious) to generate visualizations and make them searchable. Graph databases were also used in https://offshoreleaks.icij.org/ Machine learning (Universal Sentence Encoder) was used to cluster due diligence files that didn’t show offshore links and tag them in Datashare, enabling reporters to exclude them from their searches. ICIJ also used Python, ElasticSearch, Google Sheets, Microsoft Excel, Datashare-Tarentula for analysis on the use of offshore entities by politicians (published in our Power Players feature), use of U.S. trusts, use of offshore entities by Forbes billionaires, suspicious activity reports, lawyers connected to Baker McKenzie who previously held government posts, use of offshore jurisdictions by clients from different countries and distribution by provider, the role offshore finance plays in hiding looted art and ancient relics, Mossack Fonseca clients in the Pandora Papers (with the Miami Herald). ICIJ validated data using public records. The data and analysis were fact-checked through several rounds using spreadsheets and code. ICIJ used its in-house fact-checking tool “Prophecies” to","The Pandora Papers’s 11.9 million records arrived from 14 different offshore services firms in a jumble of files and formats presenting a massive data-management challenge. The Pandora Papers information brought a new challenge because the 14 providers had different ways of presenting and organizing information. Some organized documents by client, some by various offices, and others had no apparent system at all.  A single document sometimes contained years’ worth of emails and attachments. Some providers digitized their records and structured them in spreadsheets; others kept paper files that were scanned. Some PDFs were 10,000-pages and had information in forms that had to be structured. The documents arrived in English, Spanish, Russian, French, Arabic, Korean and other languages. The complexity of the data and the fact that only 4% of the records were in spreadsheet format required a major effort to validate and structure information about companies in secrecy jurisdictions and their owners in the Pandora Papers. The methods used to sort this out involved different approaches by provider, based on the quality of the data and format of the files. The scale of the leak required important computer power to process the information and structure data out of it to conduct analysis afterwards. The reporting effort of more than 600 journalists in 117 countries and territories was central to the project. Due to the sensitivity of the project and the difficult conditions of press freedom where many of the partners were, ICIJ took security considerations into account, such as the use of encryption for secure communication. As the investigation was done in the middle of the Pandemic, it was not possible to organize an in-person meeting with all reporters. Instead, the team looked to stay connected virtually and online training sessions also helped overcome some of the challenges.","Dealing with a large number of records in different formats requires a combination of approaches. Having a tool, such as Datashare, that facilitates the process of indexing, OCRing and sharing the data securely is central to a global collaboration. Also having a secure place where to coordinate efforts and communicate is key. In the case of Pandora Papers, ICIJ used the Global I-Hub, which is a communication platform that uses the software Discourse and has been adjusted to the specific project needs. Establishing security protocols in global projects is also important. ICIJ and its media partners used encryption during the project.  When working with diverse records coming from different sources and formats, it is important to identify key types of files that could be used to explore key topics in the data and structure information that could lead to the generation of datasets and analysis. Structuring information that comes from different files might require a combination of approaches including the use of code for automated data extraction, machine learning for more complex problems and manual work. Reporting outside the data is key to connect the dots and get the stories. More than 600 journalists worked for nearly two years on the Pandora Papers. Visualizations, such as the use of graph databases, can help with the reporting process and find connections in the data. In global collaborations, it’s important to make data accessible to all team members and facilitate its exploration in a way that reporters with or without coding skills can have the same capacity of navigating the data. Training sessions can help with the process of making data and technologies accessible to everyone. It is also key to allocate time for data validation and fact-checking of data analysis. Public records and comment requests can help with the validation processes.",English,https://www.icij.org/investigations/pandora-papers/about-pandora-papers-leak-dataset/,https://www.icij.org/investigations/pandora-papers/global-investigation-tax-havens-offshore/,https://www.icij.org/investigations/pandora-papers/power-players/,https://www.icij.org/investigations/pandora-papers/alcogal-panama-latin-america-politicians/,https://www.washingtonpost.com/world/interactive/2021/met-museum-cambodian-antiquities-latchford/,https://www.icij.org/investigations/pandora-papers/baker-mckenzie-global-law-firm-offshore-tax-dodging/,https://offshoreleaks.icij.org/,,600 journalists in 117 countries and territories | About: https://www.icij.org/investigations/pandora-papers/about-pandora-papers-investigation/ | Partners: https://www.icij.org/investigations/pandora-papers/pandora-papers-journalists-and-media-partners/,"The Pandora Papers is an investigation by a global team of more than 600 journalists in 117 countries and territories. The team included data journalists, reporters, editors, researchers, fact-checkers and developers that mined together more than 11.9 million records of confidential financial information from 14 offshore service providers.",
Bosnia and Herzegovina,"OCCRP, (Global), Le Monde (France), IrpiMedia (Italy), IStories (Russia), Arab Reporters for Investigative Journalism (Middle East), KRIK (Serbia), Bivol (Bulgaria), Investigace.cz (Czech Republic), Süddeutsche Zeitung (Germany), Le Soir (Belgium), Woxx (Luxembourg), McClatchy/Miami Herald/El Nuevo Herald (U.S.), Piaui (Brazil), Tempo (Indonesia), Armando.Info (Venezuela), La Nacion (Argentina), Inkyfada (Tunisia), Infolibre (Spain)",Big,Shortlist,,OpenLux,08/02/21,"Investigation, Explainer, Long-form, Breaking news, Cross-border, Multiple-newsroom collaboration, Database, Open data, Infographics, Corruption, Money-laundering, Business, Crime","Scraping, Microsoft Excel, Google Sheets, Python","The OpenLux project uncovered years of dubious financial activity involving celebrities, organized crime groups, & people with political connections that had been hidden for years thanks to Luxembourg’s corporate secrecy laws. After Le Monde journalists scraped the data from Luxembourg’s corporate register & worked with OCCRP’s data team to upload it into OCCRP Aleph, our investigative data platform, it became fully searchable for the first time. A global team of journalists investigated the names & companies in the database. Collaborating with 16 media outlets & scores of reporters, we found several instances of elites hiding their business activities from the authorities & public scrutiny.","In Luxembourg, the prosecutor has opened at least 12 cases as a result of the OpenLux investigations. The government has committed to putting more people in the prosecutor's office to investigate financial crime.
 
The Luxembourg government put forward a draft bill aimed at allowing authorities to sanction those who use the country’s financial system for money laundering and tax evasion. The law will grant the LBR the power to impose sanctions and relieve the country’s justice system in cases where the registry is abused. “I can confirm that the Minister of Justice will submit to the government a draft bill allowing the Luxembourg Business Register (LBR) to impose administrative sanctions,” Luxembourg Ministry of Justice Press Attaché Monique Feidt told OCCRP.
 
In Serbia, the Openlux investigation created a huge political scandal and forced the president to give an explanation to the press.

In Spain, the Openlux investigation connected a former IMF director, Rodrigo Rato, to the company owned by an arms trafficker, Abdul Rahman el Assir.
 
Several EU parliamentarians are using the OpenLux investigation to strengthen the legislation for the next EU directive in terms of requirements for future registers. 
 
A month after the publication of the investigation, the European Parliament held a plenary debate titled “Reforming the EU policy framework to stop tax avoidance in the EU after the OpenLux revelations.” 
 
A German MEP used OpenLux to press for open company registers after investigation with Indonesian partners showed a palm oil magnate owns a Frank Gehry structure in Dusseldorf — a fact unknown to the German authorities.
 
Reacting to the publication of OpenLux, EU Commissioner Paolo Gentiloni said he is considering an amendment to the anti-tax avoidance directive.

European Economic Commission officials requested that a Le Monde reporter explain the revelations from OpenLux.","his project was based on the exploitation of public data and was unusual in its large size and geographical scope.   We built a gigantic database from two public registers: the trade and company register ( RCS), which brings together all the administrative acts of Luxembourg companies, and the register of beneficial owners (RBE), which lists the ultimate owners of these companies.  This data and documents were mapped to match OCCRP’s Follow the Money data structure and then ingested into Aleph.  OCCRP Aleph is a data platform that brings together a vast archive of current and historic databases, documents, leaks and investigations.  Adding the Luxembourg data to Aleph automatically extracted the names of companies and people that we could then match up with other records we hold to find leads for stories.
Luxembourg has initiated in recent years a process of financial transparency, but this has its limits: the information contained in its registers is not freely offered to the general public in an open data process (open data), as in other countries. To access it, you must connect to the Luxembourg Business Register (LBR) site, and type the name of a company that you know beforehand. Impossible, on the other hand, to submit the name of a person to know the companies which it owns, nor to make a search on the contents of the documents, as on a search engine.  By scraping and importing this public data into Aleph, we enabled journalists to overcome these imposed technical obstacles and easily conduct reverse-company look-ups and searches and match the results against other datasets to find what would have otherwise proven to be difficult, if not possible, to find connections.","The size of the data was unwieldy, and without context it didn't mean much. To get the context and find the leads we needed to a) use Aleph to find overlapping data and connections and b) find the right journalists who knew enough about the people and companies mentioned to understand the importance of what they were looking at, and then spend the time to tenaciously dig to turn that into a story. Aleph enabled journalists to overcome the limitations of the site itself - reverse-company look-ups by people and in bulk, in addition to the cross referencing matches across datasets.","With the right tools and expertise, public data can be used to find important stories and push for change. 
The data can continually be mined for stories by partners and others, like this one published months after the initial OpenLux project came out. https://www.occrp.org/en/openlux/trail-of-venezuelas-stolen-billions-leads-to-caribbean-luxury-properties
And  Forbes used the OpenLux data and followed up with a piece in November 2021 https://www.forbes.com/sites/giacomotognini/2021/11/08/investigation-how-billionaires-bernard-arnault-amancio-ortega-park-vast-wealth-in-tiny-tax-light-luxembourg/?sh=6261a13327b8","English, French, German, Bahasa, Russian, Spanish, Serbian, Portuguese, Italian",https://www.occrp.org/en/openlux/,https://www.lemonde.fr/les-decodeurs/article/2021/02/08/openlux-the-secrets-of-luxembourg-a-tax-haven-at-the-heart-of-europe_6069140_4355770.html,https://www.occrp.org/en/openlux/revealed-the-secret-luxembourg-base-of-italys-ndrangheta-mafia,https://www.occrp.org/en/openlux/luxembourg-companies-lead-to-luxury-real-estate-across-europe,https://www.occrp.org/en/openlux/frequently-asked-questions,https://www.occrp.org/en/openlux/shedding-light-on-big-secrets-in-tiny-luxembourg,https://www.occrp.org/en/openlux/indonesian-paper-and-palm-oil-tycoon-secretly-bought-historic-munich-building-for-350-million-euros,,"From Le Monde: Anne Michel, Jérémie Baruch, Maxime Ferrer, Maxime Vaudano
 
From OCCRP:
Project Coordination: Antonio Baquero Iglesias
Reporting: Cecelia Anesi, Roman Anin, Antonio Baquero Iglesias, Daniela Castro, Anuška Delić, Lara Dihmis, Stevan Dojčinović, Irina Dolinina, Luiz Fernando Toledo, Nathan Jaccard, Vlad Lavrov, Ilya Lozovsky, Eli Moskovitz, Miranda Patrucic, Dragana Peco, Rana Sabbagh, Sana Sbouai, Roman Shleynov, Olesya Shmagun, Tom Stocks, Jonny Wrate, Martin Young
Editing: Brian Fitzpatrick, Jared Ferrie, Caroline Henshaw, Ilya Lozovsky, Julia Wallace
Fact-Checking: Birgit Brauer, Ivana Jeremić, Olena LaFoy, Bojana Pavlović, Dima Stoianov, Rebekah Ward
Data: Eric Barrett, Friedrich Lindenberg
Research (OCCRP ID): Amra Dzonlić, Daniel Salazar Murillo, David Ilieski, Dragana Peco, Olga Gein, Vladimir Petin, Karina Shedrofsky
Graphics and Visuals: Sergiu Nicolae Brega, Edin Pasović, Svetlana Tiourina
Web Production: Mark Nightingale, Adem Kurić","Maxime Vaudano is the editor of cross border investigations for Le Monde, in France. With a background in data journalism, fact-checking and visual storytelling, he has worked on multiple projects involving tax avoidance and financial crime, such as the Panama Papers, the Paradise Papers, and the CumexFiles. 

Based in Barcelona, Antonio Baquero joined OCCRP in 2020 and is an investigative editor covering Europe and beyond. Before that, he worked at El Periódico Catalunya, where he served as a correspondent in North Africa, specializing in migration, as well as a war reporter in Kosovo, Afghanistan, and Iraq.",
Jordan,Arab Reporters for Investigative Journalism (ARIJ),Small,Shortlist,,Nepotism in Palestine: Officials’ Relatives are Guaranteed a Job Upon Graduation,09/07/21,"Investigation, Long-form, Chart, Corruption",Python,"This investigation documents the appointmnet and promotion of the relatives of officials in the Palestinian Authority in public offices or diplomatic corps without any transparency in these appointmnets.
This is a clear violation of the Palestinian Basic Law, the Palestinian Civil Service Law, and the Diplomatic Corps Law, and it erases the principle of equal opportunities when getting a job.","This investigation provoked reactions and caused repercussions in the Palestinian and Arab public opinion, especially since it was published in the same day when the activist Nizar Banat was killed by the Palestinian Security forces for his rejection of the policies and corruption of the regime.
The investigation was circulated by many institutions, organizations, and local and international networks, and it was republished by many websites, electronic pages, and dozens of websites in Palestine and the Arab world. It also caused an uproar on social media and campaigns were made for it. The investigation also received a certificate of appreciation from ARIJ Annual Awards for Arab Investigative Journalism in 2021 in the Best Arabic Investigative Report - Data Journalism category.","In this data-driven report, we analyzed the official and unofficial databases of employment in the Palestinian government and diplomatic corps over 10 years, and built 5 databases on Microsoft Excel to organize, clean, and analyze quantitative data.
We compared the percentage of government job advertisements to the number of job applicants, as well as extracting the number of relatives of government officials in those jobs. We also used the “Tableau” program to extract tables from the official gazette files in Palestine, as it is one of the important open data sources we relied on. We also used the “Import.io” program during the process of collecting data from the local news archives in Palestine.
As for the visual representation of the data, we relied on the Tableau, Flourish, and Infogram programs. This data visualization played an important role in communicating the message of the investigation.","We faced a set of difficulties and challenges during the investigation, including: ensuring the validity of the data circulated, preparing lists of data on cases related to appointments and promotions, persuading sources to speak because of the seriousness of the issue, communicating with credible sources and experts, and ensuring the “right to comment” from senior authorities in the Palestinian Authority.

We faced indirect threats from some cases during the step of the ""right to comment"", including Hala Fariz, the Palestinian ambassador to Sweden, who hinted that she would go to the judiciary in the event of writing contrary information that denies her account. We were forced to publish the investigation under ""pseudonyms"" because it documents and exposes the corruption of senior officials in the Palestinian Authority and in the judiciary and intelligence agencies who keep promotions and appointments for their relatives and families in clear violation of the law.

The publication of the investigation coincided with the killing of the Palestinian activist Nizar Banat, and sparked widespread controversy. Which made us more insistent on the issue of the pseudonyms, fearing for our safety, especially because one of our colleagues lives in the same city in which Nizar was assassinated and the series of arrests that joined the ranks of journalists in that during that period.

Lastly, the report had to be edited several times, whether in terms of data or information, and we had to present it to more than one lawyer to ensure its credibility, which required more time and effort in completing it to avoid legal accountability","That this investigation can be applied in all regions, and it can be worked on from all over the world through databases and leaks of promotions, in addition to the need to follow up and use the official newspaper of the country, which is the primary and main reason for launching this investigation.
Also, this investigation can be proven by reviewing official press releases, news archives, and job advertisements announced in the Public Personnel Council (PPC), as well as analyzing statistical and civil society databases.",Arabic,https://arij.net/investigations/nepotism-en/,https://arij.net/investigations/nepotism/,,,,,,,"Mariam Abdullah, Mahmoud Mohammed (Pseudonyms)","Mariam Abdullah: investigative journalist
Mahmoud Mohammed: investigative journalist
(We prefer to give a more detailed bio)",
Jordan,Arab Reporters for Investigative Journalism (ARIJ),Small,Winner,"Data journalism can contribute to saving lives, and this is what Mustafa Mansour Mohamed and his team at the Arab Reporters for Investigative Journalism (ARIJ) set out to do with their “Lanes of Death in East Cairo” data investigation. They put themselves in harms ways to confront officials in a country where journalists often see the inside of a jail cell for daring to speak to authority and they collected field data about the rising death toll among pedestrians on the roads of East Cairo while trying to avoid the Egyptian security services that might have categorized their activities as subversive.  The risk paid off - the President of the country addressed the issues highlighted in the investigation and the road infrastructure was improved through the building of pedestrian bridges in dangerous traffic areas and through smart safety systems that were also put in place. The ARIJ investigation combined satellite data, direct data collection and very thorough data analysis to make a big difference for the citizens of Cairo.",Lanes of Death in East Cairo,30/09/21,"Investigation, Long-form, Database, Open data, OSINT, Chart","Microsoft Excel, Python","The investigation documents the increase in deaths and injuries of pedestrians on the restored roads (developed and expanded) in East Cairo, due to the failure of the Ministry of Local Development to establish safety means on those roads, such as speed bumps, pedestrian crossings lanes, traffic lights or providing pedestrian bridges, which increased number of victims on the asphalt, with those reposnible for the lives of people in Egypt being complicit in their deaths.
This is in addition to an environmental defect that resulted from the destruction of trees and historical plantings to expand those roads.","After publishing the investigation, Egypt's president called for emphasizing the safety of roads and facilitating pedestrian crossings lanes, and also called on government officials to expedite the launch of the smart security system that would prevent accidents, monitor roads, and ensure rapid intervention. Some pedestrian bridges were built, and the president also instructed to hold community meetings with the residents to explain what happened.
Here, I would like to point out that in Egypt there is no significant influence of the press, and there is usually a negative and misfortunate influence on the journalist or the newspaper that publishes the investigation.","1. Excel sheets: to build two big databases. The first is for the victims of road accident victims and included details of the time and place of accidents, their ages and jobs, and the type of accidents. Using functions, I got results of the places that witnessed most accidents, their time, type, vehicle type, and the type of injury or death.

2. The second Excel sheet used Google Earth to document satellite images, modifications, and lack of safety measures on the roads before its expansion and after development, to be used as evidence to prove the hypothesis.

3. Juxtapose JS: to display satellite images interchangeably to show the stages before and after development in one interactive image to highlight the size of the trees that were removed and the loss of the green color, and to clarify the absence of any safety measures on the roads for pedestrians.

4. Tableau: to convert some spreadsheets in PDF format to a database in Excel sheets, and I used it on a small scale, especially since there are no databases except for the old ones.

5. Florish: tool to turn the data into interactive graphics that highlight the rise in road and pedestrian accidents over the past and current years, as well as in other thermal graphics to see the timing of accidents in order to protect people from them. It is the areas where most accidents occurred.

6. Google Maps: to plot the roads with the most accidents, and I made a commuter car to document the lengths, widths and lack of safety for pedestrians (which I did on the ground on a real field tour).","- The hardest part of the investigation is the lack of official databases or statistics available from beginning to end, so I had to manually create databases and include them in Excel sheets and collect them myself from Facebook groups for the victims’ areas where they were writing and photographing each incident with video, photos, and data, So I compiled a case by case file of pedestrian accidents after making sure they were documented.
- I tried to update the database from local news sites, where I found a way to find the news of the victims using the keywords “in transit” in the news pieces of pedestrian accidents, which enabled me to update the databases.
- I also conducted a questionnaire to target the population in those areas and find out the extent of the damage and its spread, although this was a danger to me because I could have been tracked by the security forces, especially since some will consider this as a move to spread false news.
- I also had to do field work to confirm what I wrote about about 15 roads, and indeed I almost hit some pedestrians had it not been for the fact that we were driving slowly, and I also documented before and after pictures of the roads via satellite.
- Confrontation with the officials was inevitable but extremely dangerous. Every journalist who tries to criticize or question the achievements of the government and the state is prosecuted, and it is considered unpatriotic, and the officials try to evade answering. But in the end, I confronted them.
- Finally, I published the story under a pseudonym out of fear for my personal safety and out of fear of being stalked.","- Don't give up if there are no databases, you can build and analyze databases by yourself.
- Do not despair no matter how difficult the story is, but you have to document and prove it with more than one evidence and more than one method.
- You have to listen to the people and those affected, we write these stories for them.
- Your personal safety is more important than anything else, and you can write a story to get everyone's attention.
- You have to come up with your final story in an easy, convenient, interactive way that is backed up by all numbers","Arabic, English",https://arij.net/investigations/Cairo-Streets-en/,https://arij.net/investigations/Cairo-Streets/,,,,,,,Mustafa Mansour Mohamed,"An Egyptian investigative journalist, with over 10 years of experience in writing in-depth, data driven, and cross-border stories, as well as stories for television. He recently completed a diploma in data journalism.
He worked as an investigative editor in daily and weekly Egyptian digital and interactive newspapers. He received the Arab Journalism Award, and the Egyptian Journalism Award 2018 from the Syndicate of Journalists in Egypt.
He worked as a former head of the investigations department in its daily and weekly editions, and was responsible for investigative projects and documentaries in Arab and Egyptian news sites.",
Australia,ABC News,Big,Shortlist,,Jackpot: How the gambling industry cashed in on political donations,14/10/21,"Investigation, Long-form, Database, Infographics, Chart, Politics, Business","Scraping, D3.js, Json, Adobe Creative Suite, Microsoft Excel, Google Sheets, CSV","Australians squander more on gambling per capita than any other nation. 
Our series Hitting the Jackpot is the most comprehensive and detailed examination to date of the money flowing from Australia’s gambling industry into the political system.

Unlike most countries, Australia’s gambling industry extends far beyond lotteries and casinos. This is one of the reasons the industry’s political influence is so difficult to quantify.

Our project adopted a new approach to this problem, expanding on previous analyses to trace political payments from more than 370 gambling-related businesses and individuals over 22 years.","The project provided clear evidence of the monetary ties between the gambling industry and Australia’s political parties at a time of intense public scrutiny of the gambling sector’s links to money laundering and organised crime.

Using data from the Australian Electoral Commission (AEC) Transparency Register – a public database of annual disclosures about the financial dealings of political parties, candidates and others involved in the federal electoral process – we uncovered for the first time at least $81,769,853 in political payments linked to entities with a stake in gambling. This is more than twice the amount previously identified.

A key reason for this difference was that our project encompassed hundreds of organisations and businesses with interests in gambling, compared to the dozens identified in previous analyses.

This new data was used to build an unprecedented searchable database which visualised the flow of money through an connected bubble chart. 

The series renewed calls for greater transparency and accountability of Australia’s federal political donations laws and provided impetus for the introduction of two separate bills to federal parliament.

These bills campaigned for, among other reforms:
* banning political donations from particular industries including gambling
* lowering the threshold for reporting political donations to $1,000
* capping donations and electoral spending
* requiring real-time disclosure of political payments.","We used a combination of Excel, Google sheets, OpenRefine and Tableau Prep to collect, share, catalogue, clean and join data. 

We used Tableau Desktop to analyse and explore the data, as well as draft and create proof-of-concept visualisations.

We built a custom tool to scrape the data from the AEC website. We chose to scrape rather than download the data because the scrape picked up metadata (such as client IDs) which we knew would be invaluable not only for cleaning and joining our data to other datasets but also for future use of our work.

The custom tool was designed to pull details of all payments to and from our list of gambling “clients” (first-tier clients), whether declared by the donor or recipient.
It also fetched payments to/from associated entities, enabling us to detect vast amounts of money paid indirectly to parties.

All the charts were created using the D3 framework.
 
In Part 1, the tree chart adapts to various screen sizes/shapes to solve the issue of readability on small screens. In Part 2, we used a series of bee swarms to demonstrate how the pattern and timing of specific payments reveals clues about their purpose.

In both parts, we used a step-by-step “scrollyteller” to to avoid overwhelming people with detail. This format allowed us to guide users through the visualisations and zoom in on specific data points while providing relevant background and context to particular payments and clients.

This format also allowed us to give a sense of the sheer scale and number of payments.

Lastly, the interactive connected bubble chart (adapted from a network chart) in Part 1 allowed the user to explore the data more thoroughly and see how each of the groups are connected.","The hardest part of the project was creating a unique dataset of payments specific to the gambling sector.

We began by negotiating with three separate groups – the Democracy For Sale project, Monash University and the Centre for Public Integrity – to obtain their political donations data (specifically, their lists of clients categorised by industry). 

We then compiled these three databases into a single “master dataset” of roughly 17,800 entities required weeks of cleaning because: 
1. the databases didn’t have shared fields or IDs which would allow a straightforward join; and 
2. donor and recipient names can have any number of variations (e.g. a single donor may be known as “Australian Hotels Association”, “Australian Hotels Association (NSW)”, Australian Hotels Association, NSW”, “AHA NSW”, etc.) making string matches difficult.

After compiling the master list we identified donors with gambling interests.This step was critical because previous analyses of political donations data have tended to categorise each donor by a single interest or industry However, this masks the true size and reach of Australia’s gambling industry, which penetrates into sectors far beyond casinos and lotteries, and includes individuals and businesses which may appear, at first glance, to have no obvious links to gambling.

It involved detailed research, including examining company records, annual reports, media archives, etc., and extensive consultation with researchers, gambling reform groups, and others with expertise in the topic.

We then forensically examined all payments connected to those entities, supplementing data scraped from the AEC website with data collected manually from hundreds of PDF forms. These contain key details – and in some cases, entire payments – missed or excluded from the Transparency Register’s online database.

The resulting dataset is the most comprehensive and detailed record to date of payments made by the gambling industry to Australia’s political parties.","This project demonstrates how journalism can build on previous work to develop new approaches and purpose-built datasets in response to the news of the day.

Our data-driven approach allowed us to not only create a new dataset from existing research, but to also generate new insights and break ground at a time of intense media focus on the gambling industry.

The series also shows that journalists can reveal important information even when the available data is incomplete, patchy or very messy. Australia has some of the weakest political donations laws in the developed world; multiple loopholes mean the source of more than a third of the money remains unknown. But detailed analysis of the available data reveals clues about how strongly Australia’s political parties and elected representatives are linked to the gambling industry.",English,https://www.abc.net.au/news/2021-10-14/how-the-gambling-industry-cashed-in-on-political-donations/100509026,https://www.abc.net.au/news/2021-11-23/how-gambling-industrys-biggest-political-donors-influence-votes/100592068,,,,,,,"Inga Ting, Nathanael Scott, Alex Palmer, , Katia Shatoba, Michael Workman, Anna Freeland, Stephen Hutcheon","Inga Ting (data journalist), Alex Palmer (designer), Nathanael Scott (developer), Katia Shatoba (developer), Michael Workman (researcher) and Stephen Hutcheon (supervising producer) are part of ABC New’s Digital Story Innovations team.
Anna Freeland (researcher) is a digital/data journalist with ABC Arts who joined the DSI team for this project.",
United Kingdom,Reuters,Big,Shortlist,,COVID-19 Vaccination Tracker,25/03/21,"Explainer, Cross-border, Database, News application, Mobile App, Illustration, Infographics, Chart, Map, Health","Scraping, D3.js, Json, Node.js, GitHub Actions, Next.js, React",The Reuters vaccine tracker monitored both the progress of the vaccination campaigns in countries around the world as well as the policy schemes that decided who was given priority access when in each local health jurisdiction at a level of detail not matched elsewhere.,"The tracker page was a vital resource for our readers, both to qualify how far countries had gotten in their vaccination drive and to catalogue the policy decisions being made around who was given priority access, ultimately answering the question of whether readers, themselves, were eligible to receive a vaccine.

In addition to the data feeding our own page, Reuters partnered with Amazon developers to make the data available to Alexa users. In several markets including the United States and the United Kingdom, Alexa users could ask questions and be given answers about whether they were eligible to be vaccinated or when their local health jurisdiction planned for a particular group to be given access.","Vaccine eligibility data was entered into a spreadsheet by each priority group specified by each health jurisdiction rollout plan. At a set time each day, a node process was run in GitHub Actions to scrape the data, and then a series of checks was done to both make sure errors weren’t introduced and to also flag when data hadn’t been updated within 7 days for any priority group. That feedback was sent to a Microsoft Teams channel for clear visibility to a data collection team. Once all checks had passed, the data was published to an API that drove graphics and tables on the tracker page and which Amazon developers would in turn fetch into their own ETL pipelines for the Amazon Alexa skill.

The overall vaccination statistics were visualized via a draggable canvas-based globe, which made it easy to see the yawning gaps regionally between haves and have-nots and made for an intuitive interface to scan progress around the world.","Designing a database to catalogue the disparate vaccine rollout policies of 80 countries and 50 U.S. states was incredibly difficult. Reuters needed a schema that could correctly encode the progress of each campaign in a way that was comparable across countries, generalise categories of how groups were being prioritised, capture the vaccines each group was eligible to receive and track down the dates access was promised or finally granted. Maintaining that schema as those policies quickly changed was especially difficult, and in many cases we needed to reconcile the previous plans to the current ones through our own reporting.","We felt like the summary statistics of how many shots had gone into arms, while important to make larger comparisons of broad policy decisions and outcomes, was actually second to a more fundamental information need our readers had in the first months of the rollout: Answering when they can get a shot. That information was harder to get and standardise and ultimately didn’t lend itself to complex graphics, but we felt it was where we were best placed to fulfil an immediate public service need. That focus paid dividends for us, and we think made ours a unique resource in a crowded field of shot counters.",English,https://graphics.reuters.com/world-coronavirus-tracker-and-maps/vaccination-rollout-and-access/,,,,,,,,"Jon McClure, Prasanta Kumar Dutta and Gurman Bhatia","The Reuters graphics team publishes visual stories and data. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and executes many of the visual stories published.",
Colombia,Cuestión Pública,Small,Shortlist,,We know what you did last legislative period - Season 2,06/02/21,"Investigation, Database, Open data, News application, Fact-checking, OSINT, Mobile App, Illustration, Video, Map, Satellite images, Elections, Politics, Corruption, Money-laundering, Crime","Scraping, Microsoft Excel, Google Sheets, CSV, R, Python","This is the 1st web application revealing the assets, business & network of more than 75 Congress members, chosen by the audience on two open surveys. We exposed their companies, family businesses, state contracts, their networks, and private interests to improve citizens’ decision-making. Indi$cregram: the indiscreet Instagram of power in Colombia. Inspired by the social network, this app informs citizens about really whom they voted for in Congress ahead of the 2022 elections. This tool is based on a methodology to collect, analyze and cross information from 40 public information sources in Colombia and abroad to consolidate around 3500 entities.","Due to Juan Diego Gómez’ investigation published on Indi$cregram, the current president of the Senate, sued Claudia Báez and Diana Salinas, co-founders of Cuestión Pública criminally in the Attorney General's Office, alleging slander. Two judges have previously ruled in our favor in two legal actions undertaken by the senator. Cuestión Pública was judicially harassed by two of the congress members under investigation, and we got many reactions by them regarding our revelations on social media. 
Mainstream medias taken our investigations and put them on the public agenda, also local media republished them regarding their representatives. Three Congress members were forced to launched press released after publication (S1,S2), one of them is a recognized presidential candidate (Petro). 

Also, 48 people paid membership during publication time (1S), and we increased the membership rate to 10% during the second season. 

#Indi$cregram is our most visited page in CP history with 261.336 unique pageviews.
The project was presented as one of the benchmarks in data journalism at NICAR21, the continent's most important data and research conference organized by IRE in the United States. Indi$cregram was taken as a model to explain what data teams in the U.S. and Europe can learn to do inclusive data journalism.

We sent more than 198 (FOIA) to Congress members (two seasons). Thanks to our work to access information (started 2018), at the end of 2019 Colombian Government finally passed a law requiring public officials - Congress members-  to publish their income tax returns, asset declarations, and private interests form. We feel it as our own achievement. 
Thanks to our work and the law we opened, so far, 122 asset declarations, 109 income tax returns, and 76 records of interests of Congress members.","Cuestión Pública scraped open government databases, through R and Python scripts, for the massive extraction of contracting information, campaigns funding, and  laws to then cross-reference this information and identify connections on suspected relationships between congressional representatives in its networks and findings on looting public procurement and other irregularities. All the data were consolidated in Google Sheets for easy information visualization and integration with the web platform.
""We know what you did last legislature period"" databases were built in Google Sheets files which connect with a test environment page where we made all the changes and updates before migrated it to the public web app page. This is possible thanks to a script written within Google Apps Script environment that works like a custom API  for the data.  Also, we created an application ‘made to order’ to cross our list of references with procurement Colombian system API. 
The ""We know what you did last legislature"" application was created with  HTML, CSS, Javascript and Nuxt  framework to recreate Instagram network. An UX designer worked for four months to make the design and improve the user experience of the platform, the Design process progress were followed up in Sketch application. 
As we were under a pandemic crisis all the communications and team operation were managed trough Slack platform, and we shared files also and resources in Google Drive There we consolidated huge databases in order to improve investigation journalism operation processes.","The hardest part to carry out this project was to handle the judicial harassment during publication from the Congress members and its network. Due to reveals that linked Congress members with criminal networks, 'narcos', clientelism and nepotism practices, we received a lot of letters, amendment requests and agressive tweets regarding our publication, our legal team responded accordingly. 
Due to the high harassment received, we make our methodology stronger in the fact-checking process to avoid unjustified replies to our work. Today, our fact-checking method reach high international standards.
In the other side, the learning curve to acquire our innovative investigation methodology takes time, because we investigate each Congress member under the same methodology through eight approaches: 1) ownership of real state assets, 2) family business, 3) business partners, 4) campaign contributors and family members which contract with the State, 5) Court electronic records, 6) conflict of interest in legislative activity, 7) Congress member network, and 8) transparency. 
Due to the huge numbers of technical tools, digital investigation shortcuts, cross-reference databases that we use, and the time constraint to the release, training new journalists in digital forensics techniques, investigation, and data journalism made the project getting longer to finish its first season.
Also, as we are a small newsroom, we had to deal with a large interdisciplinary team composed by employees part-time and full time due to a narrow budget assigned to it. Coordinating those schedules was hard to match heading to date of publication.
Finally, it is a challenge to release such a huge bunch of data and facts,  using understandable language, safeguarding the investigation from a legal point of view, and showing citizens how political power works in Colombia.","The project succeeded in standardizing Cuestión Pública's investigative and data-journalism production processes. In this investigative series, investigative journalism could be measured, analyzed, optimized, and improved in workflows, investigative processes, editorial, legal, design, and development processes. Each Congress member who has been entered to the Indi$cregram application passed through an investigative, a fact-checking, an editorial, and a legal editing protocol measured.

We have created a user-centric web application - which makes investigative journalism 'sexy'.  It was designed for a mobile experience that would inform citizens who they voted for in Congress ahead of the 2022 elections. Indi$cregram app hacked consume habits of the audience (scroll down on Social Media) and replace the content with public interest information. The hashtag was #SabemosLoQueHiciste and our slogan was “Stalk your Congress member and discover whom you really voted for”.
The Indi$cregram considered several stages of publication: 1) journalistic research with data collection through open-source, reporting, data journalism techniques, and digital forensic journalism; 2) information verification to shield investigation with evidence; 3) editorial stage; 4) legal editing and proofreading. 
Cuestión Pública (CP) managed to standardize the investigation processes and create a unique, sophisticated, evolving, replicable, measurable, scalable, and deep methodology.
The Congress members investigated -from all political strands-were chosen by our community in an open poll with 522 (S1)  and 215 voters (S2). This is the first time that a citizen decides who journalists investigate.",Spanish,https://cuestionpublica.com/sabemos-lo-que-hiciste/,,,,,,,,"Claudia Báez, David Tarazona, Diana Salinas, Nacho Gómez, Edier Buitrago, Mateo Restrepo,  Paola Téllez, Sara Cely, José Escobar, Nicolás Barahona,  Andrea Rincón, Mateo Yepes, Angélica Latorre, Natalia Abril, Felipe Hurtado, Camilo Vallejo, Valentina Hoyos, Daneisi Rubio, Jaime Baquero, Javier Pinzón, David Daza, Mariana Hernández, Juan Pablo Marín, Carlos Rodríguez, Nelson Casallas, Ivonne Castillo, Iván Serrano, Daniel Pardo, Nicole Bravo, Lina Bonilla, Susana Londoño, Katherine Castro, María Angélica García, Tomás Mantilla, Ingrid Ramírez Fuquen, Isabela Granados, Alejandra Barrera.","This project involved a team of 37 people including investigative-data journalists, fact-checkers, editors, designers, cartoonists, developers, data scientists, and lawyers. Colombian data-journalist Claudia Báez was the General Director of this series. The editorial team was led by recognized senior editor, Ignacio Gómez, followed by co-founder investigative-data journalist Tarazona. Camilo Vallejo, an expert in freedom of speech and access to information, was the legal editor. Diana Salinas, a recognized investigative journalist, supported the editorial team. Our Investigative data- journalists are between 19 and 29 years old. They are professionals in sociology, mathematics, journalism, and political science.",
United Kingdom,The Economist,Big,Shortlist,,The pandemic’s true death toll,15/05/21,"Explainer, Open data, News application, Chart, Map, Health, Covid-19","AI/Machine learning, D3.js, R","In May 2021, The Economist became the first and only organisation in the world to publish an estimate of the pandemic’s true death toll, as well as estimates of excess deaths for every country in the world. These are, as far as we are aware, the only estimates of excess deaths—the best metric of pandemic mortality, with no dependency on testing—available at the global level. 

The wider project included a cover “briefing” and a leader calling for more equitable distribution of vaccines, as well as multiple follow-up pieces, an online methodology, and daily-updated interactive tracker.","As SARS-CoV-2 spread across the world, it was quickly apparent that some places were better at detecting it than others. Be it in infections or deaths, the biggest difference has always been this: victims in richer countries are more likely to be tested, and therefore more likely to be counted. 

This matters, because for important questions, such as where to allocate aid or vaccines, or even where to accept travellers from, one needs an accurate picture of the pandemic globally. Such a picture is also important to identify which regions remain vulnerable, and to identify and acknowledge the pandemic’s devastation beyond the rich world. If one relies on official covid-19 death counts, the result is predictable: most victims in poor countries will not be counted, and these countries will not receive help.

Our project aims to estimate and show a more accurate representation of the pandemic’s true death toll. The effort has been widely praised by international organisations, such as the WHO, who called it ‘heroic’; the UN who called it “exemplary”; and it has been used as the basis for analysis by the World Bank, as well as being acknowledged the Global Fund, one of the largest distributors of pandemic aid. Researchers at the University of Oxford call it: “the most comprehensive and rigorous attempt to understand how mortality has changed during the pandemic at the global level.” 

The project’s estimates have been widely used, and have even become part of Our World in Data's covid-19 “data dashboard”. We know that at least one of the world’s largest aid organisations has used our findings in discussions about how to allocate pandemic aid. They are also being used by the WHO and our journalists are regularly advising the WHO on related questions upon request.","For the modelling powering the effort, we relied on cutting-edge statistical procedures and implementations. Specifically, the modelling begins with a massive script collecting data on more than 100 statistical indicators, ranging from cell-phone data to researchers’ categorisations of countries into democracies and non-democracies to the share of covid-19 tests that are reported as positive. 
 
This information is then fed into a relatively new implementation of gradient-boosted trees, a flexible machine learning algorithm, not one, or two, but 201 times. This enables the formation of 201 models, of which 200 are there to accurately produce confidence intervals, showing where the models are uncertain, and giving ranges rather than implausible precise numbers. 

Every morning, the data are updated automatically on Github, predictions are generated from all 201 models, and these are then passed to a series of tests. If all tests are passed, all interactive elements on our pages are updated with the new data, and updated spreadsheets are posted to Github. To learn more, please see the separate methodology article, or take a look at the code, models, and data, which is 100% open-source. (Feel free to suggest improvements too!)","The hardest part of this project was the modelling. Estimating how many excess deaths there have been in every country for every day since January 1st 2020 is, to be frank, very difficult. Second hardest has been communicating the uncertainty of the estimates in a good way. 

To take a few examples. The modelling goes beyond just collecting data and feeding them to a model. For instance, data on seroprevalence (the % of people with covid-19 antibodies) in different countries were collected from academic papers. While collections of these exist, whether such surveys were representative had to be decided manually, which involved skimming hundreds of academic articles and government web pages. Lots of countries also had missing data: many countries, for instance, only report updated vaccination counts on irregular intervals. That means that in order for the main models to use vaccination data well, the underlying vaccination data need to be modelled too: a model within a model.    

Communicating uncertainty in a good way has been hard too. For all estimates, we have an upper value and lower value (forming a range of plausible values), and a central estimate within that range. For many countries, good data mean a narrow range. That is easy enough to communicate, and if people just use the central estimate within that range, it does not make much difference. In contrast, for other countries we can only give very broad ranges due to limited data. In such cases, just using one number would give a misleading sense of precision. This meant many careful choices in how we write and present our work visually. For instance, we only provide the range in our summary tables - not the central estimate.","We think that other journalists may get good ideas for how they can use new statistical tools (such as machine learning) in their work, and the importance of providing such information. We have tried to make learning from this project as easy as possible, and keep all our data, code and methods open-source to enable that. Moreover, we have tried to share what we think we have learned from it around the world: university lectures in America, the closing keynote on the use of Machine Learning in Data Journalism at Code.br, the largest data journalism conference in Latin America, and talks to the WHO, Global Fund, and Bill and Melinda Gates foundation.",English,https://www.economist.com/graphic-detail/coronavirus-excess-deaths-estimates?fsrc=core-app-economist,https://www.economist.com/briefing/2021/05/15/there-have-been-7m-13m-excess-deaths-worldwide-during-the-pandemic?fsrc=core-app-economist,https://www.economist.com/leaders/2021/05/15/ten-million-reasons-to-vaccinate-the-world?fsrc=core-app-economist,https://www.economist.com/graphic-detail/2021/05/13/how-we-estimated-the-true-death-toll-of-the-pandemic?fsrc=core-app-economist,https://github.com/TheEconomist/covid-19-the-economist-global-excess-deaths-model,,,,"Sondre Ulvund Solstad, Martín González Gómez","Sondre Ulvund Solstad is The Economist’s Senior data journalist. He writes data-driven articles, as well as models, algorithms and simulations to inform coverage throughout the newspaper. Sondre started at The Economist in February 2020. Previously, he was at Princeton University and before that at NYU. 

Martín González Gómez is a visual journalist at The Economist. He specialises in interactive data visualisation, contributing to data-driven articles and multimedia stories. Martín joined The Economist in 2017 and has a BA from Pompeu Fabra University.",
Switzerland,Neue Zürcher Zeitung,Big,Shortlist,,Is there such a thing as sustainable palm oil? Satellite images show protected rainforest on fire,18/05/21,"Investigation, Long-form, Documentary, Open data, Fact-checking, OSINT, Chart, Map, Satellite images, Politics, Environment, Corruption, Economy","Animation, QGIS, Adobe Creative Suite, Google Sheets, CSV","Palm oil is considered one of the healthiest and most affordable vegetable oils. There's a catch, though: its production comes at the expense of valuable ecosystems. Eco-labels are supposed to help make palm oil more sustainable. But do they deliver what they promise? Our visual investigation finds: only partially. Satellite images and data from Indonesia show that even on certified plantations, illegal clearing techniques and deforestation occur time and again.

The online article presents the investigation using maps, graphics, and text. It is accompanied by a short video. The story also appeared in print and on social media.","At the beginning of 2021, palm oil became a widely debated topic in Switzerland. The reason for this was the upcoming vote on the free trade agreement with Indonesia. Part of the agreement: Only palm oil with specific sustainability labels should benefit from lower import prices. The vote was approved on March 7. 

The Neue Zürcher Zeitung (NZZ) was the only media organization that fact-checked the promises made by the sustainability labels in such a detailed analysis. In doing so, we made an essential contribution to the transparency of promises from democratically elected politicians.

The story was widely read and well-received by our subscribers. Many of them spent an above-average time on the article online. On social media, the story is still shared on a weekly basis in activist circles to point out the greenwashing issues of the palm oil industry. Internally at Neue Zürcher Zeitung, the article is considered a showcase for using satellite imagery as visual evidence and for investigative research with public data.","Our story focuses on one of the largest eco-labels for palm oil, the Roundtable on Sustainable Palm Oil (RPSO). We examine three of their most important rules:
1. Fires are not allowed to be used to clear land.
2. The clearing of virgin rainforest is prohibited.
3. New plantations may not be planted on peat.

Examining various publicly available data sources, we gained new insights into how their members adhere to these rules. First, we used data from Global Forest Watch to grasp the extent of palm oil plantations and the accompanying destruction of the tropical forest. We then overlaid fire hotspots measured by NASA with concession boundaries to reveal where fires occur on certified palm oil plantations. Next, we searched and analyzed publicly available infrared imagery from satellites to track where these fires start. Finally, we demonstrate how valuable tropical forests and peatlands are cleared for plantations using land cover data and true-color satellite imagery.
 
Documents from the palm oil industry also served as important sources. These include various reports from palm oil companies available to the NZZ and the rulebook of RSPO. Using the visual analysis process described above, we could either confirm or refute these documents. 
 
The article presents the entire visual investigation in graphics, maps, and texts. Where high temporal and spatial resolution is available, we use map animations to show developments over time. Satellite images are annotated and presented in chronological order. Readers can also hide and show different data layers on top of the satellite images to view the evidence at their own pace. Finally, the text embeds the visual elements into the story, explains the complex world of sustainability labels to our readers, and guides them through the often opaque data.","The challenges of this story lay in the research and in presenting the complex data to our readers.
 
Most of the data and documents were public. But often, they were not easy to find and difficult to understand. Many of the documents were only found using Google search operators; other confidential documents were obtained through NGOs. The data on palm oil concessions was also incomplete and poorly documented. Some of RSPO's data had to be downloaded from a hidden section of their website.
 
Satellite imagery posed further challenges: Dense cloud cover often hangs over Indonesia, making it challenging to find suitable images. In addition, we had to learn how to read the images with the help of experts, for example, to recognize typical traces of peat fire.
 
Once we had the data, we had to determine which of the hundreds of RSPO-certified palm oil plantations in Indonesia were breaking sustainability rules. To do this, we had to fully understand the RSPO rulebook, which proved complicated: the rulebook uses a very technical language, is full of sub-clauses, and keeps changing over the years. Many of the palm oil companies operate in the legal gray area. So it was also necessary to distinguish whether clearing and fires were actually breaches of the rules or whether they were environmentally harmful but nonetheless legal practices.
 
The final challenge was to convey our research to the reader: We processed satellite images so that events shown in them could be recognized even by laypersons. We visualized small-scale geographic developments in such a way that they were readable on desktop as well as on mobile screens. And in the text, we tell the story of the complex RSPO-rules and the opaque data in a compelling yet straightforward manner.","Consumerism in countries in the Western hemisphere often comes with an environmental and human cost for countries of the Global South. Political decisions on responsible consumerism often involve some form of greenwashing. As journalists in Switzerland, we can contribute to a more sustainable production cycle by fact-checking promises made by politicians and sustainability labels. We hope that our project can serve as an example for this kind of work. Furthermore, we hope that our article inspires journalists to make use not only of open, structured data, but of the wide variety of open-source material, such as satellite images, satellite data, and documents.","English, German",https://www.nzz.ch/english/palm-oil-boom-threatens-protected-rainforest-in-indonesia-ld.1625490,https://www.nzz.ch/international/nachhaltiges-palmoel-bedroht-regenwald-indonesien-ld.1610359,https://www.nzz.ch/international/gibt-es-nachhaltiges-palmoel-aus-indonesien-daten-zeigen-braende-gibt-es-auch-auf-zertifizierten-plantagen-ld.1603919,https://www.instagram.com/p/COucMesLL4q/?utm_source=ig_web_copy_link,https://epaper.nzz.ch/read/6/6/2021-05-11/7?signature=58bc235b33bad5007daab17e61852cdc39db6565d7e5004da74c23bba946d572,,,,"Barnaby Skinner, Conradin Zellweger, Adina Renner","The NZZ Visuals department uses data and visualization to tell stories that range from breaking news to in-depth backgrounds. Our mission: to drive the diversity of storytelling  across the newsroom. Among other things, we implement custom story format and further develop our Toolbox Q.",
Germany,Zeit Online,Big,Shortlist,,Is This How the Pandemic Comes to an End?,24/06/21,"Explainer, Coronavirus","Animation, Simulation, JavaScript React","Is This How the Pandemic Comes to an End? is an interactive explainer on the topic of herd immunity. The piece consists of a calculator, an illustrated scrollytelling element and a simulator. They aim to explain the concept and limitations of herd immunity and answer the question: Can the pandemic be stopped by vaccinating enough people?","Back when the project was launched, there was a lot of talk about herd immunity: Would it be possible to stop the spread of the coronavirus just by vaccinating enough people? At the same time however, the new and more contagious delta variant evolved – and rendered the idea of herd immunity virtually impossible. Our calculator and simulator are based on a simple formula that epidemiologists use to estimate how many people would have to be immune in order to stop the spread – and they showed that delta, most likely, had come to stay. 

The article improved public understanding of a complex but highly relevant topic. It was read and shared widely. Subsequently, the project was translated into English and adapted as a video.","Both the calculator and the simulator are built in JavaScript React. The calculator is based on the formula for the herd immunity threshold: (1 - 1 / R0) / e where R0 is the baseline reproduction number R0 - i.e., the average number of people an infected person infects if no countermeasures are put in place – and e is the vaccination protection against transmission of the virus.

The simulation is based on a SIR model, a model that is used in epidemiology to model the spread of a virus. 

The scrollytelling is based on a custom embed in our content management system. For higher resolution, we exported the illustrations using Ai2html.","The simulator was quite an ambitious project to build - both mathematically and technically speaking. Note that our article shows an actual mathematical simulation: Every time the webpage is loaded, new random dots are generated and a randomized infection process is started.","The entire project was a great team effort. We designed all the elements collaboratively in figma. The simulator and calculator were built in pair programming sessions. We also worked closely with several scientists (special thanks go to Benjamin Maier of the Humboldt-Universität Berlin, who revised our code and calculations up until the night before publication.)","English, German",https://www.zeit.de/wissen/2021-06/herd-immunity-calculator-covid-end-of-pandemic,https://www.zeit.de/wissen/2021-06/herdenimmunitaet-corona-infektionsgeschehen-impfung-neuinfektionen-simulation,https://www.zeit.de/video/2021-07/6263516755001/corona-kann-die-herdenimmunitaet-noch-erreicht-werden,,,,,,"Annick Ehmann, Elena Erdmann, Christopher Pietsch and Julius Tröger","We are a team of journalists, designers and developers who collaborated on this project:Annick Ehmannis a designer and illustrator, Christopher Pietsch is a developer,Julius Trögerheads the interactive team at Zeit Online and Elena Erdmann is a science and data journalist. Together we have been visualizing and explaining coronavirus statistics since the beginning of the pandemic.",
United States,The New York Times,Big,Winner,"The Tulsa massacre, the New York Times has noted, has gone from “virtually unknown to emblematic with impressive speed.” Its own film critics have suggested watching all three new documentaries about the day in 1921 when the entire Black community was burned to the ground by rioting white Tulsa residents, with as many as 300 people killed. The New York Times has now contributed its own groundbreaking work on the massacre, which brings the reader/viewer into the community through a 3D virtual tour. The journalists combed through archival records, interviewed descendants of the families from the community and wrote software to bring the world back to life in a virtual representation. The stunning work is immersive, exhaustive and so impressive and haunting that words fail. By all means, watch all the documentaries on the massacre, but immerse yourself in the Times’ portrait. None of this could have been possible without the expertise of computational journalist with vision.",What the 1921 Tulsa Race Massacre Destroyed,24/05/21,"Investigation, Long-form, Multiple-newsroom collaboration, Database, Open data, Fact-checking, Illustration, Infographics, Map","Animation, 3D modelling, AI/Machine learning, Scraping, QGIS, Canvas, Json, Adobe Creative Suite, Microsoft Excel, Google Sheets, CSV, PostGIS, OpenStreetMap, Python, Node.js","With the 100th year anniversary of the Tulsa Race Massacre approaching, we wanted to help readers understand the full scope of what was lost when an angry white mob destroyed a thriving Black neighborhood in Tulsa, Okla. We spent months reconstructing the historic neighborhood of Greenwood and created a detailed 3-D model of the area as it was before the massacre. This allowed readers to fully experience the level of success and entrepreneurship accomplished by Greenwood’s Black citizens just six decades out of enslavement.","The story of the Tulsa massacre was buried in history for many decades. It is an important piece of American history that we wanted our readers to understand. While historians have pieced together details of the massacre, we wanted to create a fully immersive experience that allowed readers to gain a comprehensive look into the tremendous loss of life, property and generational wealth from the events of May 31 and June 1, 1921.

The piece was well received by readers, academics and other media organizations. Hundreds of Times readers not only lauded the project, but for some, it was the first time they had even heard of the massacre. 

The three lead authors of the project were invited to a number of conferences to speak about the project and how the project came together. Several teachers have reached out to say that the project is being incorporated into their curriculums. And the project has been cited in several academic journals. 

We also released all of the data files associated with the story to the public so that others could build on the work that we published.","We created the 3-D model of the Greenwood neighborhood using a series of computerized and manual steps that transformed historical material into digital data. A process called georeferencing was used to take images from archival Sanborn insurance maps and align them to modern geography. We wrote a computer program to extract the building outlines from those maps using a technique called machine learning. We also created an application to input the height information for each building from the Sanborn maps.

We used a combination of optical character recognition and manual data entry to digitize the Polk-Hoffhine Tulsa City Directory from 1921. This data was used to analyze and map businesses in Greenwood. 

To create maps of the occupations of African-American residents in Greenwood, we analyzed 1920 U.S. census data from Ancestry.com for residents for which occupation data was recorded. The analysis included residents who the census classified as Black or mulatto. Homes of thousands of those residents were mapped using the 1920 Sanborn maps.

Street maps from 1921 and Sanborn maps from 1939 were also used to help map addresses. The addresses of landmarks and other buildings were used to determine the order of house numbers on a block. In some cases, Open Street Maps was used to locate addresses where the numbering system had not changed.","One of the most difficult parts of the project was finding reliable source material about a place that was completely destroyed a century ago. The project required many hours of manual work to research and pore over old photographs, newspaper clippings and other archival material to verify addresses of businesses, locate business owners and gather the level of detail and precision needed to reconstruct each building in the community as accurately as possible. 
 
The project also required creative technical feats, like bringing entire blocks of a neighborhood to life from historical, two-dimensional insurance maps. 
 
The scale of this ambitious project was challenging to pull off as the department balanced other news priorities, like Covid-19, the fallout from the Capitol riot and a new presidential administration.","It’s important for newsrooms to maximize talent from across the newsroom. For this project, we collaborated with nearly two dozen people from different teams, allowing us to achieve high standards in both visual and technical storytelling techniques as well as sophisticated reporting and writing.

The project also allowed us to combine a wide range of reporting skills. This included on-the-ground reporting, in talking to descendants of the massacre. But it also included hours and hours of combing through archival material, reviewing numerous relevant photographs, newspaper clippings and published documents to track down whether an awning did indeed exist on a building during the time of the massacre or whether an address had changed before or after the event. We had to be creative and resourceful in how we tracked down some of these details, and that involved a combination of manual work as well as help from custom software.",English,https://www.nytimes.com/interactive/2021/05/24/us/tulsa-race-massacre.html,https://github.com/nytimes/tulsa-1921-data,https://www.nytimes.com/2021/05/30/insider/greenwood-tulsa-massacre-3d.html,https://www.nytimes.com/2021/05/27/learning/teaching-about-the-tulsa-race-massacre-with-the-new-york-times.html,,,,,"Yuliya Parshina-Kottas, Anjali Singhvi, Audra D.S. Burch, Troy Griggs, Mika Gröndahl, Lingdong Huang, Tim Wallace, Jeremy White and Josh Williams","This project was primarily a collaboration between Graphics and National, with contributions from others in the newsroom. 

By Yuliya Parshina-Kottas, Anjali Singhvi, Audra D.S. Burch, Troy Griggs, Mika Gröndahl, Lingdong Huang, Tim Wallace, Jeremy White and Josh Williams.

Additional reporting, development and production by Barbara Berasi, Matt Craig, Alain Delaquérière, Marcy Edelstein, Lazaro Gamio, Guilbert Gates, Jon Huang, Blacki Migliozzi, Jugal Patel, Bedel Saget, Alison Saldanha and Jessica White.
Editing by Haeyoun Park, Destinée-Charisse Royal and Jamie Stockwell. Additional editing by Grace Maalouf, Jennifer Martin and Farah Mohamed.",
Brazil,Agência Pública,Small,Shortlist,,Brazil has twice as many white people vaccinated as black people,15/03/21,"Investigation, Database, Open data, OSINT, Infographics, Health, Human rights","Adobe Creative Suite, Microsoft Excel, Google Sheets, CSV, R, R Studio","Agência Pública analyzed data from more than 8.5 million people that had been vaccinated in Brazil up to that moment and found out that the number of white people that had gotten the vaccine was double that of black people. This inequality remained when looking at the vaccination rate inside each ethnic group: less than 2 out of 100 black people had received their first covid shot, while more than 3 out of 100 white people had gotten their first jab. At the same time, data revealed that proportionally there were 10% more deaths from Covid-19 among black people.","After the article was published, our team was invited to present the report at a meeting of the National Health Council that discussed Brazil's vaccination plans. Afterward, the National Health Council published a document calling for the adoption of anti-racist actions regarding access to health care by the Ministry of Health, state and municipal health secretariats and councils.

The article was among the finalists for the 2021 Brazilian Conference on Data Journalism, Coda Awards. It was also republished by 33 websites and media outlets, in Portuguese and Spanish. Moreover, we have registered through the BuzzSumo platform more than 90 citations and backlinks of the story in national and international websites and media organizations, including The World, Newsweek and RTP (Rádio e Televisão de Portugal).","Severe Acute Respiratory Syndrome data and data from the National Vaccination Campaign against Covid-19 were exported from the OpenDataSUS website (government platform for data regarding public health) on 22 February and 15 March, respectively. We filtered the SARS data classified as Covid-19-related and counted the notifications, ICU admissions, and deaths of patients who declared their color/race as white, parda (brown), or preta (black) - the last two were added together as negra (black) in the analyses, in accordance with the Brazilian Institute of Geography and Statistics' (IBGE) convention. The script used for the data cleansing, the clean data and the analysis were included in the report. The vaccination database was initially filtered by the field describing which dose had been administered. We only analyzed the data for the 1st dose. Next, we checked the numbers of vaccinated people according to race/color, age and priority groups. The conducted analyzes are described in the article's methodology section.","Dealing with a database that had more than 10 million entries demanded a lot of work and time. It was also difficult to establish methodological criteria for filtering and cleaning the base in order to avoid errors due to possible problems in the filling of the data. The lack of government support to answer our questions was also another obstacle. The project should be selected for having circumvented all these challenges and still having revealed unprecedented information in a country of continental proportions such as Brazil, as well as of great relevance to millions of people in the midst of the largest pandemic ever experienced in the country. The report served as a warning on the impacts of structural racism in Brazil at a time of apparent equality in vaccine distribution and it even reached the highest instances responsible for analyzing the vaccination campaign.","Other journalists can access, replicate, and redo all the steps of our report, since we have published the complete methodology and the databases we used, including databases that are no longer available in public platforms due to the federal government's lack of transparency. The piece can serve as a reference for more journalists to investigate health data and reveal structural inequalities in Brazil and in other countries which have a direct impact on peoples' lives. Having access to this sort of information is fundamental for the elaboration and correction of public policies.","Portuguese, Spanish",https://apublica.org/2021/03/brasil-registra-duas-vezes-mais-pessoas-brancas-vacinadas-que-negras/,https://apublica.org/2021/03/brasil-registra-el-doble-de-personas-blancas-vacunadas-en-comparacion-con-personas-negras/,http://conselho.saude.gov.br/ultimas-noticias-cns/1681-cns-cobra-vacinacao-contra-covid-19-e-acoes-antirracistas-no-acesso-aos-servicos-de-saude,https://www.newsweek.com/racism-fueling-brazils-covid-19-crisis-opinion-1588220,https://theworld.org/stories/2021-04-29/hunger-unemployment-health-care-inequity-pandemics-devastating-impact-brazils,"https://saude.estadao.com.br/noticias/geral,vergonha-do-plano-nacional-de-imunizacao-erros-do-governo-provocam-injusticas-na-fila-da-vacina,70003671668",,,"Bianca Muniz, Bruno Fonseca, Larissa Fernandes e Rute Pina","Bruno Fonseca is an editor and multimedia reporter at Agência Pública, the first non-profit investigative news agency in Brazil. Fonseca has bachelor and master degrees from the Federal University of Minas Gerais and attended the Thomson Reuters Journalism Course on Multimedia Reporting.
Bianca Muniz is a journalism student at the University of São Paulo and a data journalism intern at Agência Pública.",
United States,The Texas Tribune,Big,Shortlist,,"One year ago, the first Texan was killed by COVID-19. 47,000 deaths followed — and it’s not over.",15/03/21,"Explainer, Solutions journalism, Long-form, Database, Open data, Illustration, Infographics, Chart, Health","Scraping, D3.js, Canvas, Json, Adobe Creative Suite, Google Sheets, Python, Node.js","This timeline tracks COVID-19’s rampage through Texas in its first year: the growing death toll, the policy decisions made in response to the pandemic that often influenced its course and the stories of some of the Texans claimed by the virus. Pressed flowers are shown throughout to represent lives lost.","Every day from April 2020 to May 2021, the Texas Tribune’s data visuals team published a coronavirus case tracker. As the numbers grew, we realized we were feeling numb to the scale of the tragedy. This project about the people who died was an effort to bring humanity back into those statistics. After publishing this, we heard from readers who had lost a relative to coronavirus — they deeply appreciated how much care we took in telling this story. This is the best feedback we’ve ever received on a project, and we believe it’s because the story came from a place of deep empathy for its subjects.","People have a hard time understanding very large numbers. Our brains can’t quite process the scope and magnitude of a mass casualty event like the slow-motion pandemic that we have lived through. As the Tribune’s five-person data visuals team updated our case tracker data each day, we also felt that we were becoming numb to the scale of the tragedy, even though many of the Texans who died are our neighbors and friends. We told this story to try to bring humanity back into those statistics. The design is innovative — we used a statistical formula to spread pressed flowers out proportionally based on the number of people who died each week. A greater abundance of flowers represents a period where more people died. Flowers are a symbol of mourning, and at the moments in the story where the flowers are thickest, more Texans were mourning their loved ones.","The reporting for this story was fairly straightforward — we gathered this data from the state every day for over a year, and the narrative is built around stories that our colleagues told along the way. We did have to overcome great obstacles in visualizing the data and designing the story. We did not want the visualization to trivialize the lives of those who died or cause trauma to their loved ones. They were people, not statistics. Once we chose a pressed flower motif, we had to solve the technical puzzle of how to add more than 2,300 images of flowers to the page and still have it load on phones. Above all, we sought to tell a story with empathy and poignancy.","Early collaboration between data reporters, designers and editors is key to telling a story like this fairly and empathetically. Sometimes the simplest approach is best, but this project is also deceptively simple — the technical lift to make it load seamlessly took a lot of effort from our team. It was worthwhile.",English,https://apps.texastribune.org/features/2021/texas-coronavirus-deaths-one-year/,https://apps.texastribune.org/features/2020/texas-coronavirus-cases-map/,https://www.texastribune.org/2021/03/22/texas-coronavirus-widows/,https://apps.texastribune.org/features/2021/texas-nursing-home-deaths-coronavirus-pandemic/,,,,,"Mandi Cai, Emily Albracht and Chris Essig",Mandi Cai tells stories with code and graphics as part of the Tribune’s data visuals team. Emily Albracht is a multidisciplinary designer who develops and maintains the visual design systems and brand guidelines for The Texas Tribune. Chris Essig is the deputy data visuals editor at the Texas Tribune.,
Portugal,Público,Big,Shortlist,,Habitação - do protesto à proposta,07/03/21,"Explainer, Solutions journalism, Long-form, News application, Illustration, Infographics, Chart, Map, Economy","Personalisation, D3.js, Json, CSV, R, R Studio","House prices have been a problem all across Europe, with Portugal being particularly hit by the increase of those prices. Even though that’s something everyone in the country is aware of (and worried about), the news cycle only allowed journalists to focus on the When and What, but never on the Who and the Why. Why is it so hard to rent an apartment? And is everyone being affected in the same way?","Our team, in collaboration with the academic contributors, built a tool that aimed to let people know where they could afford to rent with their income. Because of the level of personalization that this interactive feature gives to the readers, even people who already own a house in some places can put themselves in the shoes of someone who is trying to rent a house in the country. We believe that this level of personalization was the key to its success. The article received more than 800 interactions on Facebook and was shared by pages like the Lisbon Tenants Association, meaning that people being affected by the problem were able to understand it better. Because the article was co-authored with two academic researchers, it was also a spark to ignite the debate between economists and other researchers in the field.","R and QGIS were used to analyze the data and make some draft charts.

Then we used vanilla javascript, leaflet and d3, and chartjs to do the data visualization/ scroller and the news applications.","The biggest challenge was making accessible a topic that is so complex. Because we were working with two academic researchers, there was always the need to reframe the topic and say: ok, but how can we explain that in terms that everyone can understand?","I would say that working with academic researchers as co-authors rather than “simple” sources can be great. The data analysis can be way more interesting since someone that knows a lot about the data you are exploring is by your side. I’ve found out that scientists can also be very good at suggesting ways to tell a story. When something sounded way complex, usually I used to say: ok, but can you give me an example? Usually, THAT example was the compelling way to tell the story to the reader.",Portuguese,https://www.publico.pt/habitacao-do-protesto-a-proposta,https://www.publico.pt/habitacao-do-protesto-a-proposta/o-que-fez-disparar-preco-habitacao-ultimos-anos?ref=habitacao-do-protesto-a-proposta,https://www.publico.pt/habitacao-do-protesto-a-proposta/onde-consigo-morar-com-meus-rendimentos?ref=habitacao-do-protesto-a-proposta,https://www.publico.pt/habitacao-do-protesto-a-proposta/porque-falha-mala-ferramentas-habitacao?ref=habitacao-do-protesto-a-proposta,https://www.publico.pt/habitacao-do-protesto-a-proposta/solucoes-para-habitacao-chamem-os-acrobatas-e-os-jogadores-de-xadrez?ref=habitacao-do-protesto-a-proposta,,,,"Luísa Pinto, Rui Barros, Loraine Vilches, Gabriel Sousa, Sílvia Jorge, Aitor Varea Oro","Rui Barros is a data journalist/ journocoder/ news nerd currently working at PÚBLICO, a daily newspaper in Portugal.",
Brazil,InfoAmazonia,Small,Shortlist,,Engolindo Fumaça (Inhaling Smoke),23/08/21,"Investigation, Long-form, Database, Open data, Infographics, Chart, Map, Satellite images, Environment, Health","QGIS, Json, Adobe Creative Suite, Microsoft Excel, Google Sheets, CSV, R, R Studio, MapBox","Inhaling Smoke is a special project that investigates the effects of air pollution caused by wildfires on the health of the Brazilian Amazon population during the pandemic. 

This toxic synergy was the object of an unprecedented data analysis carried out by a multidisciplinary team of journalists, geographers, statisticians and scientists. We analysed satellite data to determine which locations were most affected by air pollution during the 2020 wildfires and how it impacted the health of amazonians. Smoke was related to an 18% increase in severe cases of Covid and 24% increase in hospitalizations for respiratory syndromes in the 5 most affected states.","The relevance of this project is to show the relationship between two seemingly disconnected events (fire and Covid), highlighting how environmental and public health issues are closely linked.

The data reveals how the pollution from the Amazon fires has a perverse effect on the population that is explained by a specific geography of fire - the most affected municipalities, in different states, indicate the expansion of the arc of deforestation. 

Through extensive data analysis, the project was able to quantify the impact of fire-attributable pollution on the worsening of Covid cases, providing subsidies both for the reader to understand the gravity of the environmental crisis and for the government to make decisions based on the data. 

The importance of a project like this in 2021 is similar to a large-scale post-mortem examination, bringing evidence and proof of the devastation of the Amazon and how this affects its citizens, even those in urban areas far from where the environmental crimes occur.

To tell the stories that the data was revealing, we assembled a team of local reporters, distributed throughout the most impacted states according to our analysis. 

Five reports were published and  gained nationwide attention, driven also by a publishing partnership with the largest national daily newspaper in Brazil and with the two institutions that were partners in the analyses, the Ufac and the Oswaldo Cruz Foundation (Fiocruz), the federal institution of science and technology. 

The articles were republished in local newspapers and by research institutes that had been a reference for our project. We have participated in several meetings and interviews to talk about the results, an academic article with Fiocruz is being written based on our data, and a technical note from the Acre Public Ministry also refers to our data to stress the importance of monitoring air quality.","As a primarily data-driven journalism project, data investigation was the core and starting point of the project. Given the absence of regional air quality data (no Amazonian city had fixed air quality monitoring stations), we processed satellite information to calculate air pollution in the region, generating open data (accessible and documented) for all municipalities in the Legal Amazon. 

This data was then, through statistical analysis, cross-checked with the respiratory illness hospitalization (SARS) database, and specifically the hospitalization cases classified as Covid-19, and we were able to prove the hypothesis that particulate matter from smoke aggravated Covid cases in the 2020 burning season. 

The work with the data was extensive but can be summarized in two main steps: geoprocessing and statistical modeling. The InfoAmazonia analysis processed the various estimates per day from CAMS to arrive at the daily average concentration of fine particulate matter (PM 2.5) for all municipalities in the Legal Amazon. 

The statistical model built specially for the analysis tested several scenarios - including wildfires, deforestation alerts, population, and precipitation - and found significance mainly between cumulative pollution and official numbers of hospitalizations for both SARS and Covid-19. 

Most of the code used to download, tidy and analyze the data was written in R, besides QGis and Google Earth Engine for geoprocessing.

Besides this, the main characteristic of the work was to join data with locally told stories. 

To support the stories, a project was developed that combines Editorial Design and Information Design, combining visual impact with data visualization. It helped to tell the story of Inhaling Smoke with interactive graphics, visual effects, colors, and typographical choices that contributed to the reader's immersion in the special.","The hardest part of the project was to work with the satellite data and defining which pollution air databases would most fit our purposes.

The immediate data cross-referencing of Covid cases and fire hotspots soon showed that the relationship with health was not the fire itself, but the pollution that it generates. We then looked at air quality datasets and realized there were a multitude of variables and models, and that it required a lot of processing to get the data we needed to analyze the impact of fires on human health. 

An initial survey, through interviews with experts, scientists consulting and documentation of academic studies, was done to identify the key air pollution datasets that would be of interest to the project. At this stage, we understood that fine Particulate Matter (PM 2.5, up to 2.5 micrometers in diameter) would be the main variable to be.

Still at this stage, we also understood that PM 2.5 cannot be observed directly from satellites, and therefore there is a need to translate it from the observed Aerosol Optical Depth (AOD). This can be done in several ways and with different models. A series of comparative tests were done with the main datasets (as explained in this documentation), and we decided to use the near real time estimates of CAMS-NRT, from the European Centre for Weather Forecasts (ECMWF). This remote sensing data processed by the InfoAmazonia team was validated with the data measured on the ground by air pollution sensors in Acre.

Working with Covid-19 data was also not easy, because we were working with data from a pandemic still ongoing, so a totally dynamic knowledge about the disease, in addition to the high underreporting of Covid-19 cases (43% of SARS admissions had undefined causes).","Knowing the health impact of air pollution related to fires gives you important knowledge to tell stories of the deforestation fires that affect not only the rural area where the fires are emitted but also the urban population hundreds kilometers away, helping to show the public, and possibly decision-makers, how the environmental and public health issues are closely related.

The difficulty of obtaining regional data on air pollution (none of the Brazilian Amazonian cities have permanent monitoring stations, for instance) can be overcome using global satellite images which, despite showing the plumes of pollution from burning and providing a good research alternative, do not allow for detailed analysis at the more local level. 

For the stories we're telling it is then interesting to confront the data observed through remote sensing with more granular information that reflects local complexity and helps to validate the results obtained in numbers with real stories. In addition to the local population affected by the problem, it is important to listen to local health authorities, doctors, and frontline professionals who can tell us if what we observed from space was really felt on the ground.","English, Portuguese",https://infoamazonia.org/project/engolindo-fumaca/,https://infoamazonia.org/en/project/inhaling-smoke/,https://infoamazonia.org/en/2021/08/23/invisible-enemies-smoke-from-burnings-worsens-covid-19-in-the-amazon/,https://infoamazonia.org/en/2021/08/23/the-victims-of-the-geography-of-fire/,https://infoamazonia.org/en/2021/08/23/social-and-environmental-crises-come-together-in-mato-grosso-in-a-year-of-record-burning-in-the-pantanal/,,,,"Juliana Mori, Renata Hirota, Eduardo Geraque, Felipe Barros, Sonaira Silva, Tatiane Moraes, Guilherme Guerreiro Neto, Juliana Arini, Leandro Chaves, Camilo Estevam, Rebeca Navarro, Lucas Lobo, Leandro Amorim, André Hanauer, Erlan De Almeida Carvalho, Erico Rosa, Guilherme Lobo, Robson Klein Ramon Aquim, Dell Pinheiro, Laiza Lopes, Laura Sanchez, Tony Gross","Juliana Mori is a journalist specialized in audiovisual productions and visualization of geospatial data. Co-founder and editorial director of InfoAmazonia, an independent media outlet that uses maps, data, and geolocalized reports to tell stories about the tropical forest over the nine Amazon countries. Graduated in journalism at Pontifícia Universidade Católica (PUC), in São Paulo, Master in Digital Arts at Universitat Pompeu Fabra (UPF), in Barcelona.",
United Kingdom,The Economist,Big,Shortlist,,Off the Charts—The Economist's data newsletter,15/02/21,"Explainer, Open data, Infographics, Chart, Newsletter","Adobe Creative Suite, Google Sheets","Our data team is all about the numbers—and we want to share that love and enthusiasm for spreadsheets with everyone. Our Off the Charts newsletter—the first data newsletter by a major newspaper—was launched in mid-February 2021 and since then our team has been sharing their behind-the-scenes processes and workflow with tens of thousands of data fans across the globe. Whether you’re fluent in R or Python, or a D3 novice or just want to learn how to elevate your data visualisations—the newsletter reaches a broad audience of data experts and those who want to become one.","Every week, we receive many messages from readers who delight in The Economist’s data journalism and love learning from our team. Our aim was to give everyone the opportunity to get a unique insight into the inner workings of an international data team at a major newspaper. We want to make ourselves approachable and invite our readers to send questions, suggestions and comments to our team (offthecharts@economist.com) and share our behind the scenes in the newsletter every week. We’ve explained the processes behind our coronavirus coverage, for example, our normalcy index and excess-death tracker. We have weighed up the pros and cons of Python compared to R and explained why we rarely use polar charts, how we visualise outliers, how we deal with unreliable stats such as China’s GDP and how we gather and check the data for our infamous Big Mac index. We’ve also explained how we adapt our work for colour blindness, how log scales work and how to declutter a chart to make it easier to read. The comments and feedback we get from colleagues across the industry and other readers—often students or academics who seek to improve their data vis for their scientific papers—show that we are reaching people in and outside of our bubble and are successfully sharing our love for data and inspiring the work of others.","We use many tools that we highlight and explain in our newsletter (Python, R, D3, Illustrator,...). To publish the newsletter we only use Salesforce and Google Docs, as well as our in-house charting tool and Illustrator.","Often the hardest part is going first. This is the first data newsletter by a major news outlet and getting the organisational buy-in may have been the hardest part. But it has since inspired other outlets (for example the Washington Post) to start up similar ventures. The newsletter is a true collaboration between the data team and the newsletter team and it takes good communication and internal processes to deliver a piece to such a high standard every week. Most importantly, the writers have to be ready to make themselves vulnerable and share their behind the scenes—which as we all know isn’t always as perfect as we would like it to be as code written for quick turnaround article can sometimes be messy. But it’s important to us to share it for a more open and accessible data journalism scene.","Since the main goal of the newsletter is to share our behind-the-scenes processes, other journalists can learn a lot about our data visualisation techniques, data gathering methods and statistical methods. We share what tools we use, how we get ideas—how we do all of our work. Other journalists can get a unique insight into our data team as we take our readers on a journey through our workflow every week. We have explained how to read and work with log scales, how to adapt charts for colourblind readers and how to make better bar charts. We also shared how we work with covid data, how we source climate data and how we sonify data. Data journalism is all about openness and sharing with the community—and that’s what we try to deliver with Off the Charts. We want to lift the curtain on how we work and share our processes and everything we learn on the way with our colleagues at other organisations.",English,https://www.economist.com/newsletters/off-the-charts/,https://view.e.economist.com/?qs=d1d1f5fca7de68a5aecdb5a14ccbbe3407e6e7c16b99230fcac811027ea23196fcca759d4954c0a19405d7b8edf055a113a6525eaad59d732d86574f770997ccf8c916d0312608808d6fa430ee3fa7d0,http://view.e.economist.com/?qs=ae339fc32e59501d58ddb942bf7586548c899b4a5bf58876e15b86949831cc7d7617c9d5e18a23353227f5a47d30d45d0bc4bec4b68635798186c94cf39e9e3f734c1e45deb2678be26cdf347dd7ac09,http://view.e.economist.com/?qs=215799c4a22ea81002143df06b3a4587d079a3cd013d463f1021368e3be96fd0f1640e7b95adbc9725b828277daa11d1c4f50a497795549ab87d69869ee994580b7c31d2c5a13d1e3e4f441f402976d7,http://view.e.economist.com/?qs=1f565279610097bc042a1d05f6fe979d99de80cffb5c6e2fdd076e812e4507415d6e846f49b26140ad3bb9735c97f12c1e6649e29b736f33cb44ca0c3e4c826175b27d9c7b47564bf86ae04007d18092,http://view.e.economist.com/?qs=175de8bc361ceb51672044a9763ae8afc7ea4138318064bb5f34c344b13fd8ad8e4f6282e7591b861b7b40201b7f3f5bdc553ffb48406eb55d9bc25a3bd85daf0c7a11c818ff9a53f0ecfdfdffc755f8,https://view.e.economist.com/?qs=175de8bc361ceb51672044a9763ae8af5c99d4935be9919fda7965be2b510d6cbbf2f45ea96ff703bb5428b37eb87d0de446c07c10ead90ab0c8bc1eccbc98f642b412fe16b867ce9707eebb2370c666,,The Economist data and newsletter teams,"We're a group of data journalists, visual journalists, interactive and digital journalists working together on this newsletter.",
Portugal,Público,Big,Shortlist,,With or without a vaccine: what can I do safely where I live?,30/06/21,"Explainer, News application, Infographics, Chart, Lifestyle, Health, Covid-19","Personalisation, Scraping, D3.js, Json, Google Sheets, R, R Studio, Svelte","When Portugal had already reached more than half of its population completely vaccinated, the restriction in place became less rigid. But doing one of those activities that everyone missed so much now meant asking: how safe if it? Should I do it? In collaboration with a local university, PÚBLICO developed a risk assessment tool to help with that decision.","The project was seen by the public as a precious tool for informed decision-making. Because it allowed people to measure on a qualitative scale how safe such activity was, it attracted a lot of traffic, since we understood that the readers were using the tool to plan their activities.","The algorithm started to be drafted by University of Minho professor Pedro Teixeira using google sheets. That same algorithm was then turned into javascript. Since one of the key features of the model was the number of cases in every single municipality, PÚBLICO’s data journalist developed a scraper to fetch the latest data. A GitHub action was created to assure that the data was updated every day. The news application was developed using svelte and it was done so that the text was adjusted to the reader’s provided information. All visualizations were developed using d3.js","We wanted the tool to be the most scientifically accurate and we wanted it to be useful enough so that people not only knew how high/low was the risk of a certain activity, but also what they could do to make it safer. That meant asking a series of questions about the activity itself (for example: if you wanted to go to a restaurant, the tool would ask you how many people were going, if it was indoors/outdoors) but also giving some extra advice about how to make it safer.","It can sound like a cliche, but I’ve learned with this project that a data journalist should do the news applications that he wished he had as a reader. I felt like I had no idea if something I wanted to do again was safe or not. Having only one dose of the vaccine at the moment of publication, I remember spending a huge amount of time asking myself if a specific thing I wanted to do was safe or not. And if I was doing it, what were the things that I could do to make it safer. “If only there was a tool to help me with that decision”, I remember thinking at the time. The next step was: well, why not build it myself?",Portuguese,https://www.publico.pt/interactivo/risco-covid-19-vacinado-nao-vacinado-o-que-fazer-seguranca-meu-concelho,,,,,,,,"Rui Barros, Ivo Neto, Dinis Correia, Pedro M. Teixeira","Rui Barros is a data journalist/ journocoder/ news nerd currently working at PÚBLICO, a daily newspaper in Portugal.",
United States,The Pudding,Small,Shortlist,,Following the Science,31/03/21,"Explainer, Documentary, Open data, Science Communication","Animation, Scraping, D3.js, Three.js, CSV, Python","The Covid-19 pandemic tossed 2020 into disarray. But while cities and countries around the globe were shutting down, scientists and researchers from nearly every country embarked on an unprecedented effort to study, understand, and contain a virus that no one had ever seen before. By exploring the 90,000+ Covid-related research articles that emerged in 2020, this project celebrated the achievements and effort of the global science and research community, while also providing the public a behind-the-scenes glimpse of the scientific process more generally, and an appreciation for the resources that it requires.","The goal of the project was two-fold: 1) to call attention to the scale and enormity of the collaborative, worldwide research effort to combat the Covid-19 pandemic; and 2) , to emphasize to the public the normal workings of the scientific process and, impart an appreciation for the work that is required and the inevitable uncertainty that ensues along the way. Unlike a project advocating for a particular policy change, for example, it is hard to know whether a project like this has achieved its intended impact or not, particularly with regard to the second goal of promoting greater public awareness of the scientific process.  However, the extent to which this story was picked up and highlighted by science, research, and journalism outlets perhaps offers some indication of its wider impact. Throughout 2020, this project was featured by, among others, The World Federation of Science Journalists, The University of Wisconsin's Scout Report on STEM and humanities, DataJournalism.com's year-end ""12 brilliant data journalism projects of 2021"", and was selected for presentation during the Science Communication panel at the 2021 Information+ Conference.","This primary dataset of Covid-related research articles was obtained from PubMed, the online repository for biomedical and life sciences research articles maintained by the National Institute of Health in the US. Using PubMed's public-facing API, I used Python and Jupyter Notebooks to download metadata on 90,000+ Covid-related research articles that came out in 2021. The libpostal library, in combination with the Google Geolocation API, was used to parse the author information from each article and geolocate to a particular city. In order to map the citation network across key research articles, additional PubMed APIs were used to scrape 2-way citation information (articles cited in current article; subsequent articles citing current article) for each article. 

The website was built using React. The visualizations within the site were built using combinations of Three.js, D3.js, P5.js, Deck.gl, and Greensock Animation library.","The biggest challenge in this project was in figuring out how to take this massive dataset of over 90,000 research articles and present it in a way that would be quickly digestible without abstracting away the individual contributions of all of the participating researchers and scientists. Focusing on the collaborations among researchers emerged as a way to do that. Moreover, featuring these collaborations on a map of the world, and allowing viewers to watch as these collaborations unfold over the course of the year, offered a chance to emphasize the truly global nature of the research community’s response to the pandemic (particularly at a time when most of the rest of the news was focusing on how countries, cities, and businesses were all shutting down). 

However, getting this data presented the biggest technical challenge of the project. Each of the 90,000+ articles included metadata on the author names and affiliations (e.g. a particular department at a particular university). The names and affiliations were returned as unstructured strings, which meant that many hours were spent fine-tuning algorithms to attempt to parse the relevant information from the string. The parsed string was then ultimately used to geolocate each author to a specific city around the world. In the end, this analysis allowed for a novel and compelling look at the global network of scientific collaborators working collectively on the Covid-19 pandemic.","It is my hope that other journalists, particularly science reporters, will see this project and be inspired to include greater emphasis on the scientific process, rather than focusing exclusively on the results, in their own reporting. By definition, science deals with the unknown, and researchers are accustomed to dealing with uncertainty. In contrast, the pandemic has underscored just how poorly the public deals with that same uncertainty, particularly around evolving public safety guidelines as new information about the virus is learned. Part of that, I suspect, has to do with how science is communicated, often presenting results as complete and definitive as opposed to placing the results in the context of ""given what we know now…"".  Recent polls from Pew Research Center show that individuals (Americans, at least) are more likely to trust scientific results when they perceive the process as open and transparent (""Trust and Mistrust in Americans' Views of Scientific Experts."" Pew Research Center, Jan 2019). To improve trust, therefore, we can do a better job of inviting the public into the scientific process through our reporting.",English,https://pudding.cool/2021/03/covid-science/,https://www.jeffmacinnes.com/projects/following-the-science,https://vimeo.com/594730939,,,,,,Jeff MacInnes,"Jeff MacInnes PhD, is a data journalist and visual storyteller based out of Seattle WA (USA). He works across all aspects of the visualization process: wrangling raw data, statistical analysis, visual design, and production coding & development. He is the former Director of Technology for Schema Design and holds a PhD in Cognitive Neuroscience from Duke University.",
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,